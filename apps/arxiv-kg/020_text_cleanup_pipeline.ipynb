{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pathlib\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = 'ArxivHealthcareNLP'\n",
    "#CORPUS = 'arxiv_cl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'account': '@ArxivHealthcareNLP@sigmoid.social',\n",
       " 'latest': '110779489140780299',\n",
       " 'corpus_base': '/home/arylwen/datasets/documents/ArxivHealthcareNLP'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_properties(filepath, sep='=', comment_char='#'):\n",
    "    '''\n",
    "    Read the file passed as parameter as a properties file.\n",
    "    '''\n",
    "    props = {}\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            if l and not l.startswith(comment_char):\n",
    "                key_value = l.split(sep)\n",
    "                key = key_value[0].strip()\n",
    "                value = sep.join(key_value[1:]).strip().strip('\"') \n",
    "                props[key] = value \n",
    "    return props\n",
    "\n",
    "corpus_properties = load_properties(f\"corpora/{CORPUS}.properties\")\n",
    "corpus_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_BASE = corpus_properties['corpus_base']\n",
    "JSON_RAW_BASE = f'{CORPUS_BASE}/json_raw/'\n",
    "TXT_BASE = f'{CORPUS_BASE}/text_cleaned/'\n",
    "JSON_BASE = f'{CORPUS_BASE}/json_cleaned/'\n",
    "\n",
    "if not os.path.exists(JSON_BASE):\n",
    "    print(f'{JSON_BASE} does not exist. Creating.')\n",
    "    os.makedirs(JSON_BASE)\n",
    "\n",
    "if not os.path.exists(TXT_BASE):\n",
    "    print(f'{TXT_BASE} does not exist. Creating.')\n",
    "    os.makedirs(TXT_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "json_files = [f for f in listdir(JSON_RAW_BASE) if isfile(join(JSON_RAW_BASE, f))]\n",
    "len(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    json_content = pathlib.Path(filename).read_bytes()\n",
    "    print(len(json_content))\n",
    "    return json_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(raw_text):\n",
    "    text = raw_text\n",
    "\n",
    "    # convert split words on line break, e.g. post-\\nediting\n",
    "    text = text.replace('-\\n', '')\n",
    "    # remove lone new lines in the middle of the sentence - leave only the new lines after .(dot)\n",
    "    one_new_line = r'(?<![\\.\\n])\\n(?!\\n)'\n",
    "    text = re.sub(one_new_line, ' ', text)\n",
    "    # encoded sequences will always generate sequences larger than the chunk size\n",
    "    latexit = r'<latexit.*latexit> *'\n",
    "    text = re.sub(latexit, ' ', text)\n",
    "    # remove urls - not needed for KG\n",
    "    text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "    # remove email addresses - not needed for KG\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    # remove references\n",
    "    text = re.sub(r'doi\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'abs\\/.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'URL\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'url\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'arXiv\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    # remove everythng between parathesis\n",
    "    text = re.sub(r'\\(([^)]+)\\)', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\[([^]]+)\\]', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\{([^}]+)\\}', '', text, flags=re.MULTILINE)   \n",
    "    # remove numbers\n",
    "    numbers = r'[0-9]'\n",
    "    text = re.sub(numbers, '', text)\n",
    "    # lists\n",
    "    lists = r'(i\\))|(ii\\))|(iii\\))|(xvii)|(xviii)'\n",
    "    text = re.sub(lists, '.', text)\n",
    "    #[] () .% -, .+ at the beginning of the line TODO |( \\. )\n",
    "    group_of_characters = r'(\\(\\))|(\\[\\])|(\\.%)|(, \\.)|(–,)|(\\-\\-)|(, \\:\\.)'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    #( ) [ ] [,][, ]\n",
    "    group_of_characters = r'(\\( \\))|(\\[ \\])|(\\[,\\])|(\\[, \\])'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    group_of_characters = r'^(\\.+\\s)|^(,+\\s)|^\\.'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    #stray abbreviations\n",
    "    words = r'(vol\\.)|(no\\.)|(pp\\.)|(Rec@)|(Fig\\.)|(\\.v)|(\\sv\\s)|(\\sb\\s)|(Aug\\.)|(Jan)|(Nov\\.)'\n",
    "    text = re.sub(words, '', text)\n",
    "    #stray commas TODO - ( \\, )|\n",
    "    group_of_characters_2 = r'(,(\\s*)\\n)'\n",
    "    text = re.sub(group_of_characters_2, '', text)\n",
    "    #stray characters\n",
    "    special_characters = r'[–♣♥♠♦•�±𝛹𝐴−𝑦✓⟨⟩ℎ𝑥\"𝜃…𝐷𝑋𝑣𝑥\"𝑐𝒆\"θδ∈×𝑟𝑡𝑨𝒗∗𝒙𝐶𝐿𝑆𝐵𝑀𝑆𝐾𝑙𝑖𝑘𝑒𝑠∀↑↓𝑑→†𝑎𝑔𝑛⊤√π♢♡µλ′]'\n",
    "    text = re.sub(special_characters, '', text)\n",
    "    #leave '-' as part of e.g. bert \n",
    "    special_characters = r'[\\&%=_\\ˆ≥+|˜!#$<>—]'\n",
    "    text = re.sub(special_characters, '', text)\n",
    "    special_characters = r'[鹏城实验室鹏城实验室推出面向中文医疗文本处理的预训练模型]'\n",
    "    text = re.sub(special_characters, '', text)\n",
    "    #fi \n",
    "    special_characters = r'[\\ufb01]'\n",
    "    text = re.sub(special_characters, 'fi', text)\n",
    "    #ffi \n",
    "    special_characters = r'[\\ufb03]'\n",
    "    text = re.sub(special_characters, 'ffi', text)\n",
    "    #empty lines\n",
    "    empty_line = r'\\.\\n(\\.+)'\n",
    "    text = re.sub(empty_line, '\\.\\n', text)\n",
    "    #paragraph names, e.g. A., B., C. - llms are picking them up as topics\n",
    "    paragraph_names = r'\\n(\\s*)[a-zA-Z]\\.'\n",
    "    text = re.sub(paragraph_names, '\\n', text)\n",
    "    #try again to remove numbers\n",
    "    numbers = r'[0-9]'\n",
    "    text = re.sub(numbers, '', text)\n",
    "    #stray letters\n",
    "    group_of_characters_3 = r'(\\bD\\b)|( B )|( o )|(\\bs\\b)|( Xn )|( X )|( y )|( m )|( i )|( c )|( r )|(\\bk\\b)|( d )|( t )|( vt )|( - )|(\\bxn\\b)|(\\bXn\\b)'\n",
    "    text = re.sub(group_of_characters_3, '', text)\n",
    "    group_of_characters_3 = r'( m )|( b )|( x )|( S )|( F )|( g )|(\\bC\\b)|( Z )|( z )|( Xu )|( R )|( \\/ )|( w )|( U= )|( V )|( M )|(dx)|(\\bxt\\b)|(\\bTn\\b)'\n",
    "    text = re.sub(group_of_characters_3, '', text)\n",
    "    group_of_characters_3 = r'(\\bT\\b)|(\\bP\\b)|(\\bMC\\b)|(\\b.-\\b)|(\\b-.\\b)|(\\bK\\b)|(\\bp\\b)|(\\bl\\b)|(\\b-.\\b)|( Xu )|( R )|( \\/ )|( w )|( U= )|( V )|( M )|(dx)|(\\bxt\\b)|(\\bTn\\b)'\n",
    "    text = re.sub(group_of_characters_3, '', text)\n",
    "    #stray words\n",
    "    words = r'(kk)|(kknum)|(pp\\.)|(Rec@)|(Fig\\.)|(\\.v)|(\\sv\\s)|(\\sb\\s)|(Apr\\.)|(Feb\\.)|(Nov\\.)|(Inf\\.)|(CoRR)|(ACM\\, \\,)|(\\bth\\b)|(\\bexp\\b)'\n",
    "    text = re.sub(words, '', text)\n",
    "    #again [] () .% -, .+ \n",
    "    group_of_characters = r'(\\(\\))|(\\[\\])|(\\.%)|(, \\.)|(–,)|(\\-\\-)|(, \\:\\.)|(\\(\\.\\))|(\\(\\, \\))'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    # again ( ) [ ] [,][, ]\n",
    "    group_of_characters = r'(\\( \\))|(\\[ \\])|(\\[,\\])|(\\[, \\])|(\\/)'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    #final commas\n",
    "    commas = r'(\\s+\\, )'\n",
    "    text = re.sub(commas, ', ', text)\n",
    "    commas = r'(\\, \\,)'\n",
    "    text = re.sub(commas, ', ', text)\n",
    "    # commas at the end of the line\n",
    "    commas = r'(\\s*\\,\\s+\\n)|(\\s*\\,\\s*\\.\\s*\\n)|(\\\\\\.)|(\\.\\s*\\.)|(\\.)+|(\\. )+'\n",
    "    text = re.sub(commas, '.', text)\n",
    "    # stray dots (\\.){2}|\n",
    "    commas = r'(\\. ){2,25}'\n",
    "    text = re.sub(commas, '.', text)\n",
    "    commas = r'(\\. ){2,25}'\n",
    "    text = re.sub(commas, '.', text)\n",
    "    commas = r'(\\.){2,25}'\n",
    "    text = re.sub(commas, '.', text)\n",
    "    commas = r'(\\-){2,25}' #--\n",
    "    text = re.sub(commas, '', text)\n",
    "    commas = r'(\\.\\s+\\.)'  #.  .\n",
    "    text = re.sub(commas, '.', text)\n",
    "    commas = r'(\\:\\s+\\:)'  #:  :\n",
    "    text = re.sub(commas, ':', text)\n",
    "    commas = r'(\\,\\s+\\,)'  #:  :\n",
    "    text = re.sub(commas, ',', text)\n",
    "    #arxiv coref hack: we shows up as topic; match only full words\n",
    "    words = r'(\\bWe\\b)'\n",
    "    text = re.sub(words, 'Authors', text)\n",
    "    words = r'(\\bwe\\b)'\n",
    "    text = re.sub(words, 'authors', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_file(filename, content):\n",
    "    pathlib.Path(TXT_BASE+filename).write_bytes(content.encode('utf-8').strip())\n",
    "\n",
    "def write_json_file(filename, content):\n",
    "    pathlib.Path(JSON_BASE+filename).write_bytes(content.encode('utf-8').strip())\n",
    "\n",
    "def save_cleaned_file(document):\n",
    "    filename = document['title']+'.json'\n",
    "    filename_txt = document['title']+'.txt'\n",
    "    json_object = json.dumps(document) \n",
    "    write_json_file(filename,json_object)\n",
    "    write_text_file(filename_txt,document['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34909\n",
      "54020\n",
      "141310\n",
      "57162\n",
      "28012\n",
      "44971\n",
      "32224\n",
      "27429\n",
      "39531\n",
      "74486\n",
      "58611\n",
      "54635\n",
      "33722\n",
      "36999\n",
      "179195\n",
      "36129\n",
      "53921\n",
      "38915\n",
      "33111\n",
      "28462\n",
      "24232\n",
      "81400\n",
      "45224\n",
      "87749\n",
      "47722\n",
      "36104\n",
      "19263\n",
      "28344\n",
      "70523\n",
      "62807\n",
      "43523\n",
      "38154\n",
      "15071\n",
      "50158\n",
      "49055\n",
      "31708\n",
      "117644\n",
      "46334\n",
      "42091\n",
      "50185\n",
      "24545\n",
      "78960\n",
      "50583\n",
      "226236\n",
      "173367\n",
      "48606\n",
      "40946\n",
      "147326\n",
      "83818\n",
      "23457\n",
      "51984\n",
      "10804\n",
      "50796\n",
      "100788\n",
      "126284\n",
      "50113\n",
      "51129\n",
      "43610\n",
      "47728\n",
      "81996\n",
      "45188\n",
      "31702\n",
      "58584\n",
      "45734\n",
      "52798\n",
      "48871\n",
      "49255\n",
      "79165\n",
      "65161\n",
      "66011\n",
      "59802\n",
      "43639\n",
      "28110\n",
      "31930\n",
      "41853\n",
      "63537\n",
      "34605\n",
      "57219\n",
      "74222\n",
      "29838\n",
      "54517\n",
      "84892\n",
      "71537\n",
      "48308\n",
      "30497\n",
      "102249\n",
      "26717\n",
      "34953\n",
      "29955\n",
      "40684\n",
      "39218\n",
      "43188\n",
      "83008\n",
      "50621\n",
      "75610\n",
      "46450\n",
      "73415\n",
      "59104\n",
      "31564\n",
      "24949\n",
      "42876\n",
      "41006\n",
      "57458\n",
      "44448\n",
      "39418\n",
      "23790\n",
      "67694\n",
      "37561\n",
      "134125\n",
      "47176\n",
      "65475\n",
      "51152\n",
      "35618\n",
      "42259\n",
      "21343\n",
      "46123\n",
      "60276\n",
      "44632\n",
      "61152\n",
      "12361\n",
      "45178\n",
      "37010\n",
      "71922\n",
      "49071\n",
      "27696\n",
      "51811\n",
      "61916\n",
      "18850\n",
      "48761\n",
      "110874\n",
      "52836\n",
      "40678\n",
      "50008\n",
      "94631\n",
      "111741\n",
      "49937\n",
      "53721\n",
      "24805\n",
      "28330\n",
      "69587\n",
      "25688\n",
      "29591\n",
      "21479\n",
      "57124\n",
      "40258\n",
      "90651\n",
      "47292\n",
      "31692\n",
      "28370\n",
      "57075\n",
      "92147\n",
      "98252\n",
      "44032\n",
      "29595\n",
      "93652\n",
      "39640\n",
      "47974\n",
      "43306\n",
      "82628\n",
      "27829\n",
      "42477\n",
      "46782\n",
      "56322\n",
      "73292\n",
      "50826\n",
      "40514\n",
      "35618\n",
      "47791\n",
      "59617\n",
      "47867\n",
      "31846\n",
      "45975\n",
      "52276\n",
      "45084\n",
      "25760\n",
      "69315\n",
      "67449\n",
      "67824\n",
      "59084\n",
      "64624\n",
      "36957\n",
      "40743\n",
      "19233\n",
      "92374\n",
      "45919\n",
      "44394\n",
      "48127\n",
      "42067\n",
      "50706\n",
      "17224\n",
      "85156\n",
      "95535\n"
     ]
    }
   ],
   "source": [
    "for json_file in json_files:\n",
    "    file_name = join(JSON_RAW_BASE, json_file)\n",
    "    doc_string = read_text_file(file_name) \n",
    "    doc_string = doc_string.decode(encoding = 'utf-8')\n",
    "    #print(doc_string)\n",
    "    document = json.loads(doc_string)\n",
    "    #print(document)\n",
    "    text = document['text']\n",
    "    #print(text)\n",
    "    document[\"text\"] = cleanup_text(text)\n",
    "    save_cleaned_file(document)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
