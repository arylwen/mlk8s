{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My OpenAI Key\n",
    "import os\n",
    "import time\n",
    "\n",
    "import openai\n",
    "os.environ['OPENAI_API_KEY'] = \"EMPTY\"\n",
    "os.environ['OPENAI_API_BASE'] = \"http://10.0.0.222:30307/v1\"\n",
    "openai.api_key = \"EMPTY\"\n",
    "openai.api_base = \"http://10.0.0.222:30307/v1\"\n",
    "\n",
    "#model = \"Writer/camel-5b-hf\"\n",
    "#model = \"mosaicml/mpt-7b-instruct\"\n",
    "model = \"mosaicml/mpt-30b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index import SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.graph_stores import SimpleGraphStore \n",
    "from llama_index.langchain_helpers.text_splitter import SentenceSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.prompts.base import Prompt\n",
    "from llama_index.prompts.prompt_type import PromptType\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from kron.llm_predictor.KronOpenAILLM import OpenAI\n",
    "from kron.llm_predictor.KronLLMPredictor import KronLLMPredictor\n",
    "from kron.indices.knowledge_graph.KronKnowledgeGraphIndex import KronKnowledgeGraphIndex \n",
    "from kron.prompts.kg_prompts import KRON_KG_TRIPLET_EXTRACT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer/camel sens endoftext back\n",
    "from llama_index.utils import globals_helper\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer = lambda text: enc.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "globals_helper._tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data/').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMEL_INLINE_KG_PROMPT_TEMPLATE = (\n",
    "            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n\"  \n",
    "            \"Some text is provided below. Given the text, extract up to {max_knowledge_triplets} knowledge triplets in the form of \" \n",
    "            \"(subject, predicate, object). \\n\\n\"  \n",
    "            \"### Input: \\n\"\n",
    "            \"Text: Alice is Bob's mother. \\n\" \n",
    "            \"Triplets: \\n\"\n",
    "            \"    (Alice, is mother of, Bob) \\n\"\n",
    "            \"Text: Philz is a coffee shop founded in Berkeley in 1982. \\n\"\n",
    "            \"Triplets: \\n\"\n",
    "            \"    (Philz, is, coffee shop) \\n\"\n",
    "            \"    (Philz, founded in, Berkeley) \\n\"\n",
    "            \"    (Philz, founded in, 1982) \\n\"\n",
    "            \"Text: This small and colorful book is intended for children. It was named after the Moon, and was gifted to Jack. \\n\"\n",
    "            \"Triplets: \\n\"\n",
    "            \"    (book, intended for, children)\\n\"\n",
    "            \"    (book, is, small) \\n\"\n",
    "            \"    (book, is, colorful) \\n\"\n",
    "            \"    (book, named after, Moon) \\n\"\n",
    "            \"    (book, gifted to, Jack) \\n\"    \n",
    "            \"Text: Nick saw a few dwellings, brightly painted cottages, shining in the sun. They were not ready for guests. \\n\"\n",
    "            \"Triplets: \\n\"\n",
    "            \"    (dwellings, are, cottages) \\n\"\n",
    "            \"    (dwellings, shine in, sun) \\n\"\n",
    "            \"    (dwellings, not ready for, guests) \\n\"\n",
    "            \"    (dwellings, seen by, Nick) \\n\"\n",
    "            \"    (dwellings, are, a few) \\n\"\n",
    "            \"    (cottages, are, brightly painted) \\n\"\n",
    "            \"\\n### Text: {text} \\n\"\n",
    "            \"\\n### Triplets:\"\n",
    ")\n",
    "\n",
    "CAMEL_INLINE_KG_TRIPLET_EXTRACT_PROMPT = Prompt(\n",
    "    CAMEL_INLINE_KG_PROMPT_TEMPLATE, prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MPT_SHORT_INLINE_KG_PROMPT_TEMPLATE = (\n",
    "            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n\"  \n",
    "            \"Some text is provided below. Given the text, extract up to {max_knowledge_triplets}  knowledge triplets in the form of \" \n",
    "            \"(subject, predicate, object). \\n\\n\" \n",
    "            \"### Input: \\n\"\n",
    "            \"Text: Alice is Bob's mother. \\n\" \n",
    "            \"Triplets: \\n\"\n",
    "            \"    (Alice, is mother of, Bob) \\n\"\n",
    "            \"Text: Philz is a coffee shop founded in Berkeley in 1982. \\n\"\n",
    "            \"Triplets: \\n\"\n",
    "            \"    (Philz, is, coffee shop) \\n\"\n",
    "            \"    (Philz, founded in, Berkeley) \\n\"\n",
    "            \"    (Philz, founded in, 1982) \\n\"\n",
    "            \"### Text: {text} \\n\\n\"\n",
    "            \"### Response:\"\n",
    ")\n",
    "\n",
    "MPT_SHORT_INLINE_KG_TRIPLET_EXTRACT_PROMPT = Prompt(\n",
    "    MPT_SHORT_INLINE_KG_PROMPT_TEMPLATE, prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MPT_LONG_INLINE_KG_PROMPT_TEMPLATE = (\n",
    "            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n\"  \n",
    "            \"Some text is provided below. Given the text, extract up to {max_knowledge_triplets} knowledge triplets in the form of \" \n",
    "            \"(subject, predicate, object). Avoid duplicates. \\n\\n\"  \n",
    "            \"### Input: \\n\"\n",
    "            \"Text: Alice is Bob's mother. \\n\" \n",
    "            \"Triplets: \\n\"\n",
    "            \"    (Alice, is mother of, Bob) \\n\"\n",
    "            \"Text: Philz is a coffee shop founded in Berkeley in 1982. \\n\"\n",
    "            \"Triplets: \\n\"\n",
    "            \"    (Philz, is, coffee shop) \\n\"\n",
    "            \"    (Philz, founded in, Berkeley) \\n\"\n",
    "            \"    (Philz, founded in, 1982) \\n\"\n",
    "            \"Text: This small and colorful book is intended for children. It was named after the Moon, and was gifted to Jack. \\n\"\n",
    "            \"Triplets: \\n\"\n",
    "            \"    (book, intended for, children)\\n\"\n",
    "            \"    (book, is, small) \\n\"\n",
    "            \"    (book, is, colorful) \\n\"\n",
    "            \"    (book, named after, Moon) \\n\"\n",
    "            \"    (book, gifted to, Jack) \\n\"    \n",
    "            \"Text: Nick saw a few dwellings, brightly painted cottages, shining in the sun. They were not ready for guests. \\n\"\n",
    "            \"Triplets: \\n\"\n",
    "            \"    (dwellings, are, cottages) \\n\"\n",
    "            \"    (dwellings, shine in, sun) \\n\"\n",
    "            \"    (dwellings, not ready for, guests) \\n\"\n",
    "            \"    (dwellings, seen by, Nick) \\n\"\n",
    "            \"    (dwellings, are, a few) \\n\"\n",
    "            \"    (cottages, are, brightly painted) \\n\"\n",
    "            \"### Text: {text} \\n\"\n",
    "            \"### Triplets:\"\n",
    ")\n",
    "\n",
    "MPT_LONG_INLINE_KG_TRIPLET_EXTRACT_PROMPT = Prompt(\n",
    "    MPT_LONG_INLINE_KG_PROMPT_TEMPLATE, prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.prompts.base.Prompt object at 0x7f545a6b5640>\n"
     ]
    }
   ],
   "source": [
    "max_triplets = 2\n",
    "chunk_size = 192\n",
    "chunk_overlap = 48\n",
    "if 'mpt-30b' in model:\n",
    "    #prompt = MPT_SHORT_INLINE_KG_TRIPLET_EXTRACT_PROMPT\n",
    "    #max_tokens = 320\n",
    "    prompt = MPT_LONG_INLINE_KG_TRIPLET_EXTRACT_PROMPT\n",
    "    max_tokens = 510\n",
    "    max_triplets = 3\n",
    "    chunk_size = 128 #160\n",
    "    chunk_overlap = 32\n",
    "elif 'camel-5b' in model:\n",
    "    #camel generates more tokens from the same text than mpt\n",
    "    prompt = CAMEL_INLINE_KG_TRIPLET_EXTRACT_PROMPT\n",
    "    max_tokens = 704\n",
    "elif 'mpt-7b' in model:\n",
    "    prompt = CAMEL_INLINE_KG_TRIPLET_EXTRACT_PROMPT\n",
    "    max_tokens = 560\n",
    "else:\n",
    "    prompt = None\n",
    "    max_tokens = 274\n",
    "\n",
    "print(str(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_window=2048 num_output=510 is_chat_model=False\n"
     ]
    }
   ],
   "source": [
    "# define LLM; this is the kron OpenAI - supports local models\n",
    "llm=OpenAI(temperature=0.01, model=model)\n",
    "#chunk_size+prompt_length+expected length of returned triples must be less than max_tokens\n",
    "llm.max_tokens = max_tokens\n",
    "llm_predictor = KronLLMPredictor(llm)\n",
    "print(llm_predictor.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define TextSplitterTextSplitter\n",
    "text_splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define NodeParser\n",
    "node_parser = SimpleNodeParser(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define ServiceContext\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, node_parser=node_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdbe4e9f708447d88dfccfa531ee8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing nodes:   0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Graph built in: 54661.5436360836s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph_store = SimpleGraphStore()\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "# NOTE: can take a while! \n",
    "\n",
    "start = time.time()\n",
    "index = KronKnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    max_triplets_per_chunk=max_triplets,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    kg_triple_extract_template=prompt,\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Knowledge Graph built in: {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_path = f\"storage/{model.replace('/', '-')}-long-inline\"\n",
    "index.storage_context.persist(persist_dir=persist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mosaicml-mpt-30b-instruct-long-inline-kg-prompt.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000px\"\n",
       "            src=\"mosaicml-mpt-30b-instruct-long-inline-kg-prompt.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f5433619910>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create graph\n",
    "from pyvis.network import Network\n",
    "\n",
    "#display all nodes\n",
    "g = index.get_networkx_graph(limit = 5000)\n",
    "net = Network(height='1000px', width='100%', notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(g)\n",
    "#net.show_buttons(filter_=True)\n",
    "net.show(f\"{model.replace('/', '-')}-long-inline-kg-prompt.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
