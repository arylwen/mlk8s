{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import re\n",
    "\n",
    "import openai\n",
    "os.environ['OPENAI_API_KEY'] = \"EMPTY\"\n",
    "os.environ['OPENAI_API_BASE'] = \"http://10.0.0.222:30307/v1\"\n",
    "openai.api_key = \"EMPTY\"\n",
    "openai.api_base = \"http://10.0.0.222:30307/v1\"\n",
    "\n",
    "model = \"Writer/camel-5b-hf\"\n",
    "#model = \"mosaicml/mpt-7b-instruct\"\n",
    "#model = \"mosaicml/mpt-30b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "#kron extensions to llama_index to support openai compatible api\n",
    "sys.path.append('../llama_index')\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import pathlib\n",
    "import fitz\n",
    "\n",
    "PDF_BASE = '/home/arylwen/datasets/documents/pdf/'\n",
    "TXT_BASE = '/home/arylwen/datasets/documents/text_cleaned/'\n",
    "JSON_BASE = '/home/arylwen/datasets/documents/json_cleaned/'\n",
    "doc_path = '/home/arylwen/datasets/documents/json_raw/2007.02871.pdf.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55211\n",
      "{\"title\": \"2007.02871.pdf\", \"text\": \"DART: Open-Domain Structured Data Record to Text Generation\\nLinyong Nan1\\nDragomir Radev1,2\\nRui Zhang3\\nAmrit Rau1\\nAbhinand Sivaprasad1\\nChiachun Hsieh4\\nXiangru Tang1\\nAadit Vyas1\\nNeha Verma1\\nPranav Krishna5\\nYangxiaokang Liu1\\nNadia Irwanto1\\nJessica Pan1\\nFaiaz Rahman1\\nAhmad Zaidi1\\nMurori Mutuma1\\nYasin Tarabar1\\nAnkit Gupta1\\nTao Yu1\\nYi Chern Tan1\\nXi Victoria Lin2\\u2217\\nCaiming Xiong2\\nRichard Socher2\\nNazneen Fatema Rajani2\\n1 Yale University\\n2 Salesforce Research\\n3 Penn State University\\n4 The University of Hong Kong\\n5 MIT\\n{linyong.nan, dragomir.radev}@yale.edu, rmz5227@psu.edu, nazneen.rajani@salesforce.com\\nAbstract\\nWe present DART, an open domain structured\\nDAta Record to Text generation dataset with\\nover 82k instances (DARTs). Data-to-Text an-\\nnotations can be a costly process, especially\\nwhen dealing with tables which are the ma-\\njor source of structured data and contain non-\\ntrivial structures. To this end, we propose a\\nprocedure of extracting semantic triples from\\ntables that encodes their structures by exploit-\\ning the semantic dependencies among table\\nheaders and the table title. Our dataset con-\\nstruction framework effectively merged hetero-\\ngeneous sources from open domain semantic\\nparsing and dialogue-act-based meaning rep-\\nresentation tasks by utilizing techniques such\\nas: tree ontology annotation, question-answer\\npair to declarative sentence conversion, and\\npredicate uni\\ufb01cation, all with minimum post-\\nediting. We present systematic evaluation on\\nDART as well as new state-of-the-art results\\non WebNLG 2017 to show that DART (1)\\nposes new challenges to existing data-to-text\\ndatasets and (2) facilitates out-of-domain gen-\\neralization. Our data and code can be found\\nat https://github.com/Yale-LILY/\\ndart.\\n1\\nIntroduction\\nAutomatically generating textual descriptions from\\nstructured data improves the accessibility of knowl-\\nedge bases to lay users. Such applications include\\nexplaining data records to non-experts (Cawsey\\net al., 1997), writing sports news (Chen and\\nMooney, 2008), summarizing information in mul-\\ntiple documents (Fan et al., 2019), and generating\\ndialogue responses (Wen et al., 2015).\\nWhile signi\\ufb01cant progress has been made in this\\n\\ufb01eld, there are still several issues with existing\\nData-to-Text datasets. First, they adopt a \\ufb02at ontol-\\nogy structure of the data, such as slot-value pairs\\nfor data records (Lebret et al., 2016; Novikova et al.,\\n\\u2217Now at Facebook AI.\\n2017b) or \\ufb02at schema for tables (Wiseman et al.,\\n2017; Chen et al., 2020a; Parikh et al., 2020). This\\n\\ufb02at structure is not powerful enough to encode rich\\nsemantic relationships in the ontology of the struc-\\ntured data, especially tables, whose representation\\ncan be further improved with these semantic knowl-\\nedge. Second, some of the datasets only focus on\\na small number of domains or knowledge graphs,\\ntherefore providing limited number of predicates\\nand data ontologies. For example, E2E (Novikova\\net al., 2017b) on restaurants and WebNLG (Gar-\\ndent et al., 2017) on 15 categories from DBPedia.\\nFurthermore, some of them only have loose align-\\nments between data input and sentence due to the\\nnature of the task (Wiseman et al., 2017) and the\\nautomatic generation procedure (Vougiouklis et al.,\\n2018; Elsahar et al., 2018).\\nTo address some of these issues and to encour-\\nage further research in natural language generation\\nfrom structured data, we introduce DART, a large\\nand open-domain structured DAta-Record-to-Text\\ngeneration corpus. The goal of DART is to har-\\nvest the diverse predicates occurred in Wikipedia\\ntables, which is signi\\ufb01cantly richer than those de-\\n\\ufb01ned in the domain speci\\ufb01c ontologies E2E and\\nWebNLG were built on (Table 2). We also in-\\ntroduce novel tree ontology annotation on tables,\\nwhich converts a \\ufb02at table schema into a tree struc-\\ntured semantic frame. The tree ontology re\\ufb02ects\\nthe core and auxiliary relations in the table schema,\\nand naturally occurs across many domains. As a\\nresult, DART provides high-quality sentence an-\\nnotations to tree structured semantic frames ex-\\ntracted from various data sources, including Wik-\\niSQL (Zhong et al., 2017) and WikiTableQuestions\\n(Pasupat and Liang, 2015), two open-domain ques-\\ntion answering datasets, as well as E2E (Novikova\\net al., 2017b) and WebNLG (Gardent et al., 2017)\\n(Figure 1). We evaluated several state-of-the-art\\ndata-to-text models on DART, and found that while\\nthese models achieve impressive performance on\\narXiv:2007.02871v2  [cs.CL]  12 Apr 2021\\ndomain-speci\\ufb01c datasets, their performance suffers\\non DART due to its open-domain nature and richer\\nsemantic structures.\\nOur contributions are as follows. (1) We present\\na large and open-domain corpus for structured data\\nrecord to text generation, annotated with tree on-\\ntologies converted from the table. This hierarchical\\ninput differentiates our corpus from existing data-\\nto-text corpora. (2) We benchmark several state-\\nof-the-art data-to-text models to show that DART\\nintroduces new generalization challenges. (3) We\\ndemonstrate that using DART for data augmenta-\\ntion improves the performance of existing models\\non the WebNLG 2017 dataset. We expect the re-\\nsults to generalize to other data-to-text datasets\\ngiven the open-domain nature of DART.\\n2\\nDART Data Collection\\nAs shown in Figure 1, DART is constructed from\\nthree different sources: (1) human annotation on\\nWikipedia tables from two table semantic parsing\\nand question answering datasets WikiSQL and Wik-\\niTableQuestions (\\u00a7 2.1), (2) automatic conversion\\nof questions in WikiSQL to declarative sentences\\n(\\u00a7 2.2), and (3) incorporation of existing datasets\\nincluding WebNLG 2017 and Cleaned E2E (\\u00a7 2.3).\\nAfter collecting the \\u27e8triple-set, sentence\\u27e9 pairs from\\nvarious data sources, we manually canonicalized\\nthe predicates and show that DART covers a broad\\nrange of topics (\\u00a7 2.4). Finally, we discuss the data\\nsplit in \\u00a7 2.5.\\n2.1\\nTree Ontology and Sentence Annotation\\non Tables\\nTables are a major source of structured data that\\ncontain a wealth of information complementary\\nto text and knowledge graphs.\\nWe aim to col-\\nlect \\u27e8triple-set, sentence\\u27e9 pairs from open-domain\\nWikipedia tables.\\nHowever, table schema are\\n\\ufb02at, making them not directly usable for building\\nsubject-predicate-object triples to capture rich rela-\\ntionships in the data.\\nAs shown in Figure 2, we propose a two-stage an-\\nnotation process that involves two groups of anno-\\ntators: internal annotators and Amazon Mechanical\\nTurk1 workers. In the \\ufb01rst stage, skilled internal an-\\nnotators specify the parent of every column header\\nto construct a tree-structured ontology for each ta-\\nble. In the second stage, both internal and external\\nannotators provide a sentential description of the\\n1https://www.mturk.com/\\nhighlighted cells in a row that are automatically-\\nchosen based on the ontology.\\nTree Ontology Annotation\\nFor each column in\\na given table, our internal annotators labeled its\\nontological parent. In Figure 2, for example, the an-\\nnotator would provide the sequence {NULL, TEAM,\\nSTADIUM, STADIUM, TEAM} as the parent of each\\ncolumn \\u2014 column TEAM has no parent, STADIUM\\nhas parent TEAM, and so on. In many cases, the\\nrelationship between a parent column and its child\\ncolumn can be conceptualized as a \\\"has-a\\\" relation-\\nship. For tables that are malformed or have dupli-\\ncate or missing column names (as shown in Figure\\n5 of the Appendix), annotators either changed or\\nadded appropriate column names in order to \\ufb01t\\nthese patterns. For each table we generate an ontol-\\nogy tree whose root is always [TABLECONTEXT].\\nThis root node either has (1) one child node [TI-\\nTLE] in the cases where the table title is the subject\\nof entire table, or (2) column header node(s) and\\na [TITLE] node as children, as shown in Figure 2.\\nThis is because in some tables, the table title itself\\nis more appropriate to be the root of the ontology\\ntree (example shown in Figure 6 of the Appendix).\\nIn these cases, annotators assigned the special to-\\nken [TITLE] as the parent of the relevant column\\nnodes. For other tables, title usually provides im-\\nportant context for understanding the table\\u2019s rows\\n(example shown in Figure 7 of the Appendix). In\\nsuch cases, [TITLE] is made a child of [TABLE-\\nCONTEXT] together with the column headers that\\nare appropriate.\\nWe evaluate the quality of the initial tree on-\\ntology annotation and made corrections with the\\nfollowing procedure: (1) reject and request correc-\\ntions from the original annotators if the provided\\nontology is disconnected or contains a cycle, (2)\\nverify that all column headers appear as a node in\\nthe tree. For many tables, the determination of an\\nontology is a subjective process with many \\\"cor-\\nrect\\\" answers - for example, swapping the positions\\nof TEAM and CITY in the tree in Figure 2 produces\\nan equally valid ontology for the referenced table.\\nIf there are multiple ways to construct an ontology\\nbased on annotators\\u2019 decisions of attribute relation-\\nships among column headers, we manually unify\\nthe annotations for similar tables (for examples,\\ntables about athletes in different sports). The on-\\ntologies exhibit a great deal of structural variety.\\nRelevant statistics are summarized in Table 7 and\\nFigure 3 of the Appendix.\\nFigure 1: DART data collection pipeline. MR: Meaning Representation.\\nInput Unit\\nExamples\\nVocab Size\\nWords per SR\\nSents per SR\\nTables\\nWikiTableText\\nRow\\n13,318\\n\\u2014\\n13.9\\n1.0\\n4,962\\nLogicNLG\\nTable\\n37,015\\n122K\\n13.8\\n1.0\\n7,392\\nToTTo\\nHighlighted Cells\\n136,161\\n136K\\n17.4\\n1.0\\n83,141\\nDART\\nTriple Set\\n82,191\\n33.2K\\n21.6\\n1.5\\n5,623\\nTable 1: DART compared with other open-domain table-to-text datasets. DART takes triple sets as input by\\nincorporating the ontology of table headers and title, and its surface realizations tend to be longer with more than\\nsingle sentence verbalization. SR: Surface Realization.\\nDART: 62,659 train / 6,980 dev / 12,552 test\\nWikiTableQuestions\\nWikiSQL\\nWebNLG\\nCleaned E2E\\nInternal\\nMTurk\\nInternal\\nDeclarative\\nDomains\\nWikipedia (open-domain)\\n15 DBPedia Categories\\nRestaurants\\nUnique Predicates\\n1,950\\n1,403\\n493\\n2,008\\n347\\n7\\nUnique Triples\\n13,505\\n5,541\\n1,648\\n7,787\\n3,220\\n946\\nTripleset-Sentence Pairs\\n4,902\\n2,120\\n772\\n4,204\\n27,731\\n42,462\\nTriples per Tripleset (min, med, max)\\n1, 3, 10\\n1, 3, 7\\n1, 2, 7\\n1, 2, 10\\n1, 3, 7\\n1, 4, 7\\nVocab Size\\n13.4K\\n8.9K\\n3.0K\\n10.7K\\n8.0K\\n3.0K\\nWords per SR\\n15.2\\n16.5\\n14.0\\n12.6\\n22.5\\n22.9\\nSentences per SR\\n1.0\\n1.1\\n1.0\\n1.0\\n1.4\\n1.6\\nTable 2: Statistics of DART decomposed by different collection methods. DART exhibits a great deal of topical\\nvariety in terms of the number of unique predicates, the number of unique triples, and the vocabulary size.\\nConnected Component Extraction\\nAfter we\\nannotated the ontology, we automatically choose\\na subset of cells for a selected table row to form\\nthe triple set. Randomly selecting cells leads to\\npoor quality annotation as the selected data could\\nlack a subject, lack cohesion, or would require in-\\nformation not encoded in the ontology to form a\\ncoherent sentence. For example, in Figure 2, if only\\ntwo nodes CITY and CAPACITY were highlighted\\nthen a coherent sentence cannot be produced as\\nthere is no direct logical relationship (functional\\ndependency) between them. To solve these issues,\\ninstead of randomly selecting cells in a row, we\\nextract connected components from the ontology.\\nThe extracted components have two controllable\\nproperties: size and shape. To create variation in\\nsize, we randomly sampled between [2, 5]. The\\nshape is determined by two numbers: the number\\nof sibling node pairs and parent-child node pairs.\\nIncreasing the number of sibling node pairs creates\\na wider tree, while increasing the latter creates a\\ndeeper tree. We created a sliding scale between\\nwidth and depth using an expansion parameter, p.\\nWe recursively visit a node if it has children with\\nprobability p and otherwise move to a sibling if it\\nexists. If p = 1, the search becomes a DFS and if\\np = 0, it becomes BFS. We found that randomly\\nselecting p from 0.5 to 0.7 created a reasonable\\nvariation in extracted component shapes. This en-\\nsures the balance between breadth and depth of\\nontology coverage of the selected cells, therefore\\nensuring the quality of the sentence annotation.\\nSentence Annotation\\nGiven the table, title, and\\nconnected highlighted cells of a row, annotators\\nFigure 2: Overview of our human annotation procedure. Top panel: We collect the parent-child relations between\\ncolumns from internal annotators (yellow is parent, green is child). Then, we collect a surface realization of the\\ncells highlighted in orange. Middle panel: We use the provided parent-child relations to construct an ontology tree\\non the columns, then select the nodes corresponding to the highlighted cells. We gather a connected subtree by\\ncollecting all nodes leading up to the highlighted cells\\u2019 lowest common ancestor. Bottom panel: We extract a set of\\ntriples from the subtree as shown. This triple-set is paired with the provided realization to form a DART instance.\\nwere asked to write a description of the highlighted\\ncells. We encouraged the annotators to use di-\\nverse vocabulary and syntactic structures. To en-\\nsure quality, internal annotators reviewed every\\ncrowd sourced sentence for correctness. They ei-\\nther rewrote or discarded the sentences that were\\nnonsensical or incorrect. In some cases, they also\\nchanged cell highlighting patterns to match the sen-\\ntence provided.\\nBuild Tripleset-Sentence Pairs\\nFinally, we con-\\nvert the highlighted cells to triplesets. For a row R,\\nwe start with the table\\u2019s column ontology T. We\\n\\ufb01rst place the cell values in R in their correspond-\\ning slots in T, e.g. in Figure 2 we \\ufb01ll TEAM with\\n\\\"Amsterdam Admirals\\\". We then check that the\\nnodes of T corresponding to the highlighted cells\\nin R form a connected subtree. If not, we walk up\\nthe tree and highlight each traversed node up un-\\ntil the lowest common ancestor of the highlighted\\nnodes (inclusive) to form a connected subtree. For\\neach node N in the tree except the root node, we\\ncan extract the triple (parent (N), title (N), N).\\nFor example, since STADIUM is highlighted in Fig-\\nure 2, we extract the triple (Amsterdam Admirals,\\nSTADIUM, Olympisch Stadion). A small number\\nof triple-sets contained more than 10 triples. We\\ndiscarded these because their associated surface\\nrealizations were of poor quality. The numbers\\nof tripleset-sentence pairs annotated by different\\nannotators are shown in Table 2.\\n2.2\\nAutomatically Converting Questions to\\nDeclarative Sentences\\nHigh quality natural language questions in open\\ndomain semantic parsing datasets such as Wik-\\niSQL and QA2D techniques found in automati-\\ncally constructing NLI datasets (Demszky et al.,\\n2018) present themselves as an attractive opportu-\\nnity to semi-automatically construct an abundance\\nof declarative sentences and align to table cells. We\\nleveraged rule-based QA2D technique2 together\\nwith manual screening to combine WikiSQL ques-\\ntions and SQL-retrieved-answers into declarative\\nsentences and manually \\ufb01ltered out bad sentences.\\nWe only execute SQL queries without aggregate\\ncommands3 to retrieve answers corresponding to\\nquestions answerable by single rows. An example\\nof such conversion is as follows:\\n2We use the rule-based model from https://github.\\ncom/kelvinguu/qanli (Demszky et al., 2018). The neu-\\nral model code is not released.\\n3MAX, MIN, COUNT, SUM, AVG, JOIN, INTER-\\nSECT, UNION, GROUP BY, ORDER BY.\\nQuestion:\\nIn which year did Greece hold its\\nlast Summer Olympics?\\nAnswer: 2004\\nDeclarative Sentence: Greece held its last Summer\\nOlympics in 2004.\\nAlignment with table cells is done at two\\nstages. We \\ufb01rst align sentences with corresponding\\nrows by changing SQL commands to SELECT\\n* and use string matching to obtain columns\\nand column headers relevant to the answer and\\nWHERE condition. After manually \\ufb01ltering out\\nbad sentences, bad alignments, or tables without\\nontology annotations, we were able to get 4,204\\nsentences. Finally, the corresponding table cells\\nare then converted into triples in the same way as\\nwe described in Section 2.1.\\nExamples of produced declarative sentences can\\nbe found in Figure 10 of the Appendix.\\n2.3\\nIncorporating Existing Datasets\\nSince they provide a large amount of strictly\\naligned data-text pairs with high quality sentences,\\nwe incorporate the following existing datasets in\\nthe same \\u27e8triple-set, sentence\\u27e9 pair format with\\nsome modi\\ufb01cations.\\nWebNLG 2017\\nAn instance of the WebNLG\\ndataset contains a set of triples extracted from DB-\\npedia and the target text written by human. We\\ninclude the WebNLG 2017 dataset4 consisting of\\n27731 triple-set sentence pairs with up to 7 RDF\\ntriples in a triple set covering 15 domains.\\nCleaned E2E\\nThe original E2E dataset includes\\ndialogue act meaning representations (MR) and\\nnatural language references in the restaurant do-\\nmain. Later, Du\\u0161ek et al. (2019) provide Cleaned\\nE2E5 by automatically \\ufb01xing the dialogue acts to\\naccount for omissions and hallucinations in the\\ntext.\\nWe incorporate Cleaned E2E because of\\nits strict alignment between the meaning repre-\\nsentation and the text. To convert the MR to a\\ntriple-set, we take the NAME slot (present in al-\\nmost all the MRs) as the subject. For example,\\nthe MR (NAME[ALIMENTUM], AREA[CITY CEN-\\nTRE], FAMILYFRIENDLY[NO]) is converted to the\\n4https://gitlab.com/shimorina/\\nwebnlg-dataset/-/tree/master/webnlg_\\nchallenge_2017\\n5https://github.com/tuetschek/\\ne2e-cleaning\\ntriple-set {(ALIMENTUM, AREA, CITY CENTRE),\\n(ALIMENTUM, FAMILYFRIENDLY, NO)}. We drop\\nMRs which do not contain the NAME slot.\\n2.4\\nPredicate Uni\\ufb01cation\\nWe canonicalized the predicates in our triple sets\\nsuch that those of the same meaning are also repre-\\nsented the same. We manually constructed a predi-\\ncate mapping table to achieve this. As an example,\\nour predicate mapping maps \\\"Hometown,\\\" \\\"Home\\nTown,\\\" and \\\"Home Town/City\\\" to the uni\\ufb01ed pred-\\nicate \\\"HOMETOWN.\\\"\\nAfter unifying predicates, we evaluated the di-\\nversity of DART by counting the number of unique\\npredicates in its partitions. As shown in Table 2, we\\nsee that the Wikipedia partition of DART contains\\nmuch more unique predicates than the WebNLG\\nand Cleaned E2E partitions combined, despite hav-\\ning smaller number of \\u27e8triple-set, sentence\\u27e9 pairs.\\nThis contributes signi\\ufb01cantly to the domain di-\\nversity of DART. In addition, we can see that\\nDART exhibits a great deal of topical variety in\\nterms of number of unique triples and vocabulary\\nsize.\\n2.5\\nDataset Split\\nFor WebNLG 2017 and Cleaned E2E, we use their\\noriginal data splits. For our annotation on Wik-\\niTableQuestions and WikiSQL, random splitting\\nwill make train, dev, and test splits contain similar\\ntables and similar \\u27e8triple-set, sentence\\u27e9 examples.\\nTherefore, to increase the generalization challenge,\\nwe compare the table title and the table header to\\n\\ufb01nd similar tables, and make sure the model is eval-\\nuated on test split tables that are least similar to\\nthose used for training. We \\ufb01rst sample some ta-\\nbles as a seed test set, and then compute Jaccard\\nsimilarity6 with remaining tables based on the titles\\nand the headers. If a table has a Jaccard similarity\\ngreater than 0.5 with any of the tables in the test\\nset, we add it into the test set. A similar process\\nis repeated to create the dev set, and the remain-\\ning tables form the training set. This results in\\n62,659/6,980/12,552 sentences in the train/dev/test\\nsets, respectively.\\n3\\nExperimental Results\\nWe conduct experiments on DART and the\\nWebNLG 2017 dataset, with an ablation study on\\n6https://en.wikipedia.org/wiki/\\nJaccard_index\\nWebNLG to show the bene\\ufb01ts of using DART for\\ndata augmentation.\\n3.1\\nModels\\nWe investigate several state-of-the-art Data-to-Text\\ngeneration models. We report results of the fol-\\nlowing models on DART-testset: (1) Bidirectional-\\nLSTM with attention, for which we use 2-layer\\nbi-LSTM for encoder, with 300 dimensional word\\nembeddings (without using pretrained word vec-\\ntors), 512 hidden units and 0.3 dropout rate for the\\ndecoder. (2) Transformer (Vaswani et al., 2017),\\npreviously used by Castro Ferreira et al. (2019) on\\nthe WebNLG dataset. The input is formed by lin-\\nearizing the unordered triple set. (3) BART (Lewis\\net al., 2020), for which we report results of both\\nBART-base and BART-large. (4) T5 (Raffel et al.,\\n2020): we add the same pre\\ufb01x \\\"translate Graph to\\nEnglish:\\\" to the input, as it is used in Ribeiro et al.\\n(2020). We report results of T5-small, T5-base and\\nT5-large models. For both BART and T5 models,\\nwe use implementations of Ribeiro et al. (2020),\\nwith same hyperparameter setting.\\n3.2\\nEvaluation Metrics\\nWe use a variety of automatic metrics and human\\nevaluation (Section 4) to evaluate the quality of the\\ngenerated text. We report BLEU, METEOR, and\\nTER which are used in the of\\ufb01cial WebNLG chal-\\nlenge. However, these measures have limitations\\nin considering the semantic meanings of words or\\nphrases (Novikova et al., 2017a), therefore we also\\nreport MoverScore (Zhao et al., 2019), BERTScore\\n(Zhang et al., 2020), and BLEURT (Sellam et al.,\\n2020) that incorporate semantics rather than sur-\\nface forms using contextual embeddings. Further-\\nmore, we include PARENT (Dhingra et al., 2019)\\nwhich explicitly aligns n-grams from the reference\\nand generated text to the data contents.\\n3.3\\nResults\\nDART\\nOur experimental results on DART are\\nsummarized in Table 3. The T5-large model has\\nthe highest performance among all models with a\\nBLEU score of 50.66. We attribute this to T5\\u2019s gen-\\neralization and transfer learning ability due to pre-\\ntraining on multi-tasks. We can see that in general,\\npretrained models outperform others by a large\\nmargin, and increasing the model size seems to\\nfurther boost the performance on DART. However,\\nlanguage models such as BART and T5 are pre-\\ntrained by reconstructing text and, as a result, we\\nfound that their output on DART often contains\\nhallucinated words (Parikh et al., 2020; Harkous\\net al., 2020; Reiter, 2020), as shown in Figure 11.\\nIn addition, while the pretrained model shows bet-\\nter text generation quality due to its generalization\\nability from pretraining, it does not fully capture\\nthe hierarchical ontology nature of the triple sets\\nin their linearized input, therefore making DART\\nmore challenging. We suspect that models that\\nare better at exploiting the ontology structure pre-\\nserved in the input tripleset will achieve better per-\\nformance on DART.\\nWebNLG\\nFurthermore,\\nwe\\ninvestigate\\nif\\nDART can improve pretrained models\\u2019 perfor-\\nmance on other Data-to-Text generation tasks.\\nTo this end, we \\ufb01netune the baseline transformer\\nmodel, BART-[base, large] and T5-[small, base,\\nlarge] on the WebNLG 2017 dataset, and augment\\nthe training by adding instances in the DART train-\\ning set. The experimental results can be found in\\nTable 4. We report performances of some competi-\\ntive models that are not pretrained, as well as the\\nstate-of-the-art performances of pretrained models\\non the WebNLG 2017 dataset by Ribeiro et al.\\n(2020). On the bottom panel, we include results\\nof experiments augmented with DART instances\\nwhose triplesets are generated with table ontology\\nannotation, paired with human written sentences.\\nWe are able to achieve new state-of-the-art results\\non all WebNLG 2017 test set splits (seen, unseen\\nand all) by \\ufb01netuning T5-large on DART. We\\nobserve that using DART for data augmentation\\nconsistently improves the performance across all\\nmodels, including the baseline transformer model\\nthat is not pretrained. Furthermore, we observe\\nthat more improvement is shown on unseen split of\\nthe test set, due to DART\\u2019s open-domain nature.\\nSee Figure 12 of the Appendix for example model\\noutputs aligned with their human references.\\n3.4\\nAblation Study\\nWe also conduct an ablation study on the WebNLG\\ndataset to investigate what part of DART con-\\ntributes most to improving the Data-to-Text tasks\\nin general. We report results of the study in Table 6\\nof the Appendix. We divide DART into 4 partitions,\\nwhere declarative sentence (auto-generated) parti-\\ntion and human annotated sentence partition con-\\ntain instances whose triplesets are extracted from\\nWikipedia tables based on ontology. E2E parti-\\ntion contains instances converted from the E2E\\nBLEU \\u2191\\nMETEOR \\u2191\\nTER \\u2193\\nMoverScore \\u2191\\nBERTScore(F1) \\u2191\\nBLEURT \\u2191\\nPARENT \\u2191\\nLSTM with Attention\\n29.66\\n0.27\\n0.63\\n0.31\\n0.90\\n-0.13\\n0.35\\nEnd-to-End Transformer\\n27.24\\n0.25\\n0.65\\n0.25\\n0.89\\n-0.29\\n0.28\\nBART-base\\n47.11\\n0.38\\n0.46\\n0.51\\n0.95\\n0.37\\n0.55\\nBART-large\\n48.56\\n0.39\\n0.45\\n0.52\\n0.95\\n0.41\\n0.57\\nT5-small\\n47.69\\n0.39\\n0.46\\n0.52\\n0.95\\n0.40\\n0.56\\nT5-base\\n49.21\\n0.40\\n0.44\\n0.53\\n0.95\\n0.43\\n0.57\\nT5-large\\n50.66\\n0.40\\n0.43\\n0.54\\n0.95\\n0.44\\n0.58\\nTable 3: Model results on the test set of DART \\u2191: Higher is better. \\u2193: Lower is better.\\nBLEU \\u2191\\nMETEOR \\u2191\\nTER \\u2193\\nSEEN\\nUNSEEN\\nALL\\nSEEN\\nUNSEEN\\nALL\\nSEEN\\nUNSEEN\\nALL\\nPipeline Transformer\\u2020 (Castro Ferreira et al., 2019)\\n56.28\\n23.04\\n42.41\\n0.42\\n0.21\\n0.32\\n0.39\\n0.63\\n0.50\\nPipeline GRU\\u2020 (Castro Ferreira et al., 2019)\\n56.09\\n25.12\\n42.73\\n0.42\\n0.22\\n0.33\\n0.39\\n0.64\\n0.51\\nMELBOURNE (Gardent et al., 2017)\\n54.52\\n33.27\\n45.13\\n0.41\\n0.33\\n0.37\\n0.40\\n0.55\\n0.47\\nBestPlan \\u2020 (Moryossef et al., 2019)\\n53.30\\n34.41\\n47.24\\n0.44\\n0.34\\n0.39\\n0.47\\n0.56\\n0.51\\nDualEnc (Zhao et al., 2020)\\n63.45\\n36.73\\n51.42\\n0.46\\n0.37\\n0.41\\n0.34\\n0.55\\n0.44\\nPlanEnc (Zhao et al., 2020)\\n64.42\\n38.23\\n52.78\\n0.45\\n0.37\\n0.41\\n0.33\\n0.53\\n0.42\\nRibeiro et al. (2020)\\nBART-base \\u2021\\n63.02\\n41.74\\n53.36\\n0.45\\n0.35\\n0.40\\n0.33\\n0.52\\n0.42\\nBART-large \\u2021\\n63.71\\n44.17\\n54.95\\n0.46\\n0.39\\n0.42\\n0.33\\n0.51\\n0.41\\nT5-small \\u2021\\n65.30\\n45.58\\n56.57\\n0.46\\n0.39\\n0.43\\n0.32\\n0.49\\n0.40\\nT5-base \\u2021\\n64.89\\n52.86\\n59.44\\n0.46\\n0.42\\n0.44\\n0.33\\n0.42\\n0.37\\nT5-large \\u2021\\n64.89\\n54.01\\n59.95\\n0.46\\n0.43\\n0.44\\n0.34\\n0.41\\n0.37\\n+ DART\\nBART-base\\n62.36\\n46.21\\n55.14\\n0.44\\n0.37\\n0.41\\n0.34\\n0.45\\n0.39\\nBART-large\\n64.51\\n50.20\\n58.06\\n0.46\\n0.40\\n0.43\\n0.32\\n0.44\\n0.38\\nT5-small\\n65.05\\n47.81\\n57.32\\n0.46\\n0.40\\n0.43\\n0.33\\n0.46\\n0.39\\nT5-base\\n65.42\\n50.71\\n58.80\\n0.46\\n0.41\\n0.44\\n0.32\\n0.43\\n0.37\\nT5-large\\n65.82\\n56.01\\n61.44\\n0.46\\n0.43\\n0.45\\n0.32\\n0.38\\n0.35\\nTable 4: The WebNLG 2017 results on the test set.\\n\\u2020: We report results from Zhao et al. (2020) who use the\\nevaluation scripts that are strictly the same as the of\\ufb01cial challenge. \\u2021: We report results calculated with the model\\noutputs on the WebNLG 2017 testset released by Ribeiro et al. (2020).\\nTripleset source\\nSentence source\\n% \\ufb02uent\\n% faithful\\n% (\\ufb02uent+\\nmostly \\ufb02uent)\\n% (faithful+\\nmostly faithful)\\nWikiTableQuestions (\\u00a7 2.1)\\nhuman-written reference\\n75%\\n81%\\n96%\\n99%\\nBART-base\\n74%\\n57%\\n93%\\n84%\\nT5-base\\n72%\\n54%\\n94%\\n76%\\nWikiSQL (\\u00a7 2.2)\\nauto-generated reference\\n59%\\n56%\\n87%\\n88%\\nBART-base\\n66%\\n51%\\n92%\\n83%\\nT5-base\\n75%\\n65%\\n97%\\n90%\\nTable 5: Human evaluation over references and model outputs.\\ndataset, and WebNLG partition keeps the original\\ndata format. In general, we observe that adding\\nDART instances that contain human written sen-\\ntences brings most improvement, especially on un-\\nseen split. While adding E2E partition boosts the\\nscores on seen test split and deteriorates the perfor-\\nmance on unseen test split. This trend is consistent\\nacross all models. Comparing results of declarative\\nsentence partition and human written sentence par-\\ntition, we see that for most of the models, DART\\ninstances with human written sentences have better\\nquality as it brings more improvement to the task.\\n4\\nHuman Evaluation\\nIn Table 5, we perform human evaluation on\\nDART based on two criteria: (1) \\ufb02uency if a sen-\\ntence is natural and grammatical, and (2) semantic\\nfaithfulness if a sentence is supported by the input\\ntriples. We de\\ufb01ned three levels of \\ufb02uency: \\ufb02uent,\\nmostly \\ufb02uent, and not \\ufb02uent, and the same for se-\\nmantic faithfulness. We ask 5 internal annotators to\\nevaluate on 100 triplesets sampled from declarative\\nsentence partition and another 100 triplesets sam-\\npled from human written sentence partition. Each\\ntripleset is paired with 3 sentences, one of them\\nis the reference sentence, and the other two are\\noutputs of BART-base and T5-base models.\\nThe results in Table 5 attest to the high quality of\\nour annotations since the human written references\\nachieve highest \\ufb02uency and faithfulness comparing\\nto outputs of two strong baseline models. The eval-\\nuation on faithfulness also demonstrates that there\\nis a considerable gap between the DART reference\\nand the outputs of the state-of-the-art pretrained\\nmodel, showing that there is a large room for im-\\nprovement. We also noticed that the auto-generated\\ndeclarative sentences are not as \\ufb02uent or faithful\\nas the model outputs because they are generated\\nwith a rule-based system. However, we decided to\\nrelease this partition, along with other partitions of\\nDART because it demonstrates an economic way\\nto obtain large amounts of DART instances and it\\nalso shows bene\\ufb01ts for generalization due to the\\ndiverse topics it contains.\\n5\\nRelated Work\\nData-to-Text\\nData-to-Text generation aims to\\nproduce natural language output from structured\\ninput. Applications include generating sports com-\\nmentaries (Chen and Mooney, 2008; Wiseman\\net al., 2017), weather forecasts (Liang et al., 2009;\\nKonstas and Lapata, 2012), biographical texts (Le-\\nbret et al., 2016; Liu et al., 2018), knowledge-base\\ndescriptions (Gardent et al., 2017), dialogue re-\\nsponse generation (Wen et al., 2015, 2016), and\\ncommonsense reasoning (Lin et al., 2020). Yet,\\nmost existing datasets are restricted to speci\\ufb01c do-\\nmains and applications. In contrast, a major source\\nof DART is from Wikipedia tables covering various\\ndomains and topics.\\nRepresentation of Data\\nThe input of the Data-\\nto-Text datasets take different formats, including\\nslot-value pairs, Abstract Meaning Representa-\\ntion (AMR) (Song et al., 2017; Ribeiro et al.,\\n2019), Minimal Recursion Semantics (MRS) (Ha-\\njdik et al., 2019), Resource Description Framework\\n(RDF triples) (Gardent et al., 2017), and logic\\nforms (Chen et al., 2020b). There are also stud-\\nies of converting tabular data to RDF triples in the\\nSemantic Web community (Kellogg et al., 2015).\\nRecently, some open-domain table-to-text datasets\\nhave been proposed including WikiTableText (Bao\\net al., 2018), LogicNLP (Chen et al., 2020a), and\\nToTTo (Parikh et al., 2020), whose inputs are rows\\nor entire tables. In ToTTo, highlighted cells are\\nalso provided as input, and the authors found using\\nonly highlighted cells with \\ufb02at row and column\\nheaders led to higher performance than using the\\nentire table.\\nIn contrast, DART is constructed by \\ufb01rst annotat-\\ning the tree-structured table ontology that encodes\\nthe semantic dependencies among table headers,\\nand we could \\ufb02exibly incorporate additional con-\\ntexts such as the table title to the ontology tree.\\nWe then use an automatic procedure to extract con-\\nnected components from the tree to form the input\\nof a DART instance. Our annotation framework\\nnot only provides a \\ufb02exible way of incorporating\\nany contexts to the representation of tables, but\\nalso encodes hierarchical relationships among ta-\\nble headers and contexts, ensuring the extracted\\ntriples are logically consistent and can be described\\nin text without loss of information.\\nModel\\nTraditional Data-to-Text models break\\nthe generation progress into different stages such\\nas signal analysis, data interpretation, document\\nplanning, microplanning, and realization (Reiter\\nand Dale, 2000; Reiter, 2007).\\nRecently, neu-\\nral encoder-decoder models based on attention\\nand copy mechanisms have shown promising re-\\nsults (Gehrmann et al., 2018; Puduppully et al.,\\n2018, 2019; Castro Ferreira et al., 2019). Further-\\nmore, recent progress on pretrained models such\\nas GPT-2 (Radford et al., 2018), BART (Lewis\\net al., 2020) and T5 (Raffel et al., 2020) has shown\\neffective results for text generation tasks on ma-\\nchine translation, summarization, and conversation\\nresponse generation. Chen et al. (2020c); Peng\\net al. (2020); Kale (2020) also \\ufb01netune pretrained\\nmodels on Data-to-Text tasks.\\n6\\nConclusion\\nIn this paper, we introduce DART, an open-domain\\ncorpus for structured data record to text generation.\\nDART\\u2019s ontology-preserving representation of data\\ninputs differentiates itself from other open-domain\\nData-to-Text corpora. We found that DART in-\\ntroduces new challenges to several state-of-the-art\\nData-to-Text models due to its open-domain nature\\nand its ontology structure of the semantic triple\\ninput. Furthermore, we found that using it for data\\naugmentation improves other Data-to-Text tasks.\\nFor future work, we will explore more controlled,\\nhigh-\\ufb01delity generation that better incorporates the\\nontology hierarchy of data.\\n7\\nEthics Statement\\nOur dataset is constructed by accumulating and\\nprocessing resources from various existing datasets\\nthat are open to the public. In addition, we collect\\nannotations on structure of tabular data and human\\nwritten sentences that describe data records.\\nThe existing resources that we utilize mainly\\nconsist of (1) tabular data from Wikipedia, (2) in-\\nformation of restaurants presented with dialogue-\\nact meaning representation and its textual descrip-\\ntion (E2E), and (3) information of various entities\\nand their relationship that are in 15 different cate-\\ngories of DBPedia, which is a knowledge base built\\non contents created in various Wikimedia projects\\n(WebNLG). It is possible that there are biases in\\nthese resources, either in the tabular data or the\\ntextual description written by humans.\\nFor additional annotations we collected, we have\\ntwo groups of annotators participating: internal\\nannotators who are the authors of this work, and\\nexternal annotators recruited from the Amazon Me-\\nchanical Turk platform. On MTurk, we use a pay\\nrate of $15 per hour approximately based on our\\nestimation of the time it takes to complete our anno-\\ntation tasks. In total, it took 125 hours to complete\\nall tasks on the Amazon Mechanical Turk platform.\\nThere are three annotation tasks: (1) Annotators\\nare asked to specify ontological structure of the\\ntable by indicating relationship between table col-\\numn headers, (2) Annotators are asked to write\\ndescriptions that are \\ufb02uent and semantically faith-\\nful to the data records presented to them, and (3)\\nAnnotators are asked to evaluate sentences that are\\neither references or model generated outputs. We\\nacknowledge that it is also possible to have biases\\nin the sentences written by the annotators, or in the\\ndata records that are presented to them.\\nWe conducted experiments on our own dataset\\nand the WebNLG dataset using BART and T5, two\\nlarge-scale pretrained models. Both models are\\ntrained on large amounts of textual data such as\\nnews, books, and web text, which may contain any\\nkinds of biases. As a result, it is possible to insert\\nthose biases into the models.\\nIn total, we conducted 43 experiments: 7 on\\nDART and 36 for our ablation study on the\\nWebNLG dataset. We use a single NVIDIA V100\\nGPU for all experiments and each experiment took\\nfrom 5 to 40 hours depending on the model size.\\nAcknowledgement\\nThe authors would like to thank the anonymous\\nreviewers for their discussion and feedback.\\nReferences\\nJunwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuan-\\nhua Lv, Ming Zhou, and Tiejun Zhao. 2018. Table-\\nto-text: Describing table region with natural lan-\\nguage. In AAAI.\\nThiago Castro Ferreira, Chris van der Lee, Emiel van\\nMiltenburg, and Emiel Krahmer. 2019. Neural data-\\nto-text generation: A comparison between pipeline\\nand end-to-end architectures. In EMNLP.\\nAlison J Cawsey, Bonnie L Webber, and Ray B Jones.\\n1997. Natural language generation in health care.\\nDavid L Chen and Raymond J Mooney. 2008. Learn-\\ning to sportscast: a test of grounded language acqui-\\nsition. In ICML.\\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\\nWilliam Yang Wang. 2020a.\\nLogical natural lan-\\nguage generation from open-domain tables. In ACL.\\nZhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou\\nZhou, Yunkai Zhang, Sairam Sundaresan, and\\nWilliam Yang Wang. 2020b.\\nLogic2Text: High-\\n\\ufb01delity natural language generation from logical\\nforms. In Findings of EMNLP.\\nZhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu,\\nand William Yang Wang. 2020c. Few-shot nlg with\\npre-trained language model. In ACL.\\nDorottya Demszky, Kelvin Guu, and Percy Liang. 2018.\\nTransforming question answering datasets into nat-\\nural language inference datasets.\\narXiv preprint\\narXiv:1809.02922.\\nBhuwan Dhingra, Manaal Faruqui, Ankur Parikh,\\nMing-Wei Chang, Dipanjan Das, and William Co-\\nhen. 2019. Handling divergent reference texts when\\nevaluating table-to-text generation. In ACL.\\nOnd\\u02c7rej Du\\u0161ek, David M. Howcroft, and Verena Rieser.\\n2019. Semantic noise matters for neural natural lan-\\nguage generation. In INLG.\\nHady Elsahar, Pavlos Vougiouklis, Arslen Remaci,\\nChristophe Gravier,\\nJonathon Hare,\\nFrederique\\nLaforest, and Elena Simperl. 2018. T-REx: A large\\nscale alignment of natural language with knowledge\\nbase triples. In LREC.\\nAngela Fan, Claire Gardent, Chlo\\u00e9 Braud, and An-\\ntoine Bordes. 2019. Using local knowledge graph\\nconstruction to scale Seq2Seq models to multi-\\ndocument inputs. In EMNLP-IJCNLP.\\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\\nand Laura Perez-Beltrachini. 2017. The WebNLG\\nchallenge: Generating text from RDF data. In INLG.\\nSebastian Gehrmann, Falcon Dai, Henry Elder, and\\nAlexander Rush. 2018. End-to-end content and plan\\nselection for data-to-text generation. In INLG.\\nValerie Hajdik, Jan Buys, Michael Wayne Goodman,\\nand Emily M Bender. 2019. Neural text generation\\nfrom rich semantic representations. In NAACL.\\nHamza Harkous, Isabel Groves, and Amir Saffari. 2020.\\nHave your text and use it too! end-to-end neural\\ndata-to-text generation with semantic \\ufb01delity. arXiv\\npreprint arXiv:2004.06577.\\nMihir Kale. 2020. Text-to-text pre-training for data-to-\\ntext tasks. arXiv preprint arXiv:2005.10433.\\nGregg Kellogg, Ivan Herman, and Jeremy Tandy.\\n2015.\\nGenerating\\nRDF\\nfrom\\ntabular\\ndata\\non the web.\\nW3C recommendation,\\nW3C.\\nHttps://www.w3.org/TR/2015/REC-csv2rdf-\\n20151217/.\\nIoannis Konstas and Mirella Lapata. 2012. Unsuper-\\nvised concept-to-text generation with hypergraphs.\\nIn NAACL.\\nR\\u00e9mi Lebret, David Grangier, and Michael Auli. 2016.\\nNeural text generation from structured data with ap-\\nplication to the biography domain. In EMNLP.\\nMike\\nLewis,\\nYinhan\\nLiu,\\nNaman\\nGoyal,\\nMar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy,\\nVes\\nStoyanov,\\nand\\nLuke\\nZettlemoyer.\\n2020. BART: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation,\\nand comprehension. In ACL.\\nPercy Liang, Michael I Jordan, and Dan Klein. 2009.\\nLearning semantic correspondences with less super-\\nvision. In ACL.\\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\\nRen. 2020. CommonGen: A constrained text gener-\\nation challenge for generative commonsense reason-\\ning. In Findings of EMNLP.\\nTianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,\\nand Zhifang Sui. 2018. Table-to-text generation by\\nstructure-aware seq2seq learning. In AAAI.\\nAmit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.\\nStep-by-step: Separating planning from realization\\nin neural data-to-text generation. In NAACL.\\nJekaterina Novikova, Ond\\u02c7rej Du\\u0161ek, Amanda Cercas\\nCurry, and Verena Rieser. 2017a. Why we need new\\nevaluation metrics for nlg. In EMNLP.\\nJekaterina Novikova, Ondrej Dusek, and Verena Rieser.\\n2017b. The E2E dataset: New challenges for end-to-\\nend generation. In SIGDIAL.\\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann,\\nManaal Faruqui, Bhuwan Dhingra, Diyi Yang, and\\nDipanjan Das. 2020. ToTTo: A controlled table-to-\\ntext generation dataset. In EMNLP.\\nPanupong Pasupat and Percy Liang. 2015. Composi-\\ntional semantic parsing on semi-structured tables. In\\nACL.\\nBaolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun\\nLi, Jinchao Li, Michael Zeng, and Jianfeng Gao.\\n2020.\\nFew-shot natural language generation for\\ntask-oriented dialog. In arXiv.\\nRatish Puduppully, Li Dong, and Mirella Lapata. 2018.\\nData-to-text generation with content selection and\\nplanning. In AAAI.\\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019.\\nData-to-text generation with entity modeling.\\nIn\\nACL.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\\nIlya Sutskever. 2018.\\nImproving language under-\\nstanding by generative pre-training.\\nTechnical re-\\nport, OpenAI.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2020. Exploring the lim-\\nits of transfer learning with a uni\\ufb01ed text-to-text\\ntransformer. Journal of Machine Learning Research,\\n21(140):1\\u201367.\\nEhud Reiter. 2007.\\nAn architecture for data-to-text\\nsystems. In Proceedings of the Eleventh European\\nWorkshop on Natural Language Generation (ENLG\\n07).\\nEhud Reiter. 2020. Openai gpt system: What does it\\ndo? Technical report, Arria.\\nEhud Reiter and Robert Dale. 2000. Building natural\\nlanguage generation systems. Cambridge university\\npress.\\nLeonardo F. R. Ribeiro, Claire Gardent, and Iryna\\nGurevych. 2019.\\nEnhancing AMR-to-text genera-\\ntion with dual graph representations. In EMNLP.\\nLeonardo F. R. Ribeiro, Martin Schmitt, Hinrich\\nSch\\u00fctze, and Iryna Gurevych. 2020. Investigating\\npretrained language models for graph-to-text gener-\\nation. arXiv.\\nThibault Sellam, Dipanjan Das, and Ankur P Parikh.\\n2020.\\nBLEURT: Learning robust metrics for text\\ngeneration. In ACL.\\nLinfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo\\nWang, and Daniel Gildea. 2017. Amr-to-text gener-\\nation with synchronous node replacement grammar.\\nIn ACL.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, \\u0141ukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In NeurIPS.\\nPavlos Vougiouklis,\\nHady ElSahar,\\nLucie-Aim\\u00e9e\\nKaffee, Christophe Gravier, Fr\\u00e9d\\u00e9rique Laforest,\\nJonathon S. Hare, and Elena Simperl. 2018. Neu-\\nral wikipedian: Generating textual summaries from\\nknowledge base triples. Journal of Web Semantics,\\n52-53:1 \\u2013 15.\\nTsung-Hsien Wen,\\nMilica Ga\\u0161i\\u00b4c,\\nNikola Mrk\\u0161i\\u00b4c,\\nLina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes,\\nDavid Vandyke, and Steve Young. 2016.\\nCondi-\\ntional generation and snapshot learning in neural di-\\nalogue systems. In EMNLP.\\nTsung-Hsien Wen, Milica Ga\\u0161i\\u00b4c, Nikola Mrk\\u0161i\\u00b4c, Pei-\\nHao Su, David Vandyke, and Steve Young. 2015.\\nSemantically conditioned LSTM-based natural lan-\\nguage generation for spoken dialogue systems. In\\nEMNLP.\\nSam Wiseman, Stuart Shieber, and Alexander Rush.\\n2017.\\nChallenges in data-to-document generation.\\nIn EMNLP.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\\nWeinberger, and Yoav Artzi. 2020.\\nBERTScore:\\nEvaluating text generation with BERT. In ICLR.\\nChao Zhao, Marilyn Walker, and Snigdha Chaturvedi.\\n2020. Bridging the structural gap between encoding\\nand decoding for data-to-text generation. In ACL.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\\ntian M Meyer, and Steffen Eger. 2019. MoverScore:\\nText generation evaluating with contextualized em-\\nbeddings and earth mover distance. In EMNLP.\\nVictor Zhong, Caiming Xiong, and Richard Socher.\\n2017.\\nSeq2sql:\\nGenerating structured queries\\nfrom natural language using reinforcement learning.\\nCoRR, abs/1709.00103.\\nAppendix\\nThe Appendix contains the following contents:\\n\\u2022 Results of the ablation study on WebNLG 2017 testset.\\n\\u2022 Statistics of the table ontology annotations.\\n\\u2022 Examples of tables that help illustrate DART\\u2019s annotation procedure.\\n\\u2022 Examples of model outputs.\\nModel\\nExperiment\\nBLEU \\u2191\\nMETEOR \\u2191\\nTER \\u2193\\nSEEN\\nUNSEEN\\nALL\\nSEEN\\nUNSEEN\\nALL\\nSEEN\\nUNSEEN\\nALL\\nBaseline\\nTransformer\\n[1] webnlg\\n49.81\\n5.51\\n31.81\\n0.39\\n0.09\\n0.24\\n0.47\\n0.86\\n0.64\\n[2] webnlg+dart_decl_sents\\n52.31\\n8.96\\n39.98\\n0.40\\n0.07\\n0.25\\n0.45\\n0.79\\n0.60\\n[3] webnlg+dart_human_annotated\\n53.68\\n7.02\\n36.36\\n0.40\\n0.09\\n0.26\\n0.43\\n0.79\\n0.59\\n[4] webnlg+dart_ontology\\n53.40\\n8.54\\n38.51\\n0.41\\n0.08\\n0.26\\n0.44\\n0.80\\n0.60\\n[5] webnlg+dart_e2e\\n51.76\\n5.92\\n32.36\\n0.40\\n0.09\\n0.25\\n0.45\\n0.86\\n0.63\\n[6] webnlg+dart_full\\n54.99\\n8.64\\n39.11\\n0.40\\n0.08\\n0.25\\n0.42\\n0.81\\n0.60\\nBART-base\\n[1] webnlg\\n63.02\\n41.74\\n53.36\\n0.45\\n0.35\\n0.40\\n0.33\\n0.52\\n0.42\\n[2] webnlg+dart_decl_sents\\n62.71\\n42.51\\n53.64\\n0.45\\n0.36\\n0.40\\n0.34\\n0.51\\n0.41\\n[3] webnlg+dart_human_annotated\\n62.36\\n46.21\\n55.14\\n0.44\\n0.37\\n0.41\\n0.34\\n0.45\\n0.39\\n[4] webnlg+dart_ontology\\n62.62\\n46.74\\n55.54\\n0.44\\n0.38\\n0.41\\n0.34\\n0.45\\n0.39\\n[5] webnlg+dart_e2e\\n64.00\\n35.07\\n51.17\\n0.45\\n0.33\\n0.40\\n0.33\\n0.61\\n0.46\\n[6] webnlg+dart_full\\n63.66\\n45.48\\n55.52\\n0.45\\n0.37\\n0.41\\n0.33\\n0.47\\n0.40\\nBART-large\\n[1] webnlg\\n63.71\\n44.17\\n54.95\\n0.46\\n0.39\\n0.42\\n0.33\\n0.51\\n0.41\\n[2] webnlg+dart_decl_sents\\n65.18\\n46.79\\n56.79\\n0.46\\n0.39\\n0.42\\n0.32\\n0.48\\n0.40\\n[3] webnlg+dart_human_annotated\\n64.51\\n50.20\\n58.06\\n0.46\\n0.40\\n0.43\\n0.32\\n0.44\\n0.38\\n[4] webnlg+dart_ontology\\n64.19\\n49.62\\n57.65\\n0.46\\n0.39\\n0.43\\n0.33\\n0.45\\n0.38\\n[5] webnlg+dart_e2e\\n65.06\\n30.17\\n48.24\\n0.46\\n0.33\\n0.40\\n0.32\\n0.69\\n0.49\\n[6] webnlg+dart_full\\n65.24\\n47.96\\n57.44\\n0.46\\n0.39\\n0.43\\n0.32\\n0.46\\n0.39\\nT5-small\\n[1] webnlg\\n65.30\\n45.58\\n56.57\\n0.46\\n0.39\\n0.43\\n0.32\\n0.49\\n0.40\\n[2] webnlg+dart_decl_sents\\n64.18\\n46.61\\n56.27\\n0.46\\n0.39\\n0.43\\n0.33\\n0.48\\n0.40\\n[3] webnlg+dart_human_annotated\\n65.05\\n47.81\\n57.32\\n0.46\\n0.40\\n0.43\\n0.33\\n0.46\\n0.39\\n[4] webnlg+dart_ontology\\n65.17\\n47.49\\n57.24\\n0.46\\n0.39\\n0.43\\n0.32\\n0.47\\n0.39\\n[5] webnlg+dart_e2e\\n65.56\\n41.28\\n54.56\\n0.46\\n0.38\\n0.42\\n0.32\\n0.54\\n0.42\\n[6] webnlg+dart_full\\n64.70\\n47.56\\n57.01\\n0.46\\n0.39\\n0.43\\n0.33\\n0.47\\n0.39\\nT5-base\\n[1] webnlg\\n64.89\\n52.86\\n59.44\\n0.46\\n0.42\\n0.44\\n0.33\\n0.42\\n0.37\\n[2] webnlg+dart_decl_sents\\n65.44\\n50.80\\n58.81\\n0.46\\n0.41\\n0.44\\n0.32\\n0.43\\n0.37\\n[3] webnlg+dart_human_annotated\\n65.42\\n50.71\\n58.80\\n0.46\\n0.41\\n0.44\\n0.32\\n0.43\\n0.37\\n[4] webnlg+dart_ontology\\n65.17\\n51.49\\n59.04\\n0.46\\n0.41\\n0.44\\n0.33\\n0.43\\n0.37\\n[5] webnlg+dart_e2e\\n65.11\\n49.64\\n58.19\\n0.46\\n0.41\\n0.44\\n0.33\\n0.46\\n0.39\\n[6] webnlg+dart_full\\n65.99\\n51.68\\n59.50\\n0.46\\n0.42\\n0.44\\n0.32\\n0.43\\n0.37\\nT5-large\\n[1] webnlg\\n64.89\\n54.01\\n59.95\\n0.46\\n0.43\\n0.44\\n0.34\\n0.41\\n0.37\\n[2] webnlg+dart_decl_sents\\n65.97\\n53.00\\n60.12\\n0.46\\n0.42\\n0.44\\n0.32\\n0.41\\n0.36\\n[3] webnlg+dart_human_annotated\\n65.82\\n56.01\\n61.44\\n0.46\\n0.43\\n0.45\\n0.32\\n0.38\\n0.35\\n[4] webnlg+dart_ontology\\n65.53\\n55.20\\n60.90\\n0.46\\n0.42\\n0.44\\n0.32\\n0.38\\n0.35\\n[5] webnlg+dart_e2e\\n66.27\\n54.13\\n60.76\\n0.46\\n0.43\\n0.45\\n0.32\\n0.41\\n0.36\\n[6] webnlg+dart_full\\n65.78\\n54.35\\n60.64\\n0.46\\n0.42\\n0.44\\n0.32\\n0.39\\n0.35\\nTable 6: Results of ablation study on WebNLG 2017 testset. dart_decl_sents refers to DART partition that contains\\nauto-generated declarative sentences mentioned in Section 2.2, dart_human_annotated refers to partition that\\ncontains human written sentences mentioned in Section 2.1, dart_ontology is the combination of dart_decl_sents\\nand dart_human_annotated, and dart_e2e refers to DART partition containing instances extracted from E2E\\ndataset, the process of which is mentioned in Section 2.3. Note that dart_full is the combination of dart_ontology\\nand dart_e2e.\\nTables\\nOntology depth\\n(min, med, max)\\nNodes in ontology\\n(min, med, max)\\nBranching factor\\n(mean)\\nWikiTableQuestions\\n2060\\n1, 1, 4\\n2, 6, 25\\n4.0\\nWikiSQL\\n3563\\n1, 1, 4\\n3, 7, 25\\n5.1\\nTable 7: Properties of the ontology in the WikiTableQuestions and WikiSQL samples in DART. Branching factor\\nrefers to the average number of children across all non-leaf nodes in a table\\u2019s ontology.\\nFigure 3:\\nDistribution of column ontology depths in the WikiTableQuestions and WikiSQL samples in\\nDART v1.1.1.\\n<entry category=\\\"MISC\\\" eid=\\\"Id5\\\" size=\\\"3\\\">\\n<modifiedtripleset>\\n<mtriple>Apertura 2006 | JORNADA_OR_OTHER | Semifinals Ida</mtriple>\\n<mtriple>Semifinals Ida | AWAY_TEAM | Am\\u00e9rica</mtriple>\\n<mtriple>Semifinals Ida | HOME_TEAM | Chivas</mtriple>\\n</modifiedtripleset>\\n<lex comment=\\\"WikiTableQuestions\\\" lid=\\\"Id1\\\">\\nChivas and Am\\u00e9rica will compete in the semifinals of the Apertura 2006 tournament.\\n</lex>\\n</entry>\\n<entry category=\\\"MISC\\\" eid=\\\"Id76\\\" size=\\\"6\\\">\\n<modifiedtripleset>\\n<mtriple>Terry Jenkins | ROUND | 1st Round</mtriple>\\n<mtriple>Terry Jenkins | YEAR | 2014</mtriple>\\n<mtriple>[TABLECONTEXT] | [TITLE] | PDC World Darts Championship</mtriple>\\n<mtriple>1st Round | OPPONENT | Per Laursen</mtriple>\\n<mtriple>1st Round | RESULT | Lost</mtriple>\\n<mtriple>[TABLECONTEXT] | PLAYER | Terry Jenkins</mtriple>\\n</modifiedtripleset>\\n<lex comment=\\\"WikiTableQuestions\\\" lid=\\\"Id1\\\">\\nTerry Jenkins lost the game with Per Laursen in\\nthe 1st Round of 2014 PDC World Darts Championship\\n</lex>\\n</entry>\\nFigure 4: Examples of DART instance\\nFigure 5: An example of the data cleaning. The top left table had a missing column name and the table title was\\nnot speci\\ufb01c to the data; our internal annotators add the missing column name \\u201cYear\\u201d and linked the rest of the\\ncolumns to the \\u201cYear\\u201d column. The bottom left table had repeat column names in the table; our internal annotators\\ndisambiguate the columns by making the column names more speci\\ufb01c.\\nFigure 6: A WikiTableQuestions table that uses [TITLE] in the ontology.\\nFigure 7: A manually annotated table from WikiTableQuestions with a sentence that uses the table title.\\nFigure 8: A manually annotated table from WikiTableQuestions. Annotators created a table ontology, and they\\nwrote sentences encapsulating the information in the orange cells for a given row. Whenever a sentence referenced\\nthe table title, that sentence was also highlighted green.\\nFigure 9: An example of collected MTurk-generated sentences for WikiTableQuestions. Internal annotators went\\nthrough the generated sentences and checked for both sentence coherence and title usage. Below the generated\\nsentences, \\u2018y\\u2019 meant the sentence references the table title, \\u2018n\\u2019 meant the sentence did not use the table title, \\u2018x\\u2019\\nmeant the sentence was nonsensical.\\nFigure 10: Automatically generated declarative sentences from WikiSQL with human validation. Annotators went\\nthrough the generated sentences and checked for both sentence coherence and title use. Below the generated\\nsentences, \\u2018y\\u2019 meant the sentence references the table title, \\u2018n\\u2019 meant the sentence did not use the table title, \\u2018x\\u2019\\nmeant the sentence was nonsensical.\\n- Sample 1 -\\nInput triples:\\n<H> Peru Earthquake <R> scale of disaster <T> 250k homeless\\n<H> Peru Earthquake <R> year <T> 2007\\nBART-base output: 250k people were killed in the 2007 philippine earthquake .\\n- Sample 2 -\\nInput triples:\\n<H> [TABLECONTEXT] <R> game <T> 3\\n<H> 3 <R> attendance <T> 10 637\\n<H> [TABLECONTEXT] <R> [title] <T> 2006 Minnesota Swarm season\\nBART-base output: the minnesota swarm played in front of a crowd of 10 , 684 people .\\n- Sample 3 -\\nInput triples:\\n<H> Andrew Phelps McCormick <R> state <T> TX\\n<H> Andrew Phelps McCormick <R> active <T> 1892-1916\\nT5-base output: andrew phelps mccormick was active from 1892 to 1616 in texas .\\nFigure 11: Examples of hallucinated outputs of pretrained models trained on DART\\n- Sample 1 -\\nInput triples:\\n<H> Andrew Rayel <R> associated Band/associated Musical Artist <T> Christian Burns\\n<H> Andrew Rayel <R> associated Band/associated Musical Artist <T> Jonathan Mendelsohn\\nreference:\\nandrew rayel , is associated with musical artist jonathan mendelsohn and christian burns .\\ntrain on WebNLG - BART-base output:\\nchristian mendelsohn and andrew rayel are both associated with the same band , christian burns .\\ntrain on DART - BART-base output:\\nandrew rayel is associated with christian burns and jonathan mendelsohn .\\n- Sample 2 -\\nInput triples:\\n<H> Indie rock <R> stylistic Origin <T> New wave music\\nreference: the stylistic origin of indie rock is new wave music .\\ntrain on WebNLG - BART-base output:\\nthe alternative rock genre is new wave .\\ntrain on DART - BART-base output:\\nindie rock is influenced by new wave music .\\n- Sample 3 -\\nInput triples:\\n<H> Abradab <R> associated Band/associated Musical Artist <T> Magik rapper\\n<H> Abradab <R> associated Band/associated Musical Artist <T> Kaliber 44\\nreference:\\nabradab , an artist for the band kaliber 44 , is associated with magik ( rapper ) .\\ntrain on WebNLG - BART-base output:\\nmagiber 44 is the creator of abradab , which is also associated with the magik rapper .\\ntrain on DART - BART-base output:\\nmagik rapper and kaliber 44 are the associated musicians of abradab .\\n- Sample 4 -\\nInput triples:\\n<H> Alfa Romeo 164 <R> assembly <T> Milan\\n<H> Alfa Romeo 164 <R> related Mean Of Transportation <T> Saab 9000\\nreference:\\nthe alfa romeo 164 , which is assembled in milan , is a related means of transportation to saab 9000 ,\\nin that they are both cars .\\ntrain on WebNLG - T5-base output:\\nalfa romeo 164 is a transport vehicle for saab 9000 and is found in milan .\\ntrain on DART - T5-base output:\\nalfa romeo 164 ( assembled in milan ) is a related transport vehicle to saab 9000 .\\n- Sample 5 -\\nInput triples:\\n<H> Akeem Ayers <R> former Team <T> Tennessee Titans\\n<H> Akeem Ayers <R> draft Pick <T> 39\\nreference:\\nakeem ayers \\u2019 former team was tennessee titans and he was number 39 in the draft pick .\\ntrain on WebNLG - T5-large output:\\nakeem ayers was drafted with the 39th pick by the tennessee titans .\\ntrain on DART - T5-large output:\\nakeem ayers , a former player of the tennessee titans , was the 39th draft pick .\\nFigure 12: Examples of model outputs - with or without DART data augmentation\\n\", \"extraction_date\": \"2023-07-16 18:40:50.440010\", \"num_pages\": 16}\n"
     ]
    }
   ],
   "source": [
    "def read_text_file(filename):\n",
    "    json_content = pathlib.Path(filename).read_bytes()\n",
    "    print(len(json_content))\n",
    "    return json_content\n",
    "\n",
    "doc_string = read_text_file(doc_path) #.decode(encoding = 'utf-8')\n",
    "doc_string = doc_string.decode(encoding = 'utf-8')\n",
    "print(doc_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = json.loads(doc_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '2007.02871.pdf',\n",
       " 'text': 'DART: Open-Domain Structured Data Record to Text Generation\\nLinyong Nan1\\nDragomir Radev1,2\\nRui Zhang3\\nAmrit Rau1\\nAbhinand Sivaprasad1\\nChiachun Hsieh4\\nXiangru Tang1\\nAadit Vyas1\\nNeha Verma1\\nPranav Krishna5\\nYangxiaokang Liu1\\nNadia Irwanto1\\nJessica Pan1\\nFaiaz Rahman1\\nAhmad Zaidi1\\nMurori Mutuma1\\nYasin Tarabar1\\nAnkit Gupta1\\nTao Yu1\\nYi Chern Tan1\\nXi Victoria Lin2∗\\nCaiming Xiong2\\nRichard Socher2\\nNazneen Fatema Rajani2\\n1 Yale University\\n2 Salesforce Research\\n3 Penn State University\\n4 The University of Hong Kong\\n5 MIT\\n{linyong.nan, dragomir.radev}@yale.edu, rmz5227@psu.edu, nazneen.rajani@salesforce.com\\nAbstract\\nWe present DART, an open domain structured\\nDAta Record to Text generation dataset with\\nover 82k instances (DARTs). Data-to-Text an-\\nnotations can be a costly process, especially\\nwhen dealing with tables which are the ma-\\njor source of structured data and contain non-\\ntrivial structures. To this end, we propose a\\nprocedure of extracting semantic triples from\\ntables that encodes their structures by exploit-\\ning the semantic dependencies among table\\nheaders and the table title. Our dataset con-\\nstruction framework effectively merged hetero-\\ngeneous sources from open domain semantic\\nparsing and dialogue-act-based meaning rep-\\nresentation tasks by utilizing techniques such\\nas: tree ontology annotation, question-answer\\npair to declarative sentence conversion, and\\npredicate uniﬁcation, all with minimum post-\\nediting. We present systematic evaluation on\\nDART as well as new state-of-the-art results\\non WebNLG 2017 to show that DART (1)\\nposes new challenges to existing data-to-text\\ndatasets and (2) facilitates out-of-domain gen-\\neralization. Our data and code can be found\\nat https://github.com/Yale-LILY/\\ndart.\\n1\\nIntroduction\\nAutomatically generating textual descriptions from\\nstructured data improves the accessibility of knowl-\\nedge bases to lay users. Such applications include\\nexplaining data records to non-experts (Cawsey\\net al., 1997), writing sports news (Chen and\\nMooney, 2008), summarizing information in mul-\\ntiple documents (Fan et al., 2019), and generating\\ndialogue responses (Wen et al., 2015).\\nWhile signiﬁcant progress has been made in this\\nﬁeld, there are still several issues with existing\\nData-to-Text datasets. First, they adopt a ﬂat ontol-\\nogy structure of the data, such as slot-value pairs\\nfor data records (Lebret et al., 2016; Novikova et al.,\\n∗Now at Facebook AI.\\n2017b) or ﬂat schema for tables (Wiseman et al.,\\n2017; Chen et al., 2020a; Parikh et al., 2020). This\\nﬂat structure is not powerful enough to encode rich\\nsemantic relationships in the ontology of the struc-\\ntured data, especially tables, whose representation\\ncan be further improved with these semantic knowl-\\nedge. Second, some of the datasets only focus on\\na small number of domains or knowledge graphs,\\ntherefore providing limited number of predicates\\nand data ontologies. For example, E2E (Novikova\\net al., 2017b) on restaurants and WebNLG (Gar-\\ndent et al., 2017) on 15 categories from DBPedia.\\nFurthermore, some of them only have loose align-\\nments between data input and sentence due to the\\nnature of the task (Wiseman et al., 2017) and the\\nautomatic generation procedure (Vougiouklis et al.,\\n2018; Elsahar et al., 2018).\\nTo address some of these issues and to encour-\\nage further research in natural language generation\\nfrom structured data, we introduce DART, a large\\nand open-domain structured DAta-Record-to-Text\\ngeneration corpus. The goal of DART is to har-\\nvest the diverse predicates occurred in Wikipedia\\ntables, which is signiﬁcantly richer than those de-\\nﬁned in the domain speciﬁc ontologies E2E and\\nWebNLG were built on (Table 2). We also in-\\ntroduce novel tree ontology annotation on tables,\\nwhich converts a ﬂat table schema into a tree struc-\\ntured semantic frame. The tree ontology reﬂects\\nthe core and auxiliary relations in the table schema,\\nand naturally occurs across many domains. As a\\nresult, DART provides high-quality sentence an-\\nnotations to tree structured semantic frames ex-\\ntracted from various data sources, including Wik-\\niSQL (Zhong et al., 2017) and WikiTableQuestions\\n(Pasupat and Liang, 2015), two open-domain ques-\\ntion answering datasets, as well as E2E (Novikova\\net al., 2017b) and WebNLG (Gardent et al., 2017)\\n(Figure 1). We evaluated several state-of-the-art\\ndata-to-text models on DART, and found that while\\nthese models achieve impressive performance on\\narXiv:2007.02871v2  [cs.CL]  12 Apr 2021\\ndomain-speciﬁc datasets, their performance suffers\\non DART due to its open-domain nature and richer\\nsemantic structures.\\nOur contributions are as follows. (1) We present\\na large and open-domain corpus for structured data\\nrecord to text generation, annotated with tree on-\\ntologies converted from the table. This hierarchical\\ninput differentiates our corpus from existing data-\\nto-text corpora. (2) We benchmark several state-\\nof-the-art data-to-text models to show that DART\\nintroduces new generalization challenges. (3) We\\ndemonstrate that using DART for data augmenta-\\ntion improves the performance of existing models\\non the WebNLG 2017 dataset. We expect the re-\\nsults to generalize to other data-to-text datasets\\ngiven the open-domain nature of DART.\\n2\\nDART Data Collection\\nAs shown in Figure 1, DART is constructed from\\nthree different sources: (1) human annotation on\\nWikipedia tables from two table semantic parsing\\nand question answering datasets WikiSQL and Wik-\\niTableQuestions (§ 2.1), (2) automatic conversion\\nof questions in WikiSQL to declarative sentences\\n(§ 2.2), and (3) incorporation of existing datasets\\nincluding WebNLG 2017 and Cleaned E2E (§ 2.3).\\nAfter collecting the ⟨triple-set, sentence⟩ pairs from\\nvarious data sources, we manually canonicalized\\nthe predicates and show that DART covers a broad\\nrange of topics (§ 2.4). Finally, we discuss the data\\nsplit in § 2.5.\\n2.1\\nTree Ontology and Sentence Annotation\\non Tables\\nTables are a major source of structured data that\\ncontain a wealth of information complementary\\nto text and knowledge graphs.\\nWe aim to col-\\nlect ⟨triple-set, sentence⟩ pairs from open-domain\\nWikipedia tables.\\nHowever, table schema are\\nﬂat, making them not directly usable for building\\nsubject-predicate-object triples to capture rich rela-\\ntionships in the data.\\nAs shown in Figure 2, we propose a two-stage an-\\nnotation process that involves two groups of anno-\\ntators: internal annotators and Amazon Mechanical\\nTurk1 workers. In the ﬁrst stage, skilled internal an-\\nnotators specify the parent of every column header\\nto construct a tree-structured ontology for each ta-\\nble. In the second stage, both internal and external\\nannotators provide a sentential description of the\\n1https://www.mturk.com/\\nhighlighted cells in a row that are automatically-\\nchosen based on the ontology.\\nTree Ontology Annotation\\nFor each column in\\na given table, our internal annotators labeled its\\nontological parent. In Figure 2, for example, the an-\\nnotator would provide the sequence {NULL, TEAM,\\nSTADIUM, STADIUM, TEAM} as the parent of each\\ncolumn — column TEAM has no parent, STADIUM\\nhas parent TEAM, and so on. In many cases, the\\nrelationship between a parent column and its child\\ncolumn can be conceptualized as a \"has-a\" relation-\\nship. For tables that are malformed or have dupli-\\ncate or missing column names (as shown in Figure\\n5 of the Appendix), annotators either changed or\\nadded appropriate column names in order to ﬁt\\nthese patterns. For each table we generate an ontol-\\nogy tree whose root is always [TABLECONTEXT].\\nThis root node either has (1) one child node [TI-\\nTLE] in the cases where the table title is the subject\\nof entire table, or (2) column header node(s) and\\na [TITLE] node as children, as shown in Figure 2.\\nThis is because in some tables, the table title itself\\nis more appropriate to be the root of the ontology\\ntree (example shown in Figure 6 of the Appendix).\\nIn these cases, annotators assigned the special to-\\nken [TITLE] as the parent of the relevant column\\nnodes. For other tables, title usually provides im-\\nportant context for understanding the table’s rows\\n(example shown in Figure 7 of the Appendix). In\\nsuch cases, [TITLE] is made a child of [TABLE-\\nCONTEXT] together with the column headers that\\nare appropriate.\\nWe evaluate the quality of the initial tree on-\\ntology annotation and made corrections with the\\nfollowing procedure: (1) reject and request correc-\\ntions from the original annotators if the provided\\nontology is disconnected or contains a cycle, (2)\\nverify that all column headers appear as a node in\\nthe tree. For many tables, the determination of an\\nontology is a subjective process with many \"cor-\\nrect\" answers - for example, swapping the positions\\nof TEAM and CITY in the tree in Figure 2 produces\\nan equally valid ontology for the referenced table.\\nIf there are multiple ways to construct an ontology\\nbased on annotators’ decisions of attribute relation-\\nships among column headers, we manually unify\\nthe annotations for similar tables (for examples,\\ntables about athletes in different sports). The on-\\ntologies exhibit a great deal of structural variety.\\nRelevant statistics are summarized in Table 7 and\\nFigure 3 of the Appendix.\\nFigure 1: DART data collection pipeline. MR: Meaning Representation.\\nInput Unit\\nExamples\\nVocab Size\\nWords per SR\\nSents per SR\\nTables\\nWikiTableText\\nRow\\n13,318\\n—\\n13.9\\n1.0\\n4,962\\nLogicNLG\\nTable\\n37,015\\n122K\\n13.8\\n1.0\\n7,392\\nToTTo\\nHighlighted Cells\\n136,161\\n136K\\n17.4\\n1.0\\n83,141\\nDART\\nTriple Set\\n82,191\\n33.2K\\n21.6\\n1.5\\n5,623\\nTable 1: DART compared with other open-domain table-to-text datasets. DART takes triple sets as input by\\nincorporating the ontology of table headers and title, and its surface realizations tend to be longer with more than\\nsingle sentence verbalization. SR: Surface Realization.\\nDART: 62,659 train / 6,980 dev / 12,552 test\\nWikiTableQuestions\\nWikiSQL\\nWebNLG\\nCleaned E2E\\nInternal\\nMTurk\\nInternal\\nDeclarative\\nDomains\\nWikipedia (open-domain)\\n15 DBPedia Categories\\nRestaurants\\nUnique Predicates\\n1,950\\n1,403\\n493\\n2,008\\n347\\n7\\nUnique Triples\\n13,505\\n5,541\\n1,648\\n7,787\\n3,220\\n946\\nTripleset-Sentence Pairs\\n4,902\\n2,120\\n772\\n4,204\\n27,731\\n42,462\\nTriples per Tripleset (min, med, max)\\n1, 3, 10\\n1, 3, 7\\n1, 2, 7\\n1, 2, 10\\n1, 3, 7\\n1, 4, 7\\nVocab Size\\n13.4K\\n8.9K\\n3.0K\\n10.7K\\n8.0K\\n3.0K\\nWords per SR\\n15.2\\n16.5\\n14.0\\n12.6\\n22.5\\n22.9\\nSentences per SR\\n1.0\\n1.1\\n1.0\\n1.0\\n1.4\\n1.6\\nTable 2: Statistics of DART decomposed by different collection methods. DART exhibits a great deal of topical\\nvariety in terms of the number of unique predicates, the number of unique triples, and the vocabulary size.\\nConnected Component Extraction\\nAfter we\\nannotated the ontology, we automatically choose\\na subset of cells for a selected table row to form\\nthe triple set. Randomly selecting cells leads to\\npoor quality annotation as the selected data could\\nlack a subject, lack cohesion, or would require in-\\nformation not encoded in the ontology to form a\\ncoherent sentence. For example, in Figure 2, if only\\ntwo nodes CITY and CAPACITY were highlighted\\nthen a coherent sentence cannot be produced as\\nthere is no direct logical relationship (functional\\ndependency) between them. To solve these issues,\\ninstead of randomly selecting cells in a row, we\\nextract connected components from the ontology.\\nThe extracted components have two controllable\\nproperties: size and shape. To create variation in\\nsize, we randomly sampled between [2, 5]. The\\nshape is determined by two numbers: the number\\nof sibling node pairs and parent-child node pairs.\\nIncreasing the number of sibling node pairs creates\\na wider tree, while increasing the latter creates a\\ndeeper tree. We created a sliding scale between\\nwidth and depth using an expansion parameter, p.\\nWe recursively visit a node if it has children with\\nprobability p and otherwise move to a sibling if it\\nexists. If p = 1, the search becomes a DFS and if\\np = 0, it becomes BFS. We found that randomly\\nselecting p from 0.5 to 0.7 created a reasonable\\nvariation in extracted component shapes. This en-\\nsures the balance between breadth and depth of\\nontology coverage of the selected cells, therefore\\nensuring the quality of the sentence annotation.\\nSentence Annotation\\nGiven the table, title, and\\nconnected highlighted cells of a row, annotators\\nFigure 2: Overview of our human annotation procedure. Top panel: We collect the parent-child relations between\\ncolumns from internal annotators (yellow is parent, green is child). Then, we collect a surface realization of the\\ncells highlighted in orange. Middle panel: We use the provided parent-child relations to construct an ontology tree\\non the columns, then select the nodes corresponding to the highlighted cells. We gather a connected subtree by\\ncollecting all nodes leading up to the highlighted cells’ lowest common ancestor. Bottom panel: We extract a set of\\ntriples from the subtree as shown. This triple-set is paired with the provided realization to form a DART instance.\\nwere asked to write a description of the highlighted\\ncells. We encouraged the annotators to use di-\\nverse vocabulary and syntactic structures. To en-\\nsure quality, internal annotators reviewed every\\ncrowd sourced sentence for correctness. They ei-\\nther rewrote or discarded the sentences that were\\nnonsensical or incorrect. In some cases, they also\\nchanged cell highlighting patterns to match the sen-\\ntence provided.\\nBuild Tripleset-Sentence Pairs\\nFinally, we con-\\nvert the highlighted cells to triplesets. For a row R,\\nwe start with the table’s column ontology T. We\\nﬁrst place the cell values in R in their correspond-\\ning slots in T, e.g. in Figure 2 we ﬁll TEAM with\\n\"Amsterdam Admirals\". We then check that the\\nnodes of T corresponding to the highlighted cells\\nin R form a connected subtree. If not, we walk up\\nthe tree and highlight each traversed node up un-\\ntil the lowest common ancestor of the highlighted\\nnodes (inclusive) to form a connected subtree. For\\neach node N in the tree except the root node, we\\ncan extract the triple (parent (N), title (N), N).\\nFor example, since STADIUM is highlighted in Fig-\\nure 2, we extract the triple (Amsterdam Admirals,\\nSTADIUM, Olympisch Stadion). A small number\\nof triple-sets contained more than 10 triples. We\\ndiscarded these because their associated surface\\nrealizations were of poor quality. The numbers\\nof tripleset-sentence pairs annotated by different\\nannotators are shown in Table 2.\\n2.2\\nAutomatically Converting Questions to\\nDeclarative Sentences\\nHigh quality natural language questions in open\\ndomain semantic parsing datasets such as Wik-\\niSQL and QA2D techniques found in automati-\\ncally constructing NLI datasets (Demszky et al.,\\n2018) present themselves as an attractive opportu-\\nnity to semi-automatically construct an abundance\\nof declarative sentences and align to table cells. We\\nleveraged rule-based QA2D technique2 together\\nwith manual screening to combine WikiSQL ques-\\ntions and SQL-retrieved-answers into declarative\\nsentences and manually ﬁltered out bad sentences.\\nWe only execute SQL queries without aggregate\\ncommands3 to retrieve answers corresponding to\\nquestions answerable by single rows. An example\\nof such conversion is as follows:\\n2We use the rule-based model from https://github.\\ncom/kelvinguu/qanli (Demszky et al., 2018). The neu-\\nral model code is not released.\\n3MAX, MIN, COUNT, SUM, AVG, JOIN, INTER-\\nSECT, UNION, GROUP BY, ORDER BY.\\nQuestion:\\nIn which year did Greece hold its\\nlast Summer Olympics?\\nAnswer: 2004\\nDeclarative Sentence: Greece held its last Summer\\nOlympics in 2004.\\nAlignment with table cells is done at two\\nstages. We ﬁrst align sentences with corresponding\\nrows by changing SQL commands to SELECT\\n* and use string matching to obtain columns\\nand column headers relevant to the answer and\\nWHERE condition. After manually ﬁltering out\\nbad sentences, bad alignments, or tables without\\nontology annotations, we were able to get 4,204\\nsentences. Finally, the corresponding table cells\\nare then converted into triples in the same way as\\nwe described in Section 2.1.\\nExamples of produced declarative sentences can\\nbe found in Figure 10 of the Appendix.\\n2.3\\nIncorporating Existing Datasets\\nSince they provide a large amount of strictly\\naligned data-text pairs with high quality sentences,\\nwe incorporate the following existing datasets in\\nthe same ⟨triple-set, sentence⟩ pair format with\\nsome modiﬁcations.\\nWebNLG 2017\\nAn instance of the WebNLG\\ndataset contains a set of triples extracted from DB-\\npedia and the target text written by human. We\\ninclude the WebNLG 2017 dataset4 consisting of\\n27731 triple-set sentence pairs with up to 7 RDF\\ntriples in a triple set covering 15 domains.\\nCleaned E2E\\nThe original E2E dataset includes\\ndialogue act meaning representations (MR) and\\nnatural language references in the restaurant do-\\nmain. Later, Dušek et al. (2019) provide Cleaned\\nE2E5 by automatically ﬁxing the dialogue acts to\\naccount for omissions and hallucinations in the\\ntext.\\nWe incorporate Cleaned E2E because of\\nits strict alignment between the meaning repre-\\nsentation and the text. To convert the MR to a\\ntriple-set, we take the NAME slot (present in al-\\nmost all the MRs) as the subject. For example,\\nthe MR (NAME[ALIMENTUM], AREA[CITY CEN-\\nTRE], FAMILYFRIENDLY[NO]) is converted to the\\n4https://gitlab.com/shimorina/\\nwebnlg-dataset/-/tree/master/webnlg_\\nchallenge_2017\\n5https://github.com/tuetschek/\\ne2e-cleaning\\ntriple-set {(ALIMENTUM, AREA, CITY CENTRE),\\n(ALIMENTUM, FAMILYFRIENDLY, NO)}. We drop\\nMRs which do not contain the NAME slot.\\n2.4\\nPredicate Uniﬁcation\\nWe canonicalized the predicates in our triple sets\\nsuch that those of the same meaning are also repre-\\nsented the same. We manually constructed a predi-\\ncate mapping table to achieve this. As an example,\\nour predicate mapping maps \"Hometown,\" \"Home\\nTown,\" and \"Home Town/City\" to the uniﬁed pred-\\nicate \"HOMETOWN.\"\\nAfter unifying predicates, we evaluated the di-\\nversity of DART by counting the number of unique\\npredicates in its partitions. As shown in Table 2, we\\nsee that the Wikipedia partition of DART contains\\nmuch more unique predicates than the WebNLG\\nand Cleaned E2E partitions combined, despite hav-\\ning smaller number of ⟨triple-set, sentence⟩ pairs.\\nThis contributes signiﬁcantly to the domain di-\\nversity of DART. In addition, we can see that\\nDART exhibits a great deal of topical variety in\\nterms of number of unique triples and vocabulary\\nsize.\\n2.5\\nDataset Split\\nFor WebNLG 2017 and Cleaned E2E, we use their\\noriginal data splits. For our annotation on Wik-\\niTableQuestions and WikiSQL, random splitting\\nwill make train, dev, and test splits contain similar\\ntables and similar ⟨triple-set, sentence⟩ examples.\\nTherefore, to increase the generalization challenge,\\nwe compare the table title and the table header to\\nﬁnd similar tables, and make sure the model is eval-\\nuated on test split tables that are least similar to\\nthose used for training. We ﬁrst sample some ta-\\nbles as a seed test set, and then compute Jaccard\\nsimilarity6 with remaining tables based on the titles\\nand the headers. If a table has a Jaccard similarity\\ngreater than 0.5 with any of the tables in the test\\nset, we add it into the test set. A similar process\\nis repeated to create the dev set, and the remain-\\ning tables form the training set. This results in\\n62,659/6,980/12,552 sentences in the train/dev/test\\nsets, respectively.\\n3\\nExperimental Results\\nWe conduct experiments on DART and the\\nWebNLG 2017 dataset, with an ablation study on\\n6https://en.wikipedia.org/wiki/\\nJaccard_index\\nWebNLG to show the beneﬁts of using DART for\\ndata augmentation.\\n3.1\\nModels\\nWe investigate several state-of-the-art Data-to-Text\\ngeneration models. We report results of the fol-\\nlowing models on DART-testset: (1) Bidirectional-\\nLSTM with attention, for which we use 2-layer\\nbi-LSTM for encoder, with 300 dimensional word\\nembeddings (without using pretrained word vec-\\ntors), 512 hidden units and 0.3 dropout rate for the\\ndecoder. (2) Transformer (Vaswani et al., 2017),\\npreviously used by Castro Ferreira et al. (2019) on\\nthe WebNLG dataset. The input is formed by lin-\\nearizing the unordered triple set. (3) BART (Lewis\\net al., 2020), for which we report results of both\\nBART-base and BART-large. (4) T5 (Raffel et al.,\\n2020): we add the same preﬁx \"translate Graph to\\nEnglish:\" to the input, as it is used in Ribeiro et al.\\n(2020). We report results of T5-small, T5-base and\\nT5-large models. For both BART and T5 models,\\nwe use implementations of Ribeiro et al. (2020),\\nwith same hyperparameter setting.\\n3.2\\nEvaluation Metrics\\nWe use a variety of automatic metrics and human\\nevaluation (Section 4) to evaluate the quality of the\\ngenerated text. We report BLEU, METEOR, and\\nTER which are used in the ofﬁcial WebNLG chal-\\nlenge. However, these measures have limitations\\nin considering the semantic meanings of words or\\nphrases (Novikova et al., 2017a), therefore we also\\nreport MoverScore (Zhao et al., 2019), BERTScore\\n(Zhang et al., 2020), and BLEURT (Sellam et al.,\\n2020) that incorporate semantics rather than sur-\\nface forms using contextual embeddings. Further-\\nmore, we include PARENT (Dhingra et al., 2019)\\nwhich explicitly aligns n-grams from the reference\\nand generated text to the data contents.\\n3.3\\nResults\\nDART\\nOur experimental results on DART are\\nsummarized in Table 3. The T5-large model has\\nthe highest performance among all models with a\\nBLEU score of 50.66. We attribute this to T5’s gen-\\neralization and transfer learning ability due to pre-\\ntraining on multi-tasks. We can see that in general,\\npretrained models outperform others by a large\\nmargin, and increasing the model size seems to\\nfurther boost the performance on DART. However,\\nlanguage models such as BART and T5 are pre-\\ntrained by reconstructing text and, as a result, we\\nfound that their output on DART often contains\\nhallucinated words (Parikh et al., 2020; Harkous\\net al., 2020; Reiter, 2020), as shown in Figure 11.\\nIn addition, while the pretrained model shows bet-\\nter text generation quality due to its generalization\\nability from pretraining, it does not fully capture\\nthe hierarchical ontology nature of the triple sets\\nin their linearized input, therefore making DART\\nmore challenging. We suspect that models that\\nare better at exploiting the ontology structure pre-\\nserved in the input tripleset will achieve better per-\\nformance on DART.\\nWebNLG\\nFurthermore,\\nwe\\ninvestigate\\nif\\nDART can improve pretrained models’ perfor-\\nmance on other Data-to-Text generation tasks.\\nTo this end, we ﬁnetune the baseline transformer\\nmodel, BART-[base, large] and T5-[small, base,\\nlarge] on the WebNLG 2017 dataset, and augment\\nthe training by adding instances in the DART train-\\ning set. The experimental results can be found in\\nTable 4. We report performances of some competi-\\ntive models that are not pretrained, as well as the\\nstate-of-the-art performances of pretrained models\\non the WebNLG 2017 dataset by Ribeiro et al.\\n(2020). On the bottom panel, we include results\\nof experiments augmented with DART instances\\nwhose triplesets are generated with table ontology\\nannotation, paired with human written sentences.\\nWe are able to achieve new state-of-the-art results\\non all WebNLG 2017 test set splits (seen, unseen\\nand all) by ﬁnetuning T5-large on DART. We\\nobserve that using DART for data augmentation\\nconsistently improves the performance across all\\nmodels, including the baseline transformer model\\nthat is not pretrained. Furthermore, we observe\\nthat more improvement is shown on unseen split of\\nthe test set, due to DART’s open-domain nature.\\nSee Figure 12 of the Appendix for example model\\noutputs aligned with their human references.\\n3.4\\nAblation Study\\nWe also conduct an ablation study on the WebNLG\\ndataset to investigate what part of DART con-\\ntributes most to improving the Data-to-Text tasks\\nin general. We report results of the study in Table 6\\nof the Appendix. We divide DART into 4 partitions,\\nwhere declarative sentence (auto-generated) parti-\\ntion and human annotated sentence partition con-\\ntain instances whose triplesets are extracted from\\nWikipedia tables based on ontology. E2E parti-\\ntion contains instances converted from the E2E\\nBLEU ↑\\nMETEOR ↑\\nTER ↓\\nMoverScore ↑\\nBERTScore(F1) ↑\\nBLEURT ↑\\nPARENT ↑\\nLSTM with Attention\\n29.66\\n0.27\\n0.63\\n0.31\\n0.90\\n-0.13\\n0.35\\nEnd-to-End Transformer\\n27.24\\n0.25\\n0.65\\n0.25\\n0.89\\n-0.29\\n0.28\\nBART-base\\n47.11\\n0.38\\n0.46\\n0.51\\n0.95\\n0.37\\n0.55\\nBART-large\\n48.56\\n0.39\\n0.45\\n0.52\\n0.95\\n0.41\\n0.57\\nT5-small\\n47.69\\n0.39\\n0.46\\n0.52\\n0.95\\n0.40\\n0.56\\nT5-base\\n49.21\\n0.40\\n0.44\\n0.53\\n0.95\\n0.43\\n0.57\\nT5-large\\n50.66\\n0.40\\n0.43\\n0.54\\n0.95\\n0.44\\n0.58\\nTable 3: Model results on the test set of DART ↑: Higher is better. ↓: Lower is better.\\nBLEU ↑\\nMETEOR ↑\\nTER ↓\\nSEEN\\nUNSEEN\\nALL\\nSEEN\\nUNSEEN\\nALL\\nSEEN\\nUNSEEN\\nALL\\nPipeline Transformer† (Castro Ferreira et al., 2019)\\n56.28\\n23.04\\n42.41\\n0.42\\n0.21\\n0.32\\n0.39\\n0.63\\n0.50\\nPipeline GRU† (Castro Ferreira et al., 2019)\\n56.09\\n25.12\\n42.73\\n0.42\\n0.22\\n0.33\\n0.39\\n0.64\\n0.51\\nMELBOURNE (Gardent et al., 2017)\\n54.52\\n33.27\\n45.13\\n0.41\\n0.33\\n0.37\\n0.40\\n0.55\\n0.47\\nBestPlan † (Moryossef et al., 2019)\\n53.30\\n34.41\\n47.24\\n0.44\\n0.34\\n0.39\\n0.47\\n0.56\\n0.51\\nDualEnc (Zhao et al., 2020)\\n63.45\\n36.73\\n51.42\\n0.46\\n0.37\\n0.41\\n0.34\\n0.55\\n0.44\\nPlanEnc (Zhao et al., 2020)\\n64.42\\n38.23\\n52.78\\n0.45\\n0.37\\n0.41\\n0.33\\n0.53\\n0.42\\nRibeiro et al. (2020)\\nBART-base ‡\\n63.02\\n41.74\\n53.36\\n0.45\\n0.35\\n0.40\\n0.33\\n0.52\\n0.42\\nBART-large ‡\\n63.71\\n44.17\\n54.95\\n0.46\\n0.39\\n0.42\\n0.33\\n0.51\\n0.41\\nT5-small ‡\\n65.30\\n45.58\\n56.57\\n0.46\\n0.39\\n0.43\\n0.32\\n0.49\\n0.40\\nT5-base ‡\\n64.89\\n52.86\\n59.44\\n0.46\\n0.42\\n0.44\\n0.33\\n0.42\\n0.37\\nT5-large ‡\\n64.89\\n54.01\\n59.95\\n0.46\\n0.43\\n0.44\\n0.34\\n0.41\\n0.37\\n+ DART\\nBART-base\\n62.36\\n46.21\\n55.14\\n0.44\\n0.37\\n0.41\\n0.34\\n0.45\\n0.39\\nBART-large\\n64.51\\n50.20\\n58.06\\n0.46\\n0.40\\n0.43\\n0.32\\n0.44\\n0.38\\nT5-small\\n65.05\\n47.81\\n57.32\\n0.46\\n0.40\\n0.43\\n0.33\\n0.46\\n0.39\\nT5-base\\n65.42\\n50.71\\n58.80\\n0.46\\n0.41\\n0.44\\n0.32\\n0.43\\n0.37\\nT5-large\\n65.82\\n56.01\\n61.44\\n0.46\\n0.43\\n0.45\\n0.32\\n0.38\\n0.35\\nTable 4: The WebNLG 2017 results on the test set.\\n†: We report results from Zhao et al. (2020) who use the\\nevaluation scripts that are strictly the same as the ofﬁcial challenge. ‡: We report results calculated with the model\\noutputs on the WebNLG 2017 testset released by Ribeiro et al. (2020).\\nTripleset source\\nSentence source\\n% ﬂuent\\n% faithful\\n% (ﬂuent+\\nmostly ﬂuent)\\n% (faithful+\\nmostly faithful)\\nWikiTableQuestions (§ 2.1)\\nhuman-written reference\\n75%\\n81%\\n96%\\n99%\\nBART-base\\n74%\\n57%\\n93%\\n84%\\nT5-base\\n72%\\n54%\\n94%\\n76%\\nWikiSQL (§ 2.2)\\nauto-generated reference\\n59%\\n56%\\n87%\\n88%\\nBART-base\\n66%\\n51%\\n92%\\n83%\\nT5-base\\n75%\\n65%\\n97%\\n90%\\nTable 5: Human evaluation over references and model outputs.\\ndataset, and WebNLG partition keeps the original\\ndata format. In general, we observe that adding\\nDART instances that contain human written sen-\\ntences brings most improvement, especially on un-\\nseen split. While adding E2E partition boosts the\\nscores on seen test split and deteriorates the perfor-\\nmance on unseen test split. This trend is consistent\\nacross all models. Comparing results of declarative\\nsentence partition and human written sentence par-\\ntition, we see that for most of the models, DART\\ninstances with human written sentences have better\\nquality as it brings more improvement to the task.\\n4\\nHuman Evaluation\\nIn Table 5, we perform human evaluation on\\nDART based on two criteria: (1) ﬂuency if a sen-\\ntence is natural and grammatical, and (2) semantic\\nfaithfulness if a sentence is supported by the input\\ntriples. We deﬁned three levels of ﬂuency: ﬂuent,\\nmostly ﬂuent, and not ﬂuent, and the same for se-\\nmantic faithfulness. We ask 5 internal annotators to\\nevaluate on 100 triplesets sampled from declarative\\nsentence partition and another 100 triplesets sam-\\npled from human written sentence partition. Each\\ntripleset is paired with 3 sentences, one of them\\nis the reference sentence, and the other two are\\noutputs of BART-base and T5-base models.\\nThe results in Table 5 attest to the high quality of\\nour annotations since the human written references\\nachieve highest ﬂuency and faithfulness comparing\\nto outputs of two strong baseline models. The eval-\\nuation on faithfulness also demonstrates that there\\nis a considerable gap between the DART reference\\nand the outputs of the state-of-the-art pretrained\\nmodel, showing that there is a large room for im-\\nprovement. We also noticed that the auto-generated\\ndeclarative sentences are not as ﬂuent or faithful\\nas the model outputs because they are generated\\nwith a rule-based system. However, we decided to\\nrelease this partition, along with other partitions of\\nDART because it demonstrates an economic way\\nto obtain large amounts of DART instances and it\\nalso shows beneﬁts for generalization due to the\\ndiverse topics it contains.\\n5\\nRelated Work\\nData-to-Text\\nData-to-Text generation aims to\\nproduce natural language output from structured\\ninput. Applications include generating sports com-\\nmentaries (Chen and Mooney, 2008; Wiseman\\net al., 2017), weather forecasts (Liang et al., 2009;\\nKonstas and Lapata, 2012), biographical texts (Le-\\nbret et al., 2016; Liu et al., 2018), knowledge-base\\ndescriptions (Gardent et al., 2017), dialogue re-\\nsponse generation (Wen et al., 2015, 2016), and\\ncommonsense reasoning (Lin et al., 2020). Yet,\\nmost existing datasets are restricted to speciﬁc do-\\nmains and applications. In contrast, a major source\\nof DART is from Wikipedia tables covering various\\ndomains and topics.\\nRepresentation of Data\\nThe input of the Data-\\nto-Text datasets take different formats, including\\nslot-value pairs, Abstract Meaning Representa-\\ntion (AMR) (Song et al., 2017; Ribeiro et al.,\\n2019), Minimal Recursion Semantics (MRS) (Ha-\\njdik et al., 2019), Resource Description Framework\\n(RDF triples) (Gardent et al., 2017), and logic\\nforms (Chen et al., 2020b). There are also stud-\\nies of converting tabular data to RDF triples in the\\nSemantic Web community (Kellogg et al., 2015).\\nRecently, some open-domain table-to-text datasets\\nhave been proposed including WikiTableText (Bao\\net al., 2018), LogicNLP (Chen et al., 2020a), and\\nToTTo (Parikh et al., 2020), whose inputs are rows\\nor entire tables. In ToTTo, highlighted cells are\\nalso provided as input, and the authors found using\\nonly highlighted cells with ﬂat row and column\\nheaders led to higher performance than using the\\nentire table.\\nIn contrast, DART is constructed by ﬁrst annotat-\\ning the tree-structured table ontology that encodes\\nthe semantic dependencies among table headers,\\nand we could ﬂexibly incorporate additional con-\\ntexts such as the table title to the ontology tree.\\nWe then use an automatic procedure to extract con-\\nnected components from the tree to form the input\\nof a DART instance. Our annotation framework\\nnot only provides a ﬂexible way of incorporating\\nany contexts to the representation of tables, but\\nalso encodes hierarchical relationships among ta-\\nble headers and contexts, ensuring the extracted\\ntriples are logically consistent and can be described\\nin text without loss of information.\\nModel\\nTraditional Data-to-Text models break\\nthe generation progress into different stages such\\nas signal analysis, data interpretation, document\\nplanning, microplanning, and realization (Reiter\\nand Dale, 2000; Reiter, 2007).\\nRecently, neu-\\nral encoder-decoder models based on attention\\nand copy mechanisms have shown promising re-\\nsults (Gehrmann et al., 2018; Puduppully et al.,\\n2018, 2019; Castro Ferreira et al., 2019). Further-\\nmore, recent progress on pretrained models such\\nas GPT-2 (Radford et al., 2018), BART (Lewis\\net al., 2020) and T5 (Raffel et al., 2020) has shown\\neffective results for text generation tasks on ma-\\nchine translation, summarization, and conversation\\nresponse generation. Chen et al. (2020c); Peng\\net al. (2020); Kale (2020) also ﬁnetune pretrained\\nmodels on Data-to-Text tasks.\\n6\\nConclusion\\nIn this paper, we introduce DART, an open-domain\\ncorpus for structured data record to text generation.\\nDART’s ontology-preserving representation of data\\ninputs differentiates itself from other open-domain\\nData-to-Text corpora. We found that DART in-\\ntroduces new challenges to several state-of-the-art\\nData-to-Text models due to its open-domain nature\\nand its ontology structure of the semantic triple\\ninput. Furthermore, we found that using it for data\\naugmentation improves other Data-to-Text tasks.\\nFor future work, we will explore more controlled,\\nhigh-ﬁdelity generation that better incorporates the\\nontology hierarchy of data.\\n7\\nEthics Statement\\nOur dataset is constructed by accumulating and\\nprocessing resources from various existing datasets\\nthat are open to the public. In addition, we collect\\nannotations on structure of tabular data and human\\nwritten sentences that describe data records.\\nThe existing resources that we utilize mainly\\nconsist of (1) tabular data from Wikipedia, (2) in-\\nformation of restaurants presented with dialogue-\\nact meaning representation and its textual descrip-\\ntion (E2E), and (3) information of various entities\\nand their relationship that are in 15 different cate-\\ngories of DBPedia, which is a knowledge base built\\non contents created in various Wikimedia projects\\n(WebNLG). It is possible that there are biases in\\nthese resources, either in the tabular data or the\\ntextual description written by humans.\\nFor additional annotations we collected, we have\\ntwo groups of annotators participating: internal\\nannotators who are the authors of this work, and\\nexternal annotators recruited from the Amazon Me-\\nchanical Turk platform. On MTurk, we use a pay\\nrate of $15 per hour approximately based on our\\nestimation of the time it takes to complete our anno-\\ntation tasks. In total, it took 125 hours to complete\\nall tasks on the Amazon Mechanical Turk platform.\\nThere are three annotation tasks: (1) Annotators\\nare asked to specify ontological structure of the\\ntable by indicating relationship between table col-\\numn headers, (2) Annotators are asked to write\\ndescriptions that are ﬂuent and semantically faith-\\nful to the data records presented to them, and (3)\\nAnnotators are asked to evaluate sentences that are\\neither references or model generated outputs. We\\nacknowledge that it is also possible to have biases\\nin the sentences written by the annotators, or in the\\ndata records that are presented to them.\\nWe conducted experiments on our own dataset\\nand the WebNLG dataset using BART and T5, two\\nlarge-scale pretrained models. Both models are\\ntrained on large amounts of textual data such as\\nnews, books, and web text, which may contain any\\nkinds of biases. As a result, it is possible to insert\\nthose biases into the models.\\nIn total, we conducted 43 experiments: 7 on\\nDART and 36 for our ablation study on the\\nWebNLG dataset. We use a single NVIDIA V100\\nGPU for all experiments and each experiment took\\nfrom 5 to 40 hours depending on the model size.\\nAcknowledgement\\nThe authors would like to thank the anonymous\\nreviewers for their discussion and feedback.\\nReferences\\nJunwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuan-\\nhua Lv, Ming Zhou, and Tiejun Zhao. 2018. Table-\\nto-text: Describing table region with natural lan-\\nguage. In AAAI.\\nThiago Castro Ferreira, Chris van der Lee, Emiel van\\nMiltenburg, and Emiel Krahmer. 2019. Neural data-\\nto-text generation: A comparison between pipeline\\nand end-to-end architectures. In EMNLP.\\nAlison J Cawsey, Bonnie L Webber, and Ray B Jones.\\n1997. Natural language generation in health care.\\nDavid L Chen and Raymond J Mooney. 2008. Learn-\\ning to sportscast: a test of grounded language acqui-\\nsition. In ICML.\\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\\nWilliam Yang Wang. 2020a.\\nLogical natural lan-\\nguage generation from open-domain tables. In ACL.\\nZhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou\\nZhou, Yunkai Zhang, Sairam Sundaresan, and\\nWilliam Yang Wang. 2020b.\\nLogic2Text: High-\\nﬁdelity natural language generation from logical\\nforms. In Findings of EMNLP.\\nZhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu,\\nand William Yang Wang. 2020c. Few-shot nlg with\\npre-trained language model. In ACL.\\nDorottya Demszky, Kelvin Guu, and Percy Liang. 2018.\\nTransforming question answering datasets into nat-\\nural language inference datasets.\\narXiv preprint\\narXiv:1809.02922.\\nBhuwan Dhingra, Manaal Faruqui, Ankur Parikh,\\nMing-Wei Chang, Dipanjan Das, and William Co-\\nhen. 2019. Handling divergent reference texts when\\nevaluating table-to-text generation. In ACL.\\nOndˇrej Dušek, David M. Howcroft, and Verena Rieser.\\n2019. Semantic noise matters for neural natural lan-\\nguage generation. In INLG.\\nHady Elsahar, Pavlos Vougiouklis, Arslen Remaci,\\nChristophe Gravier,\\nJonathon Hare,\\nFrederique\\nLaforest, and Elena Simperl. 2018. T-REx: A large\\nscale alignment of natural language with knowledge\\nbase triples. In LREC.\\nAngela Fan, Claire Gardent, Chloé Braud, and An-\\ntoine Bordes. 2019. Using local knowledge graph\\nconstruction to scale Seq2Seq models to multi-\\ndocument inputs. In EMNLP-IJCNLP.\\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\\nand Laura Perez-Beltrachini. 2017. The WebNLG\\nchallenge: Generating text from RDF data. In INLG.\\nSebastian Gehrmann, Falcon Dai, Henry Elder, and\\nAlexander Rush. 2018. End-to-end content and plan\\nselection for data-to-text generation. In INLG.\\nValerie Hajdik, Jan Buys, Michael Wayne Goodman,\\nand Emily M Bender. 2019. Neural text generation\\nfrom rich semantic representations. In NAACL.\\nHamza Harkous, Isabel Groves, and Amir Saffari. 2020.\\nHave your text and use it too! end-to-end neural\\ndata-to-text generation with semantic ﬁdelity. arXiv\\npreprint arXiv:2004.06577.\\nMihir Kale. 2020. Text-to-text pre-training for data-to-\\ntext tasks. arXiv preprint arXiv:2005.10433.\\nGregg Kellogg, Ivan Herman, and Jeremy Tandy.\\n2015.\\nGenerating\\nRDF\\nfrom\\ntabular\\ndata\\non the web.\\nW3C recommendation,\\nW3C.\\nHttps://www.w3.org/TR/2015/REC-csv2rdf-\\n20151217/.\\nIoannis Konstas and Mirella Lapata. 2012. Unsuper-\\nvised concept-to-text generation with hypergraphs.\\nIn NAACL.\\nRémi Lebret, David Grangier, and Michael Auli. 2016.\\nNeural text generation from structured data with ap-\\nplication to the biography domain. In EMNLP.\\nMike\\nLewis,\\nYinhan\\nLiu,\\nNaman\\nGoyal,\\nMar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy,\\nVes\\nStoyanov,\\nand\\nLuke\\nZettlemoyer.\\n2020. BART: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation,\\nand comprehension. In ACL.\\nPercy Liang, Michael I Jordan, and Dan Klein. 2009.\\nLearning semantic correspondences with less super-\\nvision. In ACL.\\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\\nRen. 2020. CommonGen: A constrained text gener-\\nation challenge for generative commonsense reason-\\ning. In Findings of EMNLP.\\nTianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,\\nand Zhifang Sui. 2018. Table-to-text generation by\\nstructure-aware seq2seq learning. In AAAI.\\nAmit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.\\nStep-by-step: Separating planning from realization\\nin neural data-to-text generation. In NAACL.\\nJekaterina Novikova, Ondˇrej Dušek, Amanda Cercas\\nCurry, and Verena Rieser. 2017a. Why we need new\\nevaluation metrics for nlg. In EMNLP.\\nJekaterina Novikova, Ondrej Dusek, and Verena Rieser.\\n2017b. The E2E dataset: New challenges for end-to-\\nend generation. In SIGDIAL.\\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann,\\nManaal Faruqui, Bhuwan Dhingra, Diyi Yang, and\\nDipanjan Das. 2020. ToTTo: A controlled table-to-\\ntext generation dataset. In EMNLP.\\nPanupong Pasupat and Percy Liang. 2015. Composi-\\ntional semantic parsing on semi-structured tables. In\\nACL.\\nBaolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun\\nLi, Jinchao Li, Michael Zeng, and Jianfeng Gao.\\n2020.\\nFew-shot natural language generation for\\ntask-oriented dialog. In arXiv.\\nRatish Puduppully, Li Dong, and Mirella Lapata. 2018.\\nData-to-text generation with content selection and\\nplanning. In AAAI.\\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019.\\nData-to-text generation with entity modeling.\\nIn\\nACL.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\\nIlya Sutskever. 2018.\\nImproving language under-\\nstanding by generative pre-training.\\nTechnical re-\\nport, OpenAI.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2020. Exploring the lim-\\nits of transfer learning with a uniﬁed text-to-text\\ntransformer. Journal of Machine Learning Research,\\n21(140):1–67.\\nEhud Reiter. 2007.\\nAn architecture for data-to-text\\nsystems. In Proceedings of the Eleventh European\\nWorkshop on Natural Language Generation (ENLG\\n07).\\nEhud Reiter. 2020. Openai gpt system: What does it\\ndo? Technical report, Arria.\\nEhud Reiter and Robert Dale. 2000. Building natural\\nlanguage generation systems. Cambridge university\\npress.\\nLeonardo F. R. Ribeiro, Claire Gardent, and Iryna\\nGurevych. 2019.\\nEnhancing AMR-to-text genera-\\ntion with dual graph representations. In EMNLP.\\nLeonardo F. R. Ribeiro, Martin Schmitt, Hinrich\\nSchütze, and Iryna Gurevych. 2020. Investigating\\npretrained language models for graph-to-text gener-\\nation. arXiv.\\nThibault Sellam, Dipanjan Das, and Ankur P Parikh.\\n2020.\\nBLEURT: Learning robust metrics for text\\ngeneration. In ACL.\\nLinfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo\\nWang, and Daniel Gildea. 2017. Amr-to-text gener-\\nation with synchronous node replacement grammar.\\nIn ACL.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In NeurIPS.\\nPavlos Vougiouklis,\\nHady ElSahar,\\nLucie-Aimée\\nKaffee, Christophe Gravier, Frédérique Laforest,\\nJonathon S. Hare, and Elena Simperl. 2018. Neu-\\nral wikipedian: Generating textual summaries from\\nknowledge base triples. Journal of Web Semantics,\\n52-53:1 – 15.\\nTsung-Hsien Wen,\\nMilica Gaši´c,\\nNikola Mrkši´c,\\nLina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes,\\nDavid Vandyke, and Steve Young. 2016.\\nCondi-\\ntional generation and snapshot learning in neural di-\\nalogue systems. In EMNLP.\\nTsung-Hsien Wen, Milica Gaši´c, Nikola Mrkši´c, Pei-\\nHao Su, David Vandyke, and Steve Young. 2015.\\nSemantically conditioned LSTM-based natural lan-\\nguage generation for spoken dialogue systems. In\\nEMNLP.\\nSam Wiseman, Stuart Shieber, and Alexander Rush.\\n2017.\\nChallenges in data-to-document generation.\\nIn EMNLP.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\\nWeinberger, and Yoav Artzi. 2020.\\nBERTScore:\\nEvaluating text generation with BERT. In ICLR.\\nChao Zhao, Marilyn Walker, and Snigdha Chaturvedi.\\n2020. Bridging the structural gap between encoding\\nand decoding for data-to-text generation. In ACL.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\\ntian M Meyer, and Steffen Eger. 2019. MoverScore:\\nText generation evaluating with contextualized em-\\nbeddings and earth mover distance. In EMNLP.\\nVictor Zhong, Caiming Xiong, and Richard Socher.\\n2017.\\nSeq2sql:\\nGenerating structured queries\\nfrom natural language using reinforcement learning.\\nCoRR, abs/1709.00103.\\nAppendix\\nThe Appendix contains the following contents:\\n• Results of the ablation study on WebNLG 2017 testset.\\n• Statistics of the table ontology annotations.\\n• Examples of tables that help illustrate DART’s annotation procedure.\\n• Examples of model outputs.\\nModel\\nExperiment\\nBLEU ↑\\nMETEOR ↑\\nTER ↓\\nSEEN\\nUNSEEN\\nALL\\nSEEN\\nUNSEEN\\nALL\\nSEEN\\nUNSEEN\\nALL\\nBaseline\\nTransformer\\n[1] webnlg\\n49.81\\n5.51\\n31.81\\n0.39\\n0.09\\n0.24\\n0.47\\n0.86\\n0.64\\n[2] webnlg+dart_decl_sents\\n52.31\\n8.96\\n39.98\\n0.40\\n0.07\\n0.25\\n0.45\\n0.79\\n0.60\\n[3] webnlg+dart_human_annotated\\n53.68\\n7.02\\n36.36\\n0.40\\n0.09\\n0.26\\n0.43\\n0.79\\n0.59\\n[4] webnlg+dart_ontology\\n53.40\\n8.54\\n38.51\\n0.41\\n0.08\\n0.26\\n0.44\\n0.80\\n0.60\\n[5] webnlg+dart_e2e\\n51.76\\n5.92\\n32.36\\n0.40\\n0.09\\n0.25\\n0.45\\n0.86\\n0.63\\n[6] webnlg+dart_full\\n54.99\\n8.64\\n39.11\\n0.40\\n0.08\\n0.25\\n0.42\\n0.81\\n0.60\\nBART-base\\n[1] webnlg\\n63.02\\n41.74\\n53.36\\n0.45\\n0.35\\n0.40\\n0.33\\n0.52\\n0.42\\n[2] webnlg+dart_decl_sents\\n62.71\\n42.51\\n53.64\\n0.45\\n0.36\\n0.40\\n0.34\\n0.51\\n0.41\\n[3] webnlg+dart_human_annotated\\n62.36\\n46.21\\n55.14\\n0.44\\n0.37\\n0.41\\n0.34\\n0.45\\n0.39\\n[4] webnlg+dart_ontology\\n62.62\\n46.74\\n55.54\\n0.44\\n0.38\\n0.41\\n0.34\\n0.45\\n0.39\\n[5] webnlg+dart_e2e\\n64.00\\n35.07\\n51.17\\n0.45\\n0.33\\n0.40\\n0.33\\n0.61\\n0.46\\n[6] webnlg+dart_full\\n63.66\\n45.48\\n55.52\\n0.45\\n0.37\\n0.41\\n0.33\\n0.47\\n0.40\\nBART-large\\n[1] webnlg\\n63.71\\n44.17\\n54.95\\n0.46\\n0.39\\n0.42\\n0.33\\n0.51\\n0.41\\n[2] webnlg+dart_decl_sents\\n65.18\\n46.79\\n56.79\\n0.46\\n0.39\\n0.42\\n0.32\\n0.48\\n0.40\\n[3] webnlg+dart_human_annotated\\n64.51\\n50.20\\n58.06\\n0.46\\n0.40\\n0.43\\n0.32\\n0.44\\n0.38\\n[4] webnlg+dart_ontology\\n64.19\\n49.62\\n57.65\\n0.46\\n0.39\\n0.43\\n0.33\\n0.45\\n0.38\\n[5] webnlg+dart_e2e\\n65.06\\n30.17\\n48.24\\n0.46\\n0.33\\n0.40\\n0.32\\n0.69\\n0.49\\n[6] webnlg+dart_full\\n65.24\\n47.96\\n57.44\\n0.46\\n0.39\\n0.43\\n0.32\\n0.46\\n0.39\\nT5-small\\n[1] webnlg\\n65.30\\n45.58\\n56.57\\n0.46\\n0.39\\n0.43\\n0.32\\n0.49\\n0.40\\n[2] webnlg+dart_decl_sents\\n64.18\\n46.61\\n56.27\\n0.46\\n0.39\\n0.43\\n0.33\\n0.48\\n0.40\\n[3] webnlg+dart_human_annotated\\n65.05\\n47.81\\n57.32\\n0.46\\n0.40\\n0.43\\n0.33\\n0.46\\n0.39\\n[4] webnlg+dart_ontology\\n65.17\\n47.49\\n57.24\\n0.46\\n0.39\\n0.43\\n0.32\\n0.47\\n0.39\\n[5] webnlg+dart_e2e\\n65.56\\n41.28\\n54.56\\n0.46\\n0.38\\n0.42\\n0.32\\n0.54\\n0.42\\n[6] webnlg+dart_full\\n64.70\\n47.56\\n57.01\\n0.46\\n0.39\\n0.43\\n0.33\\n0.47\\n0.39\\nT5-base\\n[1] webnlg\\n64.89\\n52.86\\n59.44\\n0.46\\n0.42\\n0.44\\n0.33\\n0.42\\n0.37\\n[2] webnlg+dart_decl_sents\\n65.44\\n50.80\\n58.81\\n0.46\\n0.41\\n0.44\\n0.32\\n0.43\\n0.37\\n[3] webnlg+dart_human_annotated\\n65.42\\n50.71\\n58.80\\n0.46\\n0.41\\n0.44\\n0.32\\n0.43\\n0.37\\n[4] webnlg+dart_ontology\\n65.17\\n51.49\\n59.04\\n0.46\\n0.41\\n0.44\\n0.33\\n0.43\\n0.37\\n[5] webnlg+dart_e2e\\n65.11\\n49.64\\n58.19\\n0.46\\n0.41\\n0.44\\n0.33\\n0.46\\n0.39\\n[6] webnlg+dart_full\\n65.99\\n51.68\\n59.50\\n0.46\\n0.42\\n0.44\\n0.32\\n0.43\\n0.37\\nT5-large\\n[1] webnlg\\n64.89\\n54.01\\n59.95\\n0.46\\n0.43\\n0.44\\n0.34\\n0.41\\n0.37\\n[2] webnlg+dart_decl_sents\\n65.97\\n53.00\\n60.12\\n0.46\\n0.42\\n0.44\\n0.32\\n0.41\\n0.36\\n[3] webnlg+dart_human_annotated\\n65.82\\n56.01\\n61.44\\n0.46\\n0.43\\n0.45\\n0.32\\n0.38\\n0.35\\n[4] webnlg+dart_ontology\\n65.53\\n55.20\\n60.90\\n0.46\\n0.42\\n0.44\\n0.32\\n0.38\\n0.35\\n[5] webnlg+dart_e2e\\n66.27\\n54.13\\n60.76\\n0.46\\n0.43\\n0.45\\n0.32\\n0.41\\n0.36\\n[6] webnlg+dart_full\\n65.78\\n54.35\\n60.64\\n0.46\\n0.42\\n0.44\\n0.32\\n0.39\\n0.35\\nTable 6: Results of ablation study on WebNLG 2017 testset. dart_decl_sents refers to DART partition that contains\\nauto-generated declarative sentences mentioned in Section 2.2, dart_human_annotated refers to partition that\\ncontains human written sentences mentioned in Section 2.1, dart_ontology is the combination of dart_decl_sents\\nand dart_human_annotated, and dart_e2e refers to DART partition containing instances extracted from E2E\\ndataset, the process of which is mentioned in Section 2.3. Note that dart_full is the combination of dart_ontology\\nand dart_e2e.\\nTables\\nOntology depth\\n(min, med, max)\\nNodes in ontology\\n(min, med, max)\\nBranching factor\\n(mean)\\nWikiTableQuestions\\n2060\\n1, 1, 4\\n2, 6, 25\\n4.0\\nWikiSQL\\n3563\\n1, 1, 4\\n3, 7, 25\\n5.1\\nTable 7: Properties of the ontology in the WikiTableQuestions and WikiSQL samples in DART. Branching factor\\nrefers to the average number of children across all non-leaf nodes in a table’s ontology.\\nFigure 3:\\nDistribution of column ontology depths in the WikiTableQuestions and WikiSQL samples in\\nDART v1.1.1.\\n<entry category=\"MISC\" eid=\"Id5\" size=\"3\">\\n<modifiedtripleset>\\n<mtriple>Apertura 2006 | JORNADA_OR_OTHER | Semifinals Ida</mtriple>\\n<mtriple>Semifinals Ida | AWAY_TEAM | América</mtriple>\\n<mtriple>Semifinals Ida | HOME_TEAM | Chivas</mtriple>\\n</modifiedtripleset>\\n<lex comment=\"WikiTableQuestions\" lid=\"Id1\">\\nChivas and América will compete in the semifinals of the Apertura 2006 tournament.\\n</lex>\\n</entry>\\n<entry category=\"MISC\" eid=\"Id76\" size=\"6\">\\n<modifiedtripleset>\\n<mtriple>Terry Jenkins | ROUND | 1st Round</mtriple>\\n<mtriple>Terry Jenkins | YEAR | 2014</mtriple>\\n<mtriple>[TABLECONTEXT] | [TITLE] | PDC World Darts Championship</mtriple>\\n<mtriple>1st Round | OPPONENT | Per Laursen</mtriple>\\n<mtriple>1st Round | RESULT | Lost</mtriple>\\n<mtriple>[TABLECONTEXT] | PLAYER | Terry Jenkins</mtriple>\\n</modifiedtripleset>\\n<lex comment=\"WikiTableQuestions\" lid=\"Id1\">\\nTerry Jenkins lost the game with Per Laursen in\\nthe 1st Round of 2014 PDC World Darts Championship\\n</lex>\\n</entry>\\nFigure 4: Examples of DART instance\\nFigure 5: An example of the data cleaning. The top left table had a missing column name and the table title was\\nnot speciﬁc to the data; our internal annotators add the missing column name “Year” and linked the rest of the\\ncolumns to the “Year” column. The bottom left table had repeat column names in the table; our internal annotators\\ndisambiguate the columns by making the column names more speciﬁc.\\nFigure 6: A WikiTableQuestions table that uses [TITLE] in the ontology.\\nFigure 7: A manually annotated table from WikiTableQuestions with a sentence that uses the table title.\\nFigure 8: A manually annotated table from WikiTableQuestions. Annotators created a table ontology, and they\\nwrote sentences encapsulating the information in the orange cells for a given row. Whenever a sentence referenced\\nthe table title, that sentence was also highlighted green.\\nFigure 9: An example of collected MTurk-generated sentences for WikiTableQuestions. Internal annotators went\\nthrough the generated sentences and checked for both sentence coherence and title usage. Below the generated\\nsentences, ‘y’ meant the sentence references the table title, ‘n’ meant the sentence did not use the table title, ‘x’\\nmeant the sentence was nonsensical.\\nFigure 10: Automatically generated declarative sentences from WikiSQL with human validation. Annotators went\\nthrough the generated sentences and checked for both sentence coherence and title use. Below the generated\\nsentences, ‘y’ meant the sentence references the table title, ‘n’ meant the sentence did not use the table title, ‘x’\\nmeant the sentence was nonsensical.\\n- Sample 1 -\\nInput triples:\\n<H> Peru Earthquake <R> scale of disaster <T> 250k homeless\\n<H> Peru Earthquake <R> year <T> 2007\\nBART-base output: 250k people were killed in the 2007 philippine earthquake .\\n- Sample 2 -\\nInput triples:\\n<H> [TABLECONTEXT] <R> game <T> 3\\n<H> 3 <R> attendance <T> 10 637\\n<H> [TABLECONTEXT] <R> [title] <T> 2006 Minnesota Swarm season\\nBART-base output: the minnesota swarm played in front of a crowd of 10 , 684 people .\\n- Sample 3 -\\nInput triples:\\n<H> Andrew Phelps McCormick <R> state <T> TX\\n<H> Andrew Phelps McCormick <R> active <T> 1892-1916\\nT5-base output: andrew phelps mccormick was active from 1892 to 1616 in texas .\\nFigure 11: Examples of hallucinated outputs of pretrained models trained on DART\\n- Sample 1 -\\nInput triples:\\n<H> Andrew Rayel <R> associated Band/associated Musical Artist <T> Christian Burns\\n<H> Andrew Rayel <R> associated Band/associated Musical Artist <T> Jonathan Mendelsohn\\nreference:\\nandrew rayel , is associated with musical artist jonathan mendelsohn and christian burns .\\ntrain on WebNLG - BART-base output:\\nchristian mendelsohn and andrew rayel are both associated with the same band , christian burns .\\ntrain on DART - BART-base output:\\nandrew rayel is associated with christian burns and jonathan mendelsohn .\\n- Sample 2 -\\nInput triples:\\n<H> Indie rock <R> stylistic Origin <T> New wave music\\nreference: the stylistic origin of indie rock is new wave music .\\ntrain on WebNLG - BART-base output:\\nthe alternative rock genre is new wave .\\ntrain on DART - BART-base output:\\nindie rock is influenced by new wave music .\\n- Sample 3 -\\nInput triples:\\n<H> Abradab <R> associated Band/associated Musical Artist <T> Magik rapper\\n<H> Abradab <R> associated Band/associated Musical Artist <T> Kaliber 44\\nreference:\\nabradab , an artist for the band kaliber 44 , is associated with magik ( rapper ) .\\ntrain on WebNLG - BART-base output:\\nmagiber 44 is the creator of abradab , which is also associated with the magik rapper .\\ntrain on DART - BART-base output:\\nmagik rapper and kaliber 44 are the associated musicians of abradab .\\n- Sample 4 -\\nInput triples:\\n<H> Alfa Romeo 164 <R> assembly <T> Milan\\n<H> Alfa Romeo 164 <R> related Mean Of Transportation <T> Saab 9000\\nreference:\\nthe alfa romeo 164 , which is assembled in milan , is a related means of transportation to saab 9000 ,\\nin that they are both cars .\\ntrain on WebNLG - T5-base output:\\nalfa romeo 164 is a transport vehicle for saab 9000 and is found in milan .\\ntrain on DART - T5-base output:\\nalfa romeo 164 ( assembled in milan ) is a related transport vehicle to saab 9000 .\\n- Sample 5 -\\nInput triples:\\n<H> Akeem Ayers <R> former Team <T> Tennessee Titans\\n<H> Akeem Ayers <R> draft Pick <T> 39\\nreference:\\nakeem ayers ’ former team was tennessee titans and he was number 39 in the draft pick .\\ntrain on WebNLG - T5-large output:\\nakeem ayers was drafted with the 39th pick by the tennessee titans .\\ntrain on DART - T5-large output:\\nakeem ayers , a former player of the tennessee titans , was the 39th draft pick .\\nFigure 12: Examples of model outputs - with or without DART data augmentation\\n',\n",
       " 'extraction_date': '2023-07-16 18:40:50.440010',\n",
       " 'num_pages': 16}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DART: Open-Domain Structured Data Record to Text Generation\\nLinyong Nan1\\nDragomir Radev1,2\\nRui Zhang3\\nAmrit Rau1\\nAbhinand Sivaprasad1\\nChiachun Hsieh4\\nXiangru Tang1\\nAadit Vyas1\\nNeha Verma1\\nPranav Krishna5\\nYangxiaokang Liu1\\nNadia Irwanto1\\nJessica Pan1\\nFaiaz Rahman1\\nAhmad Zaidi1\\nMurori Mutuma1\\nYasin Tarabar1\\nAnkit Gupta1\\nTao Yu1\\nYi Chern Tan1\\nXi Victoria Lin2∗\\nCaiming Xiong2\\nRichard Socher2\\nNazneen Fatema Rajani2\\n1 Yale University\\n2 Salesforce Research\\n3 Penn State University\\n4 The University of Hong Kong\\n5 MIT\\n{linyong.nan, dragomir.radev}@yale.edu, rmz5227@psu.edu, nazneen.rajani@salesforce.com\\nAbstract\\nWe present DART, an open domain structured\\nDAta Record to Text generation dataset with\\nover 82k instances (DARTs). Data-to-Text an-\\nnotations can be a costly process, especially\\nwhen dealing with tables which are the ma-\\njor source of structured data and contain non-\\ntrivial structures. To this end, we propose a\\nprocedure of extracting semantic triples from\\ntables that encodes their structures by exploit-\\ning the semantic dependencies among table\\nheaders and the table title. Our dataset con-\\nstruction framework effectively merged hetero-\\ngeneous sources from open domain semantic\\nparsing and dialogue-act-based meaning rep-\\nresentation tasks by utilizing techniques such\\nas: tree ontology annotation, question-answer\\npair to declarative sentence conversion, and\\npredicate uniﬁcation, all with minimum post-\\nediting. We present systematic evaluation on\\nDART as well as new state-of-the-art results\\non WebNLG 2017 to show that DART (1)\\nposes new challenges to existing data-to-text\\ndatasets and (2) facilitates out-of-domain gen-\\neralization. Our data and code can be found\\nat https://github.com/Yale-LILY/\\ndart.\\n1\\nIntroduction\\nAutomatically generating textual descriptions from\\nstructured data improves the accessibility of knowl-\\nedge bases to lay users. Such applications include\\nexplaining data records to non-experts (Cawsey\\net al., 1997), writing sports news (Chen and\\nMooney, 2008), summarizing information in mul-\\ntiple documents (Fan et al., 2019), and generating\\ndialogue responses (Wen et al., 2015).\\nWhile signiﬁcant progress has been made in this\\nﬁeld, there are still several issues with existing\\nData-to-Text datasets. First, they adopt a ﬂat ontol-\\nogy structure of the data, such as slot-value pairs\\nfor data records (Lebret et al., 2016; Novikova et al.,\\n∗Now at Facebook AI.\\n2017b) or ﬂat schema for tables (Wiseman et al.,\\n2017; Chen et al., 2020a; Parikh et al., 2020). This\\nﬂat structure is not powerful enough to encode rich\\nsemantic relationships in the ontology of the struc-\\ntured data, especially tables, whose representation\\ncan be further improved with these semantic knowl-\\nedge. Second, some of the datasets only focus on\\na small number of domains or knowledge graphs,\\ntherefore providing limited number of predicates\\nand data ontologies. For example, E2E (Novikova\\net al., 2017b) on restaurants and WebNLG (Gar-\\ndent et al., 2017) on 15 categories from DBPedia.\\nFurthermore, some of them only have loose align-\\nments between data input and sentence due to the\\nnature of the task (Wiseman et al., 2017) and the\\nautomatic generation procedure (Vougiouklis et al.,\\n2018; Elsahar et al., 2018).\\nTo address some of these issues and to encour-\\nage further research in natural language generation\\nfrom structured data, we introduce DART, a large\\nand open-domain structured DAta-Record-to-Text\\ngeneration corpus. The goal of DART is to har-\\nvest the diverse predicates occurred in Wikipedia\\ntables, which is signiﬁcantly richer than those de-\\nﬁned in the domain speciﬁc ontologies E2E and\\nWebNLG were built on (Table 2). We also in-\\ntroduce novel tree ontology annotation on tables,\\nwhich converts a ﬂat table schema into a tree struc-\\ntured semantic frame. The tree ontology reﬂects\\nthe core and auxiliary relations in the table schema,\\nand naturally occurs across many domains. As a\\nresult, DART provides high-quality sentence an-\\nnotations to tree structured semantic frames ex-\\ntracted from various data sources, including Wik-\\niSQL (Zhong et al., 2017) and WikiTableQuestions\\n(Pasupat and Liang, 2015), two open-domain ques-\\ntion answering datasets, as well as E2E (Novikova\\net al., 2017b) and WebNLG (Gardent et al., 2017)\\n(Figure 1). We evaluated several state-of-the-art\\ndata-to-text models on DART, and found that while\\nthese models achieve impressive performance on\\narXiv:2007.02871v2  [cs.CL]  12 Apr 2021\\ndomain-speciﬁc datasets, their performance suffers\\non DART due to its open-domain nature and richer\\nsemantic structures.\\nOur contributions are as follows. (1) We present\\na large and open-domain corpus for structured data\\nrecord to text generation, annotated with tree on-\\ntologies converted from the table. This hierarchical\\ninput differentiates our corpus from existing data-\\nto-text corpora. (2) We benchmark several state-\\nof-the-art data-to-text models to show that DART\\nintroduces new generalization challenges. (3) We\\ndemonstrate that using DART for data augmenta-\\ntion improves the performance of existing models\\non the WebNLG 2017 dataset. We expect the re-\\nsults to generalize to other data-to-text datasets\\ngiven the open-domain nature of DART.\\n2\\nDART Data Collection\\nAs shown in Figure 1, DART is constructed from\\nthree different sources: (1) human annotation on\\nWikipedia tables from two table semantic parsing\\nand question answering datasets WikiSQL and Wik-\\niTableQuestions (§ 2.1), (2) automatic conversion\\nof questions in WikiSQL to declarative sentences\\n(§ 2.2), and (3) incorporation of existing datasets\\nincluding WebNLG 2017 and Cleaned E2E (§ 2.3).\\nAfter collecting the ⟨triple-set, sentence⟩ pairs from\\nvarious data sources, we manually canonicalized\\nthe predicates and show that DART covers a broad\\nrange of topics (§ 2.4). Finally, we discuss the data\\nsplit in § 2.5.\\n2.1\\nTree Ontology and Sentence Annotation\\non Tables\\nTables are a major source of structured data that\\ncontain a wealth of information complementary\\nto text and knowledge graphs.\\nWe aim to col-\\nlect ⟨triple-set, sentence⟩ pairs from open-domain\\nWikipedia tables.\\nHowever, table schema are\\nﬂat, making them not directly usable for building\\nsubject-predicate-object triples to capture rich rela-\\ntionships in the data.\\nAs shown in Figure 2, we propose a two-stage an-\\nnotation process that involves two groups of anno-\\ntators: internal annotators and Amazon Mechanical\\nTurk1 workers. In the ﬁrst stage, skilled internal an-\\nnotators specify the parent of every column header\\nto construct a tree-structured ontology for each ta-\\nble. In the second stage, both internal and external\\nannotators provide a sentential description of the\\n1https://www.mturk.com/\\nhighlighted cells in a row that are automatically-\\nchosen based on the ontology.\\nTree Ontology Annotation\\nFor each column in\\na given table, our internal annotators labeled its\\nontological parent. In Figure 2, for example, the an-\\nnotator would provide the sequence {NULL, TEAM,\\nSTADIUM, STADIUM, TEAM} as the parent of each\\ncolumn — column TEAM has no parent, STADIUM\\nhas parent TEAM, and so on. In many cases, the\\nrelationship between a parent column and its child\\ncolumn can be conceptualized as a \"has-a\" relation-\\nship. For tables that are malformed or have dupli-\\ncate or missing column names (as shown in Figure\\n5 of the Appendix), annotators either changed or\\nadded appropriate column names in order to ﬁt\\nthese patterns. For each table we generate an ontol-\\nogy tree whose root is always [TABLECONTEXT].\\nThis root node either has (1) one child node [TI-\\nTLE] in the cases where the table title is the subject\\nof entire table, or (2) column header node(s) and\\na [TITLE] node as children, as shown in Figure 2.\\nThis is because in some tables, the table title itself\\nis more appropriate to be the root of the ontology\\ntree (example shown in Figure 6 of the Appendix).\\nIn these cases, annotators assigned the special to-\\nken [TITLE] as the parent of the relevant column\\nnodes. For other tables, title usually provides im-\\nportant context for understanding the table’s rows\\n(example shown in Figure 7 of the Appendix). In\\nsuch cases, [TITLE] is made a child of [TABLE-\\nCONTEXT] together with the column headers that\\nare appropriate.\\nWe evaluate the quality of the initial tree on-\\ntology annotation and made corrections with the\\nfollowing procedure: (1) reject and request correc-\\ntions from the original annotators if the provided\\nontology is disconnected or contains a cycle, (2)\\nverify that all column headers appear as a node in\\nthe tree. For many tables, the determination of an\\nontology is a subjective process with many \"cor-\\nrect\" answers - for example, swapping the positions\\nof TEAM and CITY in the tree in Figure 2 produces\\nan equally valid ontology for the referenced table.\\nIf there are multiple ways to construct an ontology\\nbased on annotators’ decisions of attribute relation-\\nships among column headers, we manually unify\\nthe annotations for similar tables (for examples,\\ntables about athletes in different sports). The on-\\ntologies exhibit a great deal of structural variety.\\nRelevant statistics are summarized in Table 7 and\\nFigure 3 of the Appendix.\\nFigure 1: DART data collection pipeline. MR: Meaning Representation.\\nInput Unit\\nExamples\\nVocab Size\\nWords per SR\\nSents per SR\\nTables\\nWikiTableText\\nRow\\n13,318\\n—\\n13.9\\n1.0\\n4,962\\nLogicNLG\\nTable\\n37,015\\n122K\\n13.8\\n1.0\\n7,392\\nToTTo\\nHighlighted Cells\\n136,161\\n136K\\n17.4\\n1.0\\n83,141\\nDART\\nTriple Set\\n82,191\\n33.2K\\n21.6\\n1.5\\n5,623\\nTable 1: DART compared with other open-domain table-to-text datasets. DART takes triple sets as input by\\nincorporating the ontology of table headers and title, and its surface realizations tend to be longer with more than\\nsingle sentence verbalization. SR: Surface Realization.\\nDART: 62,659 train / 6,980 dev / 12,552 test\\nWikiTableQuestions\\nWikiSQL\\nWebNLG\\nCleaned E2E\\nInternal\\nMTurk\\nInternal\\nDeclarative\\nDomains\\nWikipedia (open-domain)\\n15 DBPedia Categories\\nRestaurants\\nUnique Predicates\\n1,950\\n1,403\\n493\\n2,008\\n347\\n7\\nUnique Triples\\n13,505\\n5,541\\n1,648\\n7,787\\n3,220\\n946\\nTripleset-Sentence Pairs\\n4,902\\n2,120\\n772\\n4,204\\n27,731\\n42,462\\nTriples per Tripleset (min, med, max)\\n1, 3, 10\\n1, 3, 7\\n1, 2, 7\\n1, 2, 10\\n1, 3, 7\\n1, 4, 7\\nVocab Size\\n13.4K\\n8.9K\\n3.0K\\n10.7K\\n8.0K\\n3.0K\\nWords per SR\\n15.2\\n16.5\\n14.0\\n12.6\\n22.5\\n22.9\\nSentences per SR\\n1.0\\n1.1\\n1.0\\n1.0\\n1.4\\n1.6\\nTable 2: Statistics of DART decomposed by different collection methods. DART exhibits a great deal of topical\\nvariety in terms of the number of unique predicates, the number of unique triples, and the vocabulary size.\\nConnected Component Extraction\\nAfter we\\nannotated the ontology, we automatically choose\\na subset of cells for a selected table row to form\\nthe triple set. Randomly selecting cells leads to\\npoor quality annotation as the selected data could\\nlack a subject, lack cohesion, or would require in-\\nformation not encoded in the ontology to form a\\ncoherent sentence. For example, in Figure 2, if only\\ntwo nodes CITY and CAPACITY were highlighted\\nthen a coherent sentence cannot be produced as\\nthere is no direct logical relationship (functional\\ndependency) between them. To solve these issues,\\ninstead of randomly selecting cells in a row, we\\nextract connected components from the ontology.\\nThe extracted components have two controllable\\nproperties: size and shape. To create variation in\\nsize, we randomly sampled between [2, 5]. The\\nshape is determined by two numbers: the number\\nof sibling node pairs and parent-child node pairs.\\nIncreasing the number of sibling node pairs creates\\na wider tree, while increasing the latter creates a\\ndeeper tree. We created a sliding scale between\\nwidth and depth using an expansion parameter, p.\\nWe recursively visit a node if it has children with\\nprobability p and otherwise move to a sibling if it\\nexists. If p = 1, the search becomes a DFS and if\\np = 0, it becomes BFS. We found that randomly\\nselecting p from 0.5 to 0.7 created a reasonable\\nvariation in extracted component shapes. This en-\\nsures the balance between breadth and depth of\\nontology coverage of the selected cells, therefore\\nensuring the quality of the sentence annotation.\\nSentence Annotation\\nGiven the table, title, and\\nconnected highlighted cells of a row, annotators\\nFigure 2: Overview of our human annotation procedure. Top panel: We collect the parent-child relations between\\ncolumns from internal annotators (yellow is parent, green is child). Then, we collect a surface realization of the\\ncells highlighted in orange. Middle panel: We use the provided parent-child relations to construct an ontology tree\\non the columns, then select the nodes corresponding to the highlighted cells. We gather a connected subtree by\\ncollecting all nodes leading up to the highlighted cells’ lowest common ancestor. Bottom panel: We extract a set of\\ntriples from the subtree as shown. This triple-set is paired with the provided realization to form a DART instance.\\nwere asked to write a description of the highlighted\\ncells. We encouraged the annotators to use di-\\nverse vocabulary and syntactic structures. To en-\\nsure quality, internal annotators reviewed every\\ncrowd sourced sentence for correctness. They ei-\\nther rewrote or discarded the sentences that were\\nnonsensical or incorrect. In some cases, they also\\nchanged cell highlighting patterns to match the sen-\\ntence provided.\\nBuild Tripleset-Sentence Pairs\\nFinally, we con-\\nvert the highlighted cells to triplesets. For a row R,\\nwe start with the table’s column ontology T. We\\nﬁrst place the cell values in R in their correspond-\\ning slots in T, e.g. in Figure 2 we ﬁll TEAM with\\n\"Amsterdam Admirals\". We then check that the\\nnodes of T corresponding to the highlighted cells\\nin R form a connected subtree. If not, we walk up\\nthe tree and highlight each traversed node up un-\\ntil the lowest common ancestor of the highlighted\\nnodes (inclusive) to form a connected subtree. For\\neach node N in the tree except the root node, we\\ncan extract the triple (parent (N), title (N), N).\\nFor example, since STADIUM is highlighted in Fig-\\nure 2, we extract the triple (Amsterdam Admirals,\\nSTADIUM, Olympisch Stadion). A small number\\nof triple-sets contained more than 10 triples. We\\ndiscarded these because their associated surface\\nrealizations were of poor quality. The numbers\\nof tripleset-sentence pairs annotated by different\\nannotators are shown in Table 2.\\n2.2\\nAutomatically Converting Questions to\\nDeclarative Sentences\\nHigh quality natural language questions in open\\ndomain semantic parsing datasets such as Wik-\\niSQL and QA2D techniques found in automati-\\ncally constructing NLI datasets (Demszky et al.,\\n2018) present themselves as an attractive opportu-\\nnity to semi-automatically construct an abundance\\nof declarative sentences and align to table cells. We\\nleveraged rule-based QA2D technique2 together\\nwith manual screening to combine WikiSQL ques-\\ntions and SQL-retrieved-answers into declarative\\nsentences and manually ﬁltered out bad sentences.\\nWe only execute SQL queries without aggregate\\ncommands3 to retrieve answers corresponding to\\nquestions answerable by single rows. An example\\nof such conversion is as follows:\\n2We use the rule-based model from https://github.\\ncom/kelvinguu/qanli (Demszky et al., 2018). The neu-\\nral model code is not released.\\n3MAX, MIN, COUNT, SUM, AVG, JOIN, INTER-\\nSECT, UNION, GROUP BY, ORDER BY.\\nQuestion:\\nIn which year did Greece hold its\\nlast Summer Olympics?\\nAnswer: 2004\\nDeclarative Sentence: Greece held its last Summer\\nOlympics in 2004.\\nAlignment with table cells is done at two\\nstages. We ﬁrst align sentences with corresponding\\nrows by changing SQL commands to SELECT\\n* and use string matching to obtain columns\\nand column headers relevant to the answer and\\nWHERE condition. After manually ﬁltering out\\nbad sentences, bad alignments, or tables without\\nontology annotations, we were able to get 4,204\\nsentences. Finally, the corresponding table cells\\nare then converted into triples in the same way as\\nwe described in Section 2.1.\\nExamples of produced declarative sentences can\\nbe found in Figure 10 of the Appendix.\\n2.3\\nIncorporating Existing Datasets\\nSince they provide a large amount of strictly\\naligned data-text pairs with high quality sentences,\\nwe incorporate the following existing datasets in\\nthe same ⟨triple-set, sentence⟩ pair format with\\nsome modiﬁcations.\\nWebNLG 2017\\nAn instance of the WebNLG\\ndataset contains a set of triples extracted from DB-\\npedia and the target text written by human. We\\ninclude the WebNLG 2017 dataset4 consisting of\\n27731 triple-set sentence pairs with up to 7 RDF\\ntriples in a triple set covering 15 domains.\\nCleaned E2E\\nThe original E2E dataset includes\\ndialogue act meaning representations (MR) and\\nnatural language references in the restaurant do-\\nmain. Later, Dušek et al. (2019) provide Cleaned\\nE2E5 by automatically ﬁxing the dialogue acts to\\naccount for omissions and hallucinations in the\\ntext.\\nWe incorporate Cleaned E2E because of\\nits strict alignment between the meaning repre-\\nsentation and the text. To convert the MR to a\\ntriple-set, we take the NAME slot (present in al-\\nmost all the MRs) as the subject. For example,\\nthe MR (NAME[ALIMENTUM], AREA[CITY CEN-\\nTRE], FAMILYFRIENDLY[NO]) is converted to the\\n4https://gitlab.com/shimorina/\\nwebnlg-dataset/-/tree/master/webnlg_\\nchallenge_2017\\n5https://github.com/tuetschek/\\ne2e-cleaning\\ntriple-set {(ALIMENTUM, AREA, CITY CENTRE),\\n(ALIMENTUM, FAMILYFRIENDLY, NO)}. We drop\\nMRs which do not contain the NAME slot.\\n2.4\\nPredicate Uniﬁcation\\nWe canonicalized the predicates in our triple sets\\nsuch that those of the same meaning are also repre-\\nsented the same. We manually constructed a predi-\\ncate mapping table to achieve this. As an example,\\nour predicate mapping maps \"Hometown,\" \"Home\\nTown,\" and \"Home Town/City\" to the uniﬁed pred-\\nicate \"HOMETOWN.\"\\nAfter unifying predicates, we evaluated the di-\\nversity of DART by counting the number of unique\\npredicates in its partitions. As shown in Table 2, we\\nsee that the Wikipedia partition of DART contains\\nmuch more unique predicates than the WebNLG\\nand Cleaned E2E partitions combined, despite hav-\\ning smaller number of ⟨triple-set, sentence⟩ pairs.\\nThis contributes signiﬁcantly to the domain di-\\nversity of DART. In addition, we can see that\\nDART exhibits a great deal of topical variety in\\nterms of number of unique triples and vocabulary\\nsize.\\n2.5\\nDataset Split\\nFor WebNLG 2017 and Cleaned E2E, we use their\\noriginal data splits. For our annotation on Wik-\\niTableQuestions and WikiSQL, random splitting\\nwill make train, dev, and test splits contain similar\\ntables and similar ⟨triple-set, sentence⟩ examples.\\nTherefore, to increase the generalization challenge,\\nwe compare the table title and the table header to\\nﬁnd similar tables, and make sure the model is eval-\\nuated on test split tables that are least similar to\\nthose used for training. We ﬁrst sample some ta-\\nbles as a seed test set, and then compute Jaccard\\nsimilarity6 with remaining tables based on the titles\\nand the headers. If a table has a Jaccard similarity\\ngreater than 0.5 with any of the tables in the test\\nset, we add it into the test set. A similar process\\nis repeated to create the dev set, and the remain-\\ning tables form the training set. This results in\\n62,659/6,980/12,552 sentences in the train/dev/test\\nsets, respectively.\\n3\\nExperimental Results\\nWe conduct experiments on DART and the\\nWebNLG 2017 dataset, with an ablation study on\\n6https://en.wikipedia.org/wiki/\\nJaccard_index\\nWebNLG to show the beneﬁts of using DART for\\ndata augmentation.\\n3.1\\nModels\\nWe investigate several state-of-the-art Data-to-Text\\ngeneration models. We report results of the fol-\\nlowing models on DART-testset: (1) Bidirectional-\\nLSTM with attention, for which we use 2-layer\\nbi-LSTM for encoder, with 300 dimensional word\\nembeddings (without using pretrained word vec-\\ntors), 512 hidden units and 0.3 dropout rate for the\\ndecoder. (2) Transformer (Vaswani et al., 2017),\\npreviously used by Castro Ferreira et al. (2019) on\\nthe WebNLG dataset. The input is formed by lin-\\nearizing the unordered triple set. (3) BART (Lewis\\net al., 2020), for which we report results of both\\nBART-base and BART-large. (4) T5 (Raffel et al.,\\n2020): we add the same preﬁx \"translate Graph to\\nEnglish:\" to the input, as it is used in Ribeiro et al.\\n(2020). We report results of T5-small, T5-base and\\nT5-large models. For both BART and T5 models,\\nwe use implementations of Ribeiro et al. (2020),\\nwith same hyperparameter setting.\\n3.2\\nEvaluation Metrics\\nWe use a variety of automatic metrics and human\\nevaluation (Section 4) to evaluate the quality of the\\ngenerated text. We report BLEU, METEOR, and\\nTER which are used in the ofﬁcial WebNLG chal-\\nlenge. However, these measures have limitations\\nin considering the semantic meanings of words or\\nphrases (Novikova et al., 2017a), therefore we also\\nreport MoverScore (Zhao et al., 2019), BERTScore\\n(Zhang et al., 2020), and BLEURT (Sellam et al.,\\n2020) that incorporate semantics rather than sur-\\nface forms using contextual embeddings. Further-\\nmore, we include PARENT (Dhingra et al., 2019)\\nwhich explicitly aligns n-grams from the reference\\nand generated text to the data contents.\\n3.3\\nResults\\nDART\\nOur experimental results on DART are\\nsummarized in Table 3. The T5-large model has\\nthe highest performance among all models with a\\nBLEU score of 50.66. We attribute this to T5’s gen-\\neralization and transfer learning ability due to pre-\\ntraining on multi-tasks. We can see that in general,\\npretrained models outperform others by a large\\nmargin, and increasing the model size seems to\\nfurther boost the performance on DART. However,\\nlanguage models such as BART and T5 are pre-\\ntrained by reconstructing text and, as a result, we\\nfound that their output on DART often contains\\nhallucinated words (Parikh et al., 2020; Harkous\\net al., 2020; Reiter, 2020), as shown in Figure 11.\\nIn addition, while the pretrained model shows bet-\\nter text generation quality due to its generalization\\nability from pretraining, it does not fully capture\\nthe hierarchical ontology nature of the triple sets\\nin their linearized input, therefore making DART\\nmore challenging. We suspect that models that\\nare better at exploiting the ontology structure pre-\\nserved in the input tripleset will achieve better per-\\nformance on DART.\\nWebNLG\\nFurthermore,\\nwe\\ninvestigate\\nif\\nDART can improve pretrained models’ perfor-\\nmance on other Data-to-Text generation tasks.\\nTo this end, we ﬁnetune the baseline transformer\\nmodel, BART-[base, large] and T5-[small, base,\\nlarge] on the WebNLG 2017 dataset, and augment\\nthe training by adding instances in the DART train-\\ning set. The experimental results can be found in\\nTable 4. We report performances of some competi-\\ntive models that are not pretrained, as well as the\\nstate-of-the-art performances of pretrained models\\non the WebNLG 2017 dataset by Ribeiro et al.\\n(2020). On the bottom panel, we include results\\nof experiments augmented with DART instances\\nwhose triplesets are generated with table ontology\\nannotation, paired with human written sentences.\\nWe are able to achieve new state-of-the-art results\\non all WebNLG 2017 test set splits (seen, unseen\\nand all) by ﬁnetuning T5-large on DART. We\\nobserve that using DART for data augmentation\\nconsistently improves the performance across all\\nmodels, including the baseline transformer model\\nthat is not pretrained. Furthermore, we observe\\nthat more improvement is shown on unseen split of\\nthe test set, due to DART’s open-domain nature.\\nSee Figure 12 of the Appendix for example model\\noutputs aligned with their human references.\\n3.4\\nAblation Study\\nWe also conduct an ablation study on the WebNLG\\ndataset to investigate what part of DART con-\\ntributes most to improving the Data-to-Text tasks\\nin general. We report results of the study in Table 6\\nof the Appendix. We divide DART into 4 partitions,\\nwhere declarative sentence (auto-generated) parti-\\ntion and human annotated sentence partition con-\\ntain instances whose triplesets are extracted from\\nWikipedia tables based on ontology. E2E parti-\\ntion contains instances converted from the E2E\\nBLEU ↑\\nMETEOR ↑\\nTER ↓\\nMoverScore ↑\\nBERTScore(F1) ↑\\nBLEURT ↑\\nPARENT ↑\\nLSTM with Attention\\n29.66\\n0.27\\n0.63\\n0.31\\n0.90\\n-0.13\\n0.35\\nEnd-to-End Transformer\\n27.24\\n0.25\\n0.65\\n0.25\\n0.89\\n-0.29\\n0.28\\nBART-base\\n47.11\\n0.38\\n0.46\\n0.51\\n0.95\\n0.37\\n0.55\\nBART-large\\n48.56\\n0.39\\n0.45\\n0.52\\n0.95\\n0.41\\n0.57\\nT5-small\\n47.69\\n0.39\\n0.46\\n0.52\\n0.95\\n0.40\\n0.56\\nT5-base\\n49.21\\n0.40\\n0.44\\n0.53\\n0.95\\n0.43\\n0.57\\nT5-large\\n50.66\\n0.40\\n0.43\\n0.54\\n0.95\\n0.44\\n0.58\\nTable 3: Model results on the test set of DART ↑: Higher is better. ↓: Lower is better.\\nBLEU ↑\\nMETEOR ↑\\nTER ↓\\nSEEN\\nUNSEEN\\nALL\\nSEEN\\nUNSEEN\\nALL\\nSEEN\\nUNSEEN\\nALL\\nPipeline Transformer† (Castro Ferreira et al., 2019)\\n56.28\\n23.04\\n42.41\\n0.42\\n0.21\\n0.32\\n0.39\\n0.63\\n0.50\\nPipeline GRU† (Castro Ferreira et al., 2019)\\n56.09\\n25.12\\n42.73\\n0.42\\n0.22\\n0.33\\n0.39\\n0.64\\n0.51\\nMELBOURNE (Gardent et al., 2017)\\n54.52\\n33.27\\n45.13\\n0.41\\n0.33\\n0.37\\n0.40\\n0.55\\n0.47\\nBestPlan † (Moryossef et al., 2019)\\n53.30\\n34.41\\n47.24\\n0.44\\n0.34\\n0.39\\n0.47\\n0.56\\n0.51\\nDualEnc (Zhao et al., 2020)\\n63.45\\n36.73\\n51.42\\n0.46\\n0.37\\n0.41\\n0.34\\n0.55\\n0.44\\nPlanEnc (Zhao et al., 2020)\\n64.42\\n38.23\\n52.78\\n0.45\\n0.37\\n0.41\\n0.33\\n0.53\\n0.42\\nRibeiro et al. (2020)\\nBART-base ‡\\n63.02\\n41.74\\n53.36\\n0.45\\n0.35\\n0.40\\n0.33\\n0.52\\n0.42\\nBART-large ‡\\n63.71\\n44.17\\n54.95\\n0.46\\n0.39\\n0.42\\n0.33\\n0.51\\n0.41\\nT5-small ‡\\n65.30\\n45.58\\n56.57\\n0.46\\n0.39\\n0.43\\n0.32\\n0.49\\n0.40\\nT5-base ‡\\n64.89\\n52.86\\n59.44\\n0.46\\n0.42\\n0.44\\n0.33\\n0.42\\n0.37\\nT5-large ‡\\n64.89\\n54.01\\n59.95\\n0.46\\n0.43\\n0.44\\n0.34\\n0.41\\n0.37\\n+ DART\\nBART-base\\n62.36\\n46.21\\n55.14\\n0.44\\n0.37\\n0.41\\n0.34\\n0.45\\n0.39\\nBART-large\\n64.51\\n50.20\\n58.06\\n0.46\\n0.40\\n0.43\\n0.32\\n0.44\\n0.38\\nT5-small\\n65.05\\n47.81\\n57.32\\n0.46\\n0.40\\n0.43\\n0.33\\n0.46\\n0.39\\nT5-base\\n65.42\\n50.71\\n58.80\\n0.46\\n0.41\\n0.44\\n0.32\\n0.43\\n0.37\\nT5-large\\n65.82\\n56.01\\n61.44\\n0.46\\n0.43\\n0.45\\n0.32\\n0.38\\n0.35\\nTable 4: The WebNLG 2017 results on the test set.\\n†: We report results from Zhao et al. (2020) who use the\\nevaluation scripts that are strictly the same as the ofﬁcial challenge. ‡: We report results calculated with the model\\noutputs on the WebNLG 2017 testset released by Ribeiro et al. (2020).\\nTripleset source\\nSentence source\\n% ﬂuent\\n% faithful\\n% (ﬂuent+\\nmostly ﬂuent)\\n% (faithful+\\nmostly faithful)\\nWikiTableQuestions (§ 2.1)\\nhuman-written reference\\n75%\\n81%\\n96%\\n99%\\nBART-base\\n74%\\n57%\\n93%\\n84%\\nT5-base\\n72%\\n54%\\n94%\\n76%\\nWikiSQL (§ 2.2)\\nauto-generated reference\\n59%\\n56%\\n87%\\n88%\\nBART-base\\n66%\\n51%\\n92%\\n83%\\nT5-base\\n75%\\n65%\\n97%\\n90%\\nTable 5: Human evaluation over references and model outputs.\\ndataset, and WebNLG partition keeps the original\\ndata format. In general, we observe that adding\\nDART instances that contain human written sen-\\ntences brings most improvement, especially on un-\\nseen split. While adding E2E partition boosts the\\nscores on seen test split and deteriorates the perfor-\\nmance on unseen test split. This trend is consistent\\nacross all models. Comparing results of declarative\\nsentence partition and human written sentence par-\\ntition, we see that for most of the models, DART\\ninstances with human written sentences have better\\nquality as it brings more improvement to the task.\\n4\\nHuman Evaluation\\nIn Table 5, we perform human evaluation on\\nDART based on two criteria: (1) ﬂuency if a sen-\\ntence is natural and grammatical, and (2) semantic\\nfaithfulness if a sentence is supported by the input\\ntriples. We deﬁned three levels of ﬂuency: ﬂuent,\\nmostly ﬂuent, and not ﬂuent, and the same for se-\\nmantic faithfulness. We ask 5 internal annotators to\\nevaluate on 100 triplesets sampled from declarative\\nsentence partition and another 100 triplesets sam-\\npled from human written sentence partition. Each\\ntripleset is paired with 3 sentences, one of them\\nis the reference sentence, and the other two are\\noutputs of BART-base and T5-base models.\\nThe results in Table 5 attest to the high quality of\\nour annotations since the human written references\\nachieve highest ﬂuency and faithfulness comparing\\nto outputs of two strong baseline models. The eval-\\nuation on faithfulness also demonstrates that there\\nis a considerable gap between the DART reference\\nand the outputs of the state-of-the-art pretrained\\nmodel, showing that there is a large room for im-\\nprovement. We also noticed that the auto-generated\\ndeclarative sentences are not as ﬂuent or faithful\\nas the model outputs because they are generated\\nwith a rule-based system. However, we decided to\\nrelease this partition, along with other partitions of\\nDART because it demonstrates an economic way\\nto obtain large amounts of DART instances and it\\nalso shows beneﬁts for generalization due to the\\ndiverse topics it contains.\\n5\\nRelated Work\\nData-to-Text\\nData-to-Text generation aims to\\nproduce natural language output from structured\\ninput. Applications include generating sports com-\\nmentaries (Chen and Mooney, 2008; Wiseman\\net al., 2017), weather forecasts (Liang et al., 2009;\\nKonstas and Lapata, 2012), biographical texts (Le-\\nbret et al., 2016; Liu et al., 2018), knowledge-base\\ndescriptions (Gardent et al., 2017), dialogue re-\\nsponse generation (Wen et al., 2015, 2016), and\\ncommonsense reasoning (Lin et al., 2020). Yet,\\nmost existing datasets are restricted to speciﬁc do-\\nmains and applications. In contrast, a major source\\nof DART is from Wikipedia tables covering various\\ndomains and topics.\\nRepresentation of Data\\nThe input of the Data-\\nto-Text datasets take different formats, including\\nslot-value pairs, Abstract Meaning Representa-\\ntion (AMR) (Song et al., 2017; Ribeiro et al.,\\n2019), Minimal Recursion Semantics (MRS) (Ha-\\njdik et al., 2019), Resource Description Framework\\n(RDF triples) (Gardent et al., 2017), and logic\\nforms (Chen et al., 2020b). There are also stud-\\nies of converting tabular data to RDF triples in the\\nSemantic Web community (Kellogg et al., 2015).\\nRecently, some open-domain table-to-text datasets\\nhave been proposed including WikiTableText (Bao\\net al., 2018), LogicNLP (Chen et al., 2020a), and\\nToTTo (Parikh et al., 2020), whose inputs are rows\\nor entire tables. In ToTTo, highlighted cells are\\nalso provided as input, and the authors found using\\nonly highlighted cells with ﬂat row and column\\nheaders led to higher performance than using the\\nentire table.\\nIn contrast, DART is constructed by ﬁrst annotat-\\ning the tree-structured table ontology that encodes\\nthe semantic dependencies among table headers,\\nand we could ﬂexibly incorporate additional con-\\ntexts such as the table title to the ontology tree.\\nWe then use an automatic procedure to extract con-\\nnected components from the tree to form the input\\nof a DART instance. Our annotation framework\\nnot only provides a ﬂexible way of incorporating\\nany contexts to the representation of tables, but\\nalso encodes hierarchical relationships among ta-\\nble headers and contexts, ensuring the extracted\\ntriples are logically consistent and can be described\\nin text without loss of information.\\nModel\\nTraditional Data-to-Text models break\\nthe generation progress into different stages such\\nas signal analysis, data interpretation, document\\nplanning, microplanning, and realization (Reiter\\nand Dale, 2000; Reiter, 2007).\\nRecently, neu-\\nral encoder-decoder models based on attention\\nand copy mechanisms have shown promising re-\\nsults (Gehrmann et al., 2018; Puduppully et al.,\\n2018, 2019; Castro Ferreira et al., 2019). Further-\\nmore, recent progress on pretrained models such\\nas GPT-2 (Radford et al., 2018), BART (Lewis\\net al., 2020) and T5 (Raffel et al., 2020) has shown\\neffective results for text generation tasks on ma-\\nchine translation, summarization, and conversation\\nresponse generation. Chen et al. (2020c); Peng\\net al. (2020); Kale (2020) also ﬁnetune pretrained\\nmodels on Data-to-Text tasks.\\n6\\nConclusion\\nIn this paper, we introduce DART, an open-domain\\ncorpus for structured data record to text generation.\\nDART’s ontology-preserving representation of data\\ninputs differentiates itself from other open-domain\\nData-to-Text corpora. We found that DART in-\\ntroduces new challenges to several state-of-the-art\\nData-to-Text models due to its open-domain nature\\nand its ontology structure of the semantic triple\\ninput. Furthermore, we found that using it for data\\naugmentation improves other Data-to-Text tasks.\\nFor future work, we will explore more controlled,\\nhigh-ﬁdelity generation that better incorporates the\\nontology hierarchy of data.\\n7\\nEthics Statement\\nOur dataset is constructed by accumulating and\\nprocessing resources from various existing datasets\\nthat are open to the public. In addition, we collect\\nannotations on structure of tabular data and human\\nwritten sentences that describe data records.\\nThe existing resources that we utilize mainly\\nconsist of (1) tabular data from Wikipedia, (2) in-\\nformation of restaurants presented with dialogue-\\nact meaning representation and its textual descrip-\\ntion (E2E), and (3) information of various entities\\nand their relationship that are in 15 different cate-\\ngories of DBPedia, which is a knowledge base built\\non contents created in various Wikimedia projects\\n(WebNLG). It is possible that there are biases in\\nthese resources, either in the tabular data or the\\ntextual description written by humans.\\nFor additional annotations we collected, we have\\ntwo groups of annotators participating: internal\\nannotators who are the authors of this work, and\\nexternal annotators recruited from the Amazon Me-\\nchanical Turk platform. On MTurk, we use a pay\\nrate of $15 per hour approximately based on our\\nestimation of the time it takes to complete our anno-\\ntation tasks. In total, it took 125 hours to complete\\nall tasks on the Amazon Mechanical Turk platform.\\nThere are three annotation tasks: (1) Annotators\\nare asked to specify ontological structure of the\\ntable by indicating relationship between table col-\\numn headers, (2) Annotators are asked to write\\ndescriptions that are ﬂuent and semantically faith-\\nful to the data records presented to them, and (3)\\nAnnotators are asked to evaluate sentences that are\\neither references or model generated outputs. We\\nacknowledge that it is also possible to have biases\\nin the sentences written by the annotators, or in the\\ndata records that are presented to them.\\nWe conducted experiments on our own dataset\\nand the WebNLG dataset using BART and T5, two\\nlarge-scale pretrained models. Both models are\\ntrained on large amounts of textual data such as\\nnews, books, and web text, which may contain any\\nkinds of biases. As a result, it is possible to insert\\nthose biases into the models.\\nIn total, we conducted 43 experiments: 7 on\\nDART and 36 for our ablation study on the\\nWebNLG dataset. We use a single NVIDIA V100\\nGPU for all experiments and each experiment took\\nfrom 5 to 40 hours depending on the model size.\\nAcknowledgement\\nThe authors would like to thank the anonymous\\nreviewers for their discussion and feedback.\\nReferences\\nJunwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuan-\\nhua Lv, Ming Zhou, and Tiejun Zhao. 2018. Table-\\nto-text: Describing table region with natural lan-\\nguage. In AAAI.\\nThiago Castro Ferreira, Chris van der Lee, Emiel van\\nMiltenburg, and Emiel Krahmer. 2019. Neural data-\\nto-text generation: A comparison between pipeline\\nand end-to-end architectures. In EMNLP.\\nAlison J Cawsey, Bonnie L Webber, and Ray B Jones.\\n1997. Natural language generation in health care.\\nDavid L Chen and Raymond J Mooney. 2008. Learn-\\ning to sportscast: a test of grounded language acqui-\\nsition. In ICML.\\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\\nWilliam Yang Wang. 2020a.\\nLogical natural lan-\\nguage generation from open-domain tables. In ACL.\\nZhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou\\nZhou, Yunkai Zhang, Sairam Sundaresan, and\\nWilliam Yang Wang. 2020b.\\nLogic2Text: High-\\nﬁdelity natural language generation from logical\\nforms. In Findings of EMNLP.\\nZhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu,\\nand William Yang Wang. 2020c. Few-shot nlg with\\npre-trained language model. In ACL.\\nDorottya Demszky, Kelvin Guu, and Percy Liang. 2018.\\nTransforming question answering datasets into nat-\\nural language inference datasets.\\narXiv preprint\\narXiv:1809.02922.\\nBhuwan Dhingra, Manaal Faruqui, Ankur Parikh,\\nMing-Wei Chang, Dipanjan Das, and William Co-\\nhen. 2019. Handling divergent reference texts when\\nevaluating table-to-text generation. In ACL.\\nOndˇrej Dušek, David M. Howcroft, and Verena Rieser.\\n2019. Semantic noise matters for neural natural lan-\\nguage generation. In INLG.\\nHady Elsahar, Pavlos Vougiouklis, Arslen Remaci,\\nChristophe Gravier,\\nJonathon Hare,\\nFrederique\\nLaforest, and Elena Simperl. 2018. T-REx: A large\\nscale alignment of natural language with knowledge\\nbase triples. In LREC.\\nAngela Fan, Claire Gardent, Chloé Braud, and An-\\ntoine Bordes. 2019. Using local knowledge graph\\nconstruction to scale Seq2Seq models to multi-\\ndocument inputs. In EMNLP-IJCNLP.\\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\\nand Laura Perez-Beltrachini. 2017. The WebNLG\\nchallenge: Generating text from RDF data. In INLG.\\nSebastian Gehrmann, Falcon Dai, Henry Elder, and\\nAlexander Rush. 2018. End-to-end content and plan\\nselection for data-to-text generation. In INLG.\\nValerie Hajdik, Jan Buys, Michael Wayne Goodman,\\nand Emily M Bender. 2019. Neural text generation\\nfrom rich semantic representations. In NAACL.\\nHamza Harkous, Isabel Groves, and Amir Saffari. 2020.\\nHave your text and use it too! end-to-end neural\\ndata-to-text generation with semantic ﬁdelity. arXiv\\npreprint arXiv:2004.06577.\\nMihir Kale. 2020. Text-to-text pre-training for data-to-\\ntext tasks. arXiv preprint arXiv:2005.10433.\\nGregg Kellogg, Ivan Herman, and Jeremy Tandy.\\n2015.\\nGenerating\\nRDF\\nfrom\\ntabular\\ndata\\non the web.\\nW3C recommendation,\\nW3C.\\nHttps://www.w3.org/TR/2015/REC-csv2rdf-\\n20151217/.\\nIoannis Konstas and Mirella Lapata. 2012. Unsuper-\\nvised concept-to-text generation with hypergraphs.\\nIn NAACL.\\nRémi Lebret, David Grangier, and Michael Auli. 2016.\\nNeural text generation from structured data with ap-\\nplication to the biography domain. In EMNLP.\\nMike\\nLewis,\\nYinhan\\nLiu,\\nNaman\\nGoyal,\\nMar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy,\\nVes\\nStoyanov,\\nand\\nLuke\\nZettlemoyer.\\n2020. BART: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation,\\nand comprehension. In ACL.\\nPercy Liang, Michael I Jordan, and Dan Klein. 2009.\\nLearning semantic correspondences with less super-\\nvision. In ACL.\\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\\nRen. 2020. CommonGen: A constrained text gener-\\nation challenge for generative commonsense reason-\\ning. In Findings of EMNLP.\\nTianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,\\nand Zhifang Sui. 2018. Table-to-text generation by\\nstructure-aware seq2seq learning. In AAAI.\\nAmit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.\\nStep-by-step: Separating planning from realization\\nin neural data-to-text generation. In NAACL.\\nJekaterina Novikova, Ondˇrej Dušek, Amanda Cercas\\nCurry, and Verena Rieser. 2017a. Why we need new\\nevaluation metrics for nlg. In EMNLP.\\nJekaterina Novikova, Ondrej Dusek, and Verena Rieser.\\n2017b. The E2E dataset: New challenges for end-to-\\nend generation. In SIGDIAL.\\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann,\\nManaal Faruqui, Bhuwan Dhingra, Diyi Yang, and\\nDipanjan Das. 2020. ToTTo: A controlled table-to-\\ntext generation dataset. In EMNLP.\\nPanupong Pasupat and Percy Liang. 2015. Composi-\\ntional semantic parsing on semi-structured tables. In\\nACL.\\nBaolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun\\nLi, Jinchao Li, Michael Zeng, and Jianfeng Gao.\\n2020.\\nFew-shot natural language generation for\\ntask-oriented dialog. In arXiv.\\nRatish Puduppully, Li Dong, and Mirella Lapata. 2018.\\nData-to-text generation with content selection and\\nplanning. In AAAI.\\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019.\\nData-to-text generation with entity modeling.\\nIn\\nACL.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\\nIlya Sutskever. 2018.\\nImproving language under-\\nstanding by generative pre-training.\\nTechnical re-\\nport, OpenAI.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2020. Exploring the lim-\\nits of transfer learning with a uniﬁed text-to-text\\ntransformer. Journal of Machine Learning Research,\\n21(140):1–67.\\nEhud Reiter. 2007.\\nAn architecture for data-to-text\\nsystems. In Proceedings of the Eleventh European\\nWorkshop on Natural Language Generation (ENLG\\n07).\\nEhud Reiter. 2020. Openai gpt system: What does it\\ndo? Technical report, Arria.\\nEhud Reiter and Robert Dale. 2000. Building natural\\nlanguage generation systems. Cambridge university\\npress.\\nLeonardo F. R. Ribeiro, Claire Gardent, and Iryna\\nGurevych. 2019.\\nEnhancing AMR-to-text genera-\\ntion with dual graph representations. In EMNLP.\\nLeonardo F. R. Ribeiro, Martin Schmitt, Hinrich\\nSchütze, and Iryna Gurevych. 2020. Investigating\\npretrained language models for graph-to-text gener-\\nation. arXiv.\\nThibault Sellam, Dipanjan Das, and Ankur P Parikh.\\n2020.\\nBLEURT: Learning robust metrics for text\\ngeneration. In ACL.\\nLinfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo\\nWang, and Daniel Gildea. 2017. Amr-to-text gener-\\nation with synchronous node replacement grammar.\\nIn ACL.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In NeurIPS.\\nPavlos Vougiouklis,\\nHady ElSahar,\\nLucie-Aimée\\nKaffee, Christophe Gravier, Frédérique Laforest,\\nJonathon S. Hare, and Elena Simperl. 2018. Neu-\\nral wikipedian: Generating textual summaries from\\nknowledge base triples. Journal of Web Semantics,\\n52-53:1 – 15.\\nTsung-Hsien Wen,\\nMilica Gaši´c,\\nNikola Mrkši´c,\\nLina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes,\\nDavid Vandyke, and Steve Young. 2016.\\nCondi-\\ntional generation and snapshot learning in neural di-\\nalogue systems. In EMNLP.\\nTsung-Hsien Wen, Milica Gaši´c, Nikola Mrkši´c, Pei-\\nHao Su, David Vandyke, and Steve Young. 2015.\\nSemantically conditioned LSTM-based natural lan-\\nguage generation for spoken dialogue systems. In\\nEMNLP.\\nSam Wiseman, Stuart Shieber, and Alexander Rush.\\n2017.\\nChallenges in data-to-document generation.\\nIn EMNLP.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\\nWeinberger, and Yoav Artzi. 2020.\\nBERTScore:\\nEvaluating text generation with BERT. In ICLR.\\nChao Zhao, Marilyn Walker, and Snigdha Chaturvedi.\\n2020. Bridging the structural gap between encoding\\nand decoding for data-to-text generation. In ACL.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\\ntian M Meyer, and Steffen Eger. 2019. MoverScore:\\nText generation evaluating with contextualized em-\\nbeddings and earth mover distance. In EMNLP.\\nVictor Zhong, Caiming Xiong, and Richard Socher.\\n2017.\\nSeq2sql:\\nGenerating structured queries\\nfrom natural language using reinforcement learning.\\nCoRR, abs/1709.00103.\\nAppendix\\nThe Appendix contains the following contents:\\n• Results of the ablation study on WebNLG 2017 testset.\\n• Statistics of the table ontology annotations.\\n• Examples of tables that help illustrate DART’s annotation procedure.\\n• Examples of model outputs.\\nModel\\nExperiment\\nBLEU ↑\\nMETEOR ↑\\nTER ↓\\nSEEN\\nUNSEEN\\nALL\\nSEEN\\nUNSEEN\\nALL\\nSEEN\\nUNSEEN\\nALL\\nBaseline\\nTransformer\\n[1] webnlg\\n49.81\\n5.51\\n31.81\\n0.39\\n0.09\\n0.24\\n0.47\\n0.86\\n0.64\\n[2] webnlg+dart_decl_sents\\n52.31\\n8.96\\n39.98\\n0.40\\n0.07\\n0.25\\n0.45\\n0.79\\n0.60\\n[3] webnlg+dart_human_annotated\\n53.68\\n7.02\\n36.36\\n0.40\\n0.09\\n0.26\\n0.43\\n0.79\\n0.59\\n[4] webnlg+dart_ontology\\n53.40\\n8.54\\n38.51\\n0.41\\n0.08\\n0.26\\n0.44\\n0.80\\n0.60\\n[5] webnlg+dart_e2e\\n51.76\\n5.92\\n32.36\\n0.40\\n0.09\\n0.25\\n0.45\\n0.86\\n0.63\\n[6] webnlg+dart_full\\n54.99\\n8.64\\n39.11\\n0.40\\n0.08\\n0.25\\n0.42\\n0.81\\n0.60\\nBART-base\\n[1] webnlg\\n63.02\\n41.74\\n53.36\\n0.45\\n0.35\\n0.40\\n0.33\\n0.52\\n0.42\\n[2] webnlg+dart_decl_sents\\n62.71\\n42.51\\n53.64\\n0.45\\n0.36\\n0.40\\n0.34\\n0.51\\n0.41\\n[3] webnlg+dart_human_annotated\\n62.36\\n46.21\\n55.14\\n0.44\\n0.37\\n0.41\\n0.34\\n0.45\\n0.39\\n[4] webnlg+dart_ontology\\n62.62\\n46.74\\n55.54\\n0.44\\n0.38\\n0.41\\n0.34\\n0.45\\n0.39\\n[5] webnlg+dart_e2e\\n64.00\\n35.07\\n51.17\\n0.45\\n0.33\\n0.40\\n0.33\\n0.61\\n0.46\\n[6] webnlg+dart_full\\n63.66\\n45.48\\n55.52\\n0.45\\n0.37\\n0.41\\n0.33\\n0.47\\n0.40\\nBART-large\\n[1] webnlg\\n63.71\\n44.17\\n54.95\\n0.46\\n0.39\\n0.42\\n0.33\\n0.51\\n0.41\\n[2] webnlg+dart_decl_sents\\n65.18\\n46.79\\n56.79\\n0.46\\n0.39\\n0.42\\n0.32\\n0.48\\n0.40\\n[3] webnlg+dart_human_annotated\\n64.51\\n50.20\\n58.06\\n0.46\\n0.40\\n0.43\\n0.32\\n0.44\\n0.38\\n[4] webnlg+dart_ontology\\n64.19\\n49.62\\n57.65\\n0.46\\n0.39\\n0.43\\n0.33\\n0.45\\n0.38\\n[5] webnlg+dart_e2e\\n65.06\\n30.17\\n48.24\\n0.46\\n0.33\\n0.40\\n0.32\\n0.69\\n0.49\\n[6] webnlg+dart_full\\n65.24\\n47.96\\n57.44\\n0.46\\n0.39\\n0.43\\n0.32\\n0.46\\n0.39\\nT5-small\\n[1] webnlg\\n65.30\\n45.58\\n56.57\\n0.46\\n0.39\\n0.43\\n0.32\\n0.49\\n0.40\\n[2] webnlg+dart_decl_sents\\n64.18\\n46.61\\n56.27\\n0.46\\n0.39\\n0.43\\n0.33\\n0.48\\n0.40\\n[3] webnlg+dart_human_annotated\\n65.05\\n47.81\\n57.32\\n0.46\\n0.40\\n0.43\\n0.33\\n0.46\\n0.39\\n[4] webnlg+dart_ontology\\n65.17\\n47.49\\n57.24\\n0.46\\n0.39\\n0.43\\n0.32\\n0.47\\n0.39\\n[5] webnlg+dart_e2e\\n65.56\\n41.28\\n54.56\\n0.46\\n0.38\\n0.42\\n0.32\\n0.54\\n0.42\\n[6] webnlg+dart_full\\n64.70\\n47.56\\n57.01\\n0.46\\n0.39\\n0.43\\n0.33\\n0.47\\n0.39\\nT5-base\\n[1] webnlg\\n64.89\\n52.86\\n59.44\\n0.46\\n0.42\\n0.44\\n0.33\\n0.42\\n0.37\\n[2] webnlg+dart_decl_sents\\n65.44\\n50.80\\n58.81\\n0.46\\n0.41\\n0.44\\n0.32\\n0.43\\n0.37\\n[3] webnlg+dart_human_annotated\\n65.42\\n50.71\\n58.80\\n0.46\\n0.41\\n0.44\\n0.32\\n0.43\\n0.37\\n[4] webnlg+dart_ontology\\n65.17\\n51.49\\n59.04\\n0.46\\n0.41\\n0.44\\n0.33\\n0.43\\n0.37\\n[5] webnlg+dart_e2e\\n65.11\\n49.64\\n58.19\\n0.46\\n0.41\\n0.44\\n0.33\\n0.46\\n0.39\\n[6] webnlg+dart_full\\n65.99\\n51.68\\n59.50\\n0.46\\n0.42\\n0.44\\n0.32\\n0.43\\n0.37\\nT5-large\\n[1] webnlg\\n64.89\\n54.01\\n59.95\\n0.46\\n0.43\\n0.44\\n0.34\\n0.41\\n0.37\\n[2] webnlg+dart_decl_sents\\n65.97\\n53.00\\n60.12\\n0.46\\n0.42\\n0.44\\n0.32\\n0.41\\n0.36\\n[3] webnlg+dart_human_annotated\\n65.82\\n56.01\\n61.44\\n0.46\\n0.43\\n0.45\\n0.32\\n0.38\\n0.35\\n[4] webnlg+dart_ontology\\n65.53\\n55.20\\n60.90\\n0.46\\n0.42\\n0.44\\n0.32\\n0.38\\n0.35\\n[5] webnlg+dart_e2e\\n66.27\\n54.13\\n60.76\\n0.46\\n0.43\\n0.45\\n0.32\\n0.41\\n0.36\\n[6] webnlg+dart_full\\n65.78\\n54.35\\n60.64\\n0.46\\n0.42\\n0.44\\n0.32\\n0.39\\n0.35\\nTable 6: Results of ablation study on WebNLG 2017 testset. dart_decl_sents refers to DART partition that contains\\nauto-generated declarative sentences mentioned in Section 2.2, dart_human_annotated refers to partition that\\ncontains human written sentences mentioned in Section 2.1, dart_ontology is the combination of dart_decl_sents\\nand dart_human_annotated, and dart_e2e refers to DART partition containing instances extracted from E2E\\ndataset, the process of which is mentioned in Section 2.3. Note that dart_full is the combination of dart_ontology\\nand dart_e2e.\\nTables\\nOntology depth\\n(min, med, max)\\nNodes in ontology\\n(min, med, max)\\nBranching factor\\n(mean)\\nWikiTableQuestions\\n2060\\n1, 1, 4\\n2, 6, 25\\n4.0\\nWikiSQL\\n3563\\n1, 1, 4\\n3, 7, 25\\n5.1\\nTable 7: Properties of the ontology in the WikiTableQuestions and WikiSQL samples in DART. Branching factor\\nrefers to the average number of children across all non-leaf nodes in a table’s ontology.\\nFigure 3:\\nDistribution of column ontology depths in the WikiTableQuestions and WikiSQL samples in\\nDART v1.1.1.\\n<entry category=\"MISC\" eid=\"Id5\" size=\"3\">\\n<modifiedtripleset>\\n<mtriple>Apertura 2006 | JORNADA_OR_OTHER | Semifinals Ida</mtriple>\\n<mtriple>Semifinals Ida | AWAY_TEAM | América</mtriple>\\n<mtriple>Semifinals Ida | HOME_TEAM | Chivas</mtriple>\\n</modifiedtripleset>\\n<lex comment=\"WikiTableQuestions\" lid=\"Id1\">\\nChivas and América will compete in the semifinals of the Apertura 2006 tournament.\\n</lex>\\n</entry>\\n<entry category=\"MISC\" eid=\"Id76\" size=\"6\">\\n<modifiedtripleset>\\n<mtriple>Terry Jenkins | ROUND | 1st Round</mtriple>\\n<mtriple>Terry Jenkins | YEAR | 2014</mtriple>\\n<mtriple>[TABLECONTEXT] | [TITLE] | PDC World Darts Championship</mtriple>\\n<mtriple>1st Round | OPPONENT | Per Laursen</mtriple>\\n<mtriple>1st Round | RESULT | Lost</mtriple>\\n<mtriple>[TABLECONTEXT] | PLAYER | Terry Jenkins</mtriple>\\n</modifiedtripleset>\\n<lex comment=\"WikiTableQuestions\" lid=\"Id1\">\\nTerry Jenkins lost the game with Per Laursen in\\nthe 1st Round of 2014 PDC World Darts Championship\\n</lex>\\n</entry>\\nFigure 4: Examples of DART instance\\nFigure 5: An example of the data cleaning. The top left table had a missing column name and the table title was\\nnot speciﬁc to the data; our internal annotators add the missing column name “Year” and linked the rest of the\\ncolumns to the “Year” column. The bottom left table had repeat column names in the table; our internal annotators\\ndisambiguate the columns by making the column names more speciﬁc.\\nFigure 6: A WikiTableQuestions table that uses [TITLE] in the ontology.\\nFigure 7: A manually annotated table from WikiTableQuestions with a sentence that uses the table title.\\nFigure 8: A manually annotated table from WikiTableQuestions. Annotators created a table ontology, and they\\nwrote sentences encapsulating the information in the orange cells for a given row. Whenever a sentence referenced\\nthe table title, that sentence was also highlighted green.\\nFigure 9: An example of collected MTurk-generated sentences for WikiTableQuestions. Internal annotators went\\nthrough the generated sentences and checked for both sentence coherence and title usage. Below the generated\\nsentences, ‘y’ meant the sentence references the table title, ‘n’ meant the sentence did not use the table title, ‘x’\\nmeant the sentence was nonsensical.\\nFigure 10: Automatically generated declarative sentences from WikiSQL with human validation. Annotators went\\nthrough the generated sentences and checked for both sentence coherence and title use. Below the generated\\nsentences, ‘y’ meant the sentence references the table title, ‘n’ meant the sentence did not use the table title, ‘x’\\nmeant the sentence was nonsensical.\\n- Sample 1 -\\nInput triples:\\n<H> Peru Earthquake <R> scale of disaster <T> 250k homeless\\n<H> Peru Earthquake <R> year <T> 2007\\nBART-base output: 250k people were killed in the 2007 philippine earthquake .\\n- Sample 2 -\\nInput triples:\\n<H> [TABLECONTEXT] <R> game <T> 3\\n<H> 3 <R> attendance <T> 10 637\\n<H> [TABLECONTEXT] <R> [title] <T> 2006 Minnesota Swarm season\\nBART-base output: the minnesota swarm played in front of a crowd of 10 , 684 people .\\n- Sample 3 -\\nInput triples:\\n<H> Andrew Phelps McCormick <R> state <T> TX\\n<H> Andrew Phelps McCormick <R> active <T> 1892-1916\\nT5-base output: andrew phelps mccormick was active from 1892 to 1616 in texas .\\nFigure 11: Examples of hallucinated outputs of pretrained models trained on DART\\n- Sample 1 -\\nInput triples:\\n<H> Andrew Rayel <R> associated Band/associated Musical Artist <T> Christian Burns\\n<H> Andrew Rayel <R> associated Band/associated Musical Artist <T> Jonathan Mendelsohn\\nreference:\\nandrew rayel , is associated with musical artist jonathan mendelsohn and christian burns .\\ntrain on WebNLG - BART-base output:\\nchristian mendelsohn and andrew rayel are both associated with the same band , christian burns .\\ntrain on DART - BART-base output:\\nandrew rayel is associated with christian burns and jonathan mendelsohn .\\n- Sample 2 -\\nInput triples:\\n<H> Indie rock <R> stylistic Origin <T> New wave music\\nreference: the stylistic origin of indie rock is new wave music .\\ntrain on WebNLG - BART-base output:\\nthe alternative rock genre is new wave .\\ntrain on DART - BART-base output:\\nindie rock is influenced by new wave music .\\n- Sample 3 -\\nInput triples:\\n<H> Abradab <R> associated Band/associated Musical Artist <T> Magik rapper\\n<H> Abradab <R> associated Band/associated Musical Artist <T> Kaliber 44\\nreference:\\nabradab , an artist for the band kaliber 44 , is associated with magik ( rapper ) .\\ntrain on WebNLG - BART-base output:\\nmagiber 44 is the creator of abradab , which is also associated with the magik rapper .\\ntrain on DART - BART-base output:\\nmagik rapper and kaliber 44 are the associated musicians of abradab .\\n- Sample 4 -\\nInput triples:\\n<H> Alfa Romeo 164 <R> assembly <T> Milan\\n<H> Alfa Romeo 164 <R> related Mean Of Transportation <T> Saab 9000\\nreference:\\nthe alfa romeo 164 , which is assembled in milan , is a related means of transportation to saab 9000 ,\\nin that they are both cars .\\ntrain on WebNLG - T5-base output:\\nalfa romeo 164 is a transport vehicle for saab 9000 and is found in milan .\\ntrain on DART - T5-base output:\\nalfa romeo 164 ( assembled in milan ) is a related transport vehicle to saab 9000 .\\n- Sample 5 -\\nInput triples:\\n<H> Akeem Ayers <R> former Team <T> Tennessee Titans\\n<H> Akeem Ayers <R> draft Pick <T> 39\\nreference:\\nakeem ayers ’ former team was tennessee titans and he was number 39 in the draft pick .\\ntrain on WebNLG - T5-large output:\\nakeem ayers was drafted with the 39th pick by the tennessee titans .\\ntrain on DART - T5-large output:\\nakeem ayers , a former player of the tennessee titans , was the 39th draft pick .\\nFigure 12: Examples of model outputs - with or without DART data augmentation\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = document['text']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert split words on line break, e.g. post-\\nediting\n",
    "text = text.replace('-\\n', '')\n",
    "#remove new lines in the middle of the sentence\n",
    "one_new_line = r'(?<![\\.\\n])\\n(?!\\n)'\n",
    "text = re.sub(one_new_line, ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DART: Open-Domain Structured Data Record to Text Generation Linyong Nan1 Dragomir Radev1,2 Rui Zhang3 Amrit Rau1 Abhinand Sivaprasad1 Chiachun Hsieh4 Xiangru Tang1 Aadit Vyas1 Neha Verma1 Pranav Krishna5 Yangxiaokang Liu1 Nadia Irwanto1 Jessica Pan1 Faiaz Rahman1 Ahmad Zaidi1 Murori Mutuma1 Yasin Tarabar1 Ankit Gupta1 Tao Yu1 Yi Chern Tan1 Xi Victoria Lin2∗ Caiming Xiong2 Richard Socher2 Nazneen Fatema Rajani2 1 Yale University 2 Salesforce Research 3 Penn State University 4 The University of Hong Kong 5 MIT {linyong.nan, dragomir.radev}@yale.edu, rmz5227@psu.edu, nazneen.rajani@salesforce.com Abstract We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-Text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and dialogue-act-based meaning representation tasks by utilizing techniques such as: tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate uniﬁcation, all with minimum postediting. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-LILY/ dart.\\n1 Introduction Automatically generating textual descriptions from structured data improves the accessibility of knowledge bases to lay users. Such applications include explaining data records to non-experts (Cawsey et al., 1997), writing sports news (Chen and Mooney, 2008), summarizing information in multiple documents (Fan et al., 2019), and generating dialogue responses (Wen et al., 2015).\\nWhile signiﬁcant progress has been made in this ﬁeld, there are still several issues with existing Data-to-Text datasets. First, they adopt a ﬂat ontology structure of the data, such as slot-value pairs for data records (Lebret et al., 2016; Novikova et al., ∗Now at Facebook AI.\\n2017b) or ﬂat schema for tables (Wiseman et al., 2017; Chen et al., 2020a; Parikh et al., 2020). This ﬂat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia.\\nFurthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018).\\nTo address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia tables, which is signiﬁcantly richer than those deﬁned in the domain speciﬁc ontologies E2E and WebNLG were built on (Table 2). We also introduce novel tree ontology annotation on tables, which converts a ﬂat table schema into a tree structured semantic frame. The tree ontology reﬂects the core and auxiliary relations in the table schema, and naturally occurs across many domains. As a result, DART provides high-quality sentence annotations to tree structured semantic frames extracted from various data sources, including WikiSQL (Zhong et al., 2017) and WikiTableQuestions (Pasupat and Liang, 2015), two open-domain question answering datasets, as well as E2E (Novikova et al., 2017b) and WebNLG (Gardent et al., 2017) (Figure 1). We evaluated several state-of-the-art data-to-text models on DART, and found that while these models achieve impressive performance on arXiv:2007.02871v2  [cs.CL]  12 Apr 2021 domain-speciﬁc datasets, their performance suffers on DART due to its open-domain nature and richer semantic structures.\\nOur contributions are as follows. (1) We present a large and open-domain corpus for structured data record to text generation, annotated with tree ontologies converted from the table. This hierarchical input differentiates our corpus from existing datato-text corpora. (2) We benchmark several stateof-the-art data-to-text models to show that DART introduces new generalization challenges. (3) We demonstrate that using DART for data augmentation improves the performance of existing models on the WebNLG 2017 dataset. We expect the results to generalize to other data-to-text datasets given the open-domain nature of DART.\\n2 DART Data Collection As shown in Figure 1, DART is constructed from three different sources: (1) human annotation on Wikipedia tables from two table semantic parsing and question answering datasets WikiSQL and WikiTableQuestions (§ 2.1), (2) automatic conversion of questions in WikiSQL to declarative sentences (§ 2.2), and (3) incorporation of existing datasets including WebNLG 2017 and Cleaned E2E (§ 2.3).\\nAfter collecting the ⟨triple-set, sentence⟩ pairs from various data sources, we manually canonicalized the predicates and show that DART covers a broad range of topics (§ 2.4). Finally, we discuss the data split in § 2.5.\\n2.1 Tree Ontology and Sentence Annotation on Tables Tables are a major source of structured data that contain a wealth of information complementary to text and knowledge graphs.\\nWe aim to collect ⟨triple-set, sentence⟩ pairs from open-domain Wikipedia tables.\\nHowever, table schema are ﬂat, making them not directly usable for building subject-predicate-object triples to capture rich relationships in the data.\\nAs shown in Figure 2, we propose a two-stage annotation process that involves two groups of annotators: internal annotators and Amazon Mechanical Turk1 workers. In the ﬁrst stage, skilled internal annotators specify the parent of every column header to construct a tree-structured ontology for each table. In the second stage, both internal and external annotators provide a sentential description of the 1https://www.mturk.com/ highlighted cells in a row that are automaticallychosen based on the ontology.\\nTree Ontology Annotation For each column in a given table, our internal annotators labeled its ontological parent. In Figure 2, for example, the annotator would provide the sequence {NULL, TEAM, STADIUM, STADIUM, TEAM} as the parent of each column — column TEAM has no parent, STADIUM has parent TEAM, and so on. In many cases, the relationship between a parent column and its child column can be conceptualized as a \"has-a\" relationship. For tables that are malformed or have duplicate or missing column names (as shown in Figure 5 of the Appendix), annotators either changed or added appropriate column names in order to ﬁt these patterns. For each table we generate an ontology tree whose root is always [TABLECONTEXT].\\nThis root node either has (1) one child node [TITLE] in the cases where the table title is the subject of entire table, or (2) column header node(s) and a [TITLE] node as children, as shown in Figure 2.\\nThis is because in some tables, the table title itself is more appropriate to be the root of the ontology tree (example shown in Figure 6 of the Appendix).\\nIn these cases, annotators assigned the special token [TITLE] as the parent of the relevant column nodes. For other tables, title usually provides important context for understanding the table’s rows (example shown in Figure 7 of the Appendix). In such cases, [TITLE] is made a child of [TABLECONTEXT] together with the column headers that are appropriate.\\nWe evaluate the quality of the initial tree ontology annotation and made corrections with the following procedure: (1) reject and request corrections from the original annotators if the provided ontology is disconnected or contains a cycle, (2) verify that all column headers appear as a node in the tree. For many tables, the determination of an ontology is a subjective process with many \"correct\" answers - for example, swapping the positions of TEAM and CITY in the tree in Figure 2 produces an equally valid ontology for the referenced table.\\nIf there are multiple ways to construct an ontology based on annotators’ decisions of attribute relationships among column headers, we manually unify the annotations for similar tables (for examples, tables about athletes in different sports). The ontologies exhibit a great deal of structural variety.\\nRelevant statistics are summarized in Table 7 and Figure 3 of the Appendix.\\nFigure 1: DART data collection pipeline. MR: Meaning Representation.\\nInput Unit Examples Vocab Size Words per SR Sents per SR Tables WikiTableText Row 13,318 — 13.9 1.0 4,962 LogicNLG Table 37,015 122K 13.8 1.0 7,392 ToTTo Highlighted Cells 136,161 136K 17.4 1.0 83,141 DART Triple Set 82,191 33.2K 21.6 1.5 5,623 Table 1: DART compared with other open-domain table-to-text datasets. DART takes triple sets as input by incorporating the ontology of table headers and title, and its surface realizations tend to be longer with more than single sentence verbalization. SR: Surface Realization.\\nDART: 62,659 train / 6,980 dev / 12,552 test WikiTableQuestions WikiSQL WebNLG Cleaned E2E Internal MTurk Internal Declarative Domains Wikipedia (open-domain) 15 DBPedia Categories Restaurants Unique Predicates 1,950 1,403 493 2,008 347 7 Unique Triples 13,505 5,541 1,648 7,787 3,220 946 Tripleset-Sentence Pairs 4,902 2,120 772 4,204 27,731 42,462 Triples per Tripleset (min, med, max) 1, 3, 10 1, 3, 7 1, 2, 7 1, 2, 10 1, 3, 7 1, 4, 7 Vocab Size 13.4K 8.9K 3.0K 10.7K 8.0K 3.0K Words per SR 15.2 16.5 14.0 12.6 22.5 22.9 Sentences per SR 1.0 1.1 1.0 1.0 1.4 1.6 Table 2: Statistics of DART decomposed by different collection methods. DART exhibits a great deal of topical variety in terms of the number of unique predicates, the number of unique triples, and the vocabulary size.\\nConnected Component Extraction After we annotated the ontology, we automatically choose a subset of cells for a selected table row to form the triple set. Randomly selecting cells leads to poor quality annotation as the selected data could lack a subject, lack cohesion, or would require information not encoded in the ontology to form a coherent sentence. For example, in Figure 2, if only two nodes CITY and CAPACITY were highlighted then a coherent sentence cannot be produced as there is no direct logical relationship (functional dependency) between them. To solve these issues, instead of randomly selecting cells in a row, we extract connected components from the ontology.\\nThe extracted components have two controllable properties: size and shape. To create variation in size, we randomly sampled between [2, 5]. The shape is determined by two numbers: the number of sibling node pairs and parent-child node pairs.\\nIncreasing the number of sibling node pairs creates a wider tree, while increasing the latter creates a deeper tree. We created a sliding scale between width and depth using an expansion parameter, p.\\nWe recursively visit a node if it has children with probability p and otherwise move to a sibling if it exists. If p = 1, the search becomes a DFS and if p = 0, it becomes BFS. We found that randomly selecting p from 0.5 to 0.7 created a reasonable variation in extracted component shapes. This ensures the balance between breadth and depth of ontology coverage of the selected cells, therefore ensuring the quality of the sentence annotation.\\nSentence Annotation Given the table, title, and connected highlighted cells of a row, annotators Figure 2: Overview of our human annotation procedure. Top panel: We collect the parent-child relations between columns from internal annotators (yellow is parent, green is child). Then, we collect a surface realization of the cells highlighted in orange. Middle panel: We use the provided parent-child relations to construct an ontology tree on the columns, then select the nodes corresponding to the highlighted cells. We gather a connected subtree by collecting all nodes leading up to the highlighted cells’ lowest common ancestor. Bottom panel: We extract a set of triples from the subtree as shown. This triple-set is paired with the provided realization to form a DART instance.\\nwere asked to write a description of the highlighted cells. We encouraged the annotators to use diverse vocabulary and syntactic structures. To ensure quality, internal annotators reviewed every crowd sourced sentence for correctness. They either rewrote or discarded the sentences that were nonsensical or incorrect. In some cases, they also changed cell highlighting patterns to match the sentence provided.\\nBuild Tripleset-Sentence Pairs Finally, we convert the highlighted cells to triplesets. For a row R, we start with the table’s column ontology T. We ﬁrst place the cell values in R in their corresponding slots in T, e.g. in Figure 2 we ﬁll TEAM with \"Amsterdam Admirals\". We then check that the nodes of T corresponding to the highlighted cells in R form a connected subtree. If not, we walk up the tree and highlight each traversed node up until the lowest common ancestor of the highlighted nodes (inclusive) to form a connected subtree. For each node N in the tree except the root node, we can extract the triple (parent (N), title (N), N).\\nFor example, since STADIUM is highlighted in Figure 2, we extract the triple (Amsterdam Admirals, STADIUM, Olympisch Stadion). A small number of triple-sets contained more than 10 triples. We discarded these because their associated surface realizations were of poor quality. The numbers of tripleset-sentence pairs annotated by different annotators are shown in Table 2.\\n2.2 Automatically Converting Questions to Declarative Sentences High quality natural language questions in open domain semantic parsing datasets such as WikiSQL and QA2D techniques found in automatically constructing NLI datasets (Demszky et al., 2018) present themselves as an attractive opportunity to semi-automatically construct an abundance of declarative sentences and align to table cells. We leveraged rule-based QA2D technique2 together with manual screening to combine WikiSQL questions and SQL-retrieved-answers into declarative sentences and manually ﬁltered out bad sentences.\\nWe only execute SQL queries without aggregate commands3 to retrieve answers corresponding to questions answerable by single rows. An example of such conversion is as follows: 2We use the rule-based model from https://github.\\ncom/kelvinguu/qanli (Demszky et al., 2018). The neural model code is not released.\\n3MAX, MIN, COUNT, SUM, AVG, JOIN, INTERSECT, UNION, GROUP BY, ORDER BY.\\nQuestion: In which year did Greece hold its last Summer Olympics? Answer: 2004 Declarative Sentence: Greece held its last Summer Olympics in 2004.\\nAlignment with table cells is done at two stages. We ﬁrst align sentences with corresponding rows by changing SQL commands to SELECT * and use string matching to obtain columns and column headers relevant to the answer and WHERE condition. After manually ﬁltering out bad sentences, bad alignments, or tables without ontology annotations, we were able to get 4,204 sentences. Finally, the corresponding table cells are then converted into triples in the same way as we described in Section 2.1.\\nExamples of produced declarative sentences can be found in Figure 10 of the Appendix.\\n2.3 Incorporating Existing Datasets Since they provide a large amount of strictly aligned data-text pairs with high quality sentences, we incorporate the following existing datasets in the same ⟨triple-set, sentence⟩ pair format with some modiﬁcations.\\nWebNLG 2017 An instance of the WebNLG dataset contains a set of triples extracted from DBpedia and the target text written by human. We include the WebNLG 2017 dataset4 consisting of 27731 triple-set sentence pairs with up to 7 RDF triples in a triple set covering 15 domains.\\nCleaned E2E The original E2E dataset includes dialogue act meaning representations (MR) and natural language references in the restaurant domain. Later, Dušek et al. (2019) provide Cleaned E2E5 by automatically ﬁxing the dialogue acts to account for omissions and hallucinations in the text.\\nWe incorporate Cleaned E2E because of its strict alignment between the meaning representation and the text. To convert the MR to a triple-set, we take the NAME slot (present in almost all the MRs) as the subject. For example, the MR (NAME[ALIMENTUM], AREA[CITY CENTRE], FAMILYFRIENDLY[NO]) is converted to the 4https://gitlab.com/shimorina/ webnlg-dataset/-/tree/master/webnlg_ challenge_2017 5https://github.com/tuetschek/ e2e-cleaning triple-set {(ALIMENTUM, AREA, CITY CENTRE), (ALIMENTUM, FAMILYFRIENDLY, NO)}. We drop MRs which do not contain the NAME slot.\\n2.4 Predicate Uniﬁcation We canonicalized the predicates in our triple sets such that those of the same meaning are also represented the same. We manually constructed a predicate mapping table to achieve this. As an example, our predicate mapping maps \"Hometown,\" \"Home Town,\" and \"Home Town/City\" to the uniﬁed predicate \"HOMETOWN.\" After unifying predicates, we evaluated the diversity of DART by counting the number of unique predicates in its partitions. As shown in Table 2, we see that the Wikipedia partition of DART contains much more unique predicates than the WebNLG and Cleaned E2E partitions combined, despite having smaller number of ⟨triple-set, sentence⟩ pairs.\\nThis contributes signiﬁcantly to the domain diversity of DART. In addition, we can see that DART exhibits a great deal of topical variety in terms of number of unique triples and vocabulary size.\\n2.5 Dataset Split For WebNLG 2017 and Cleaned E2E, we use their original data splits. For our annotation on WikiTableQuestions and WikiSQL, random splitting will make train, dev, and test splits contain similar tables and similar ⟨triple-set, sentence⟩ examples.\\nTherefore, to increase the generalization challenge, we compare the table title and the table header to ﬁnd similar tables, and make sure the model is evaluated on test split tables that are least similar to those used for training. We ﬁrst sample some tables as a seed test set, and then compute Jaccard similarity6 with remaining tables based on the titles and the headers. If a table has a Jaccard similarity greater than 0.5 with any of the tables in the test set, we add it into the test set. A similar process is repeated to create the dev set, and the remaining tables form the training set. This results in 62,659/6,980/12,552 sentences in the train/dev/test sets, respectively.\\n3 Experimental Results We conduct experiments on DART and the WebNLG 2017 dataset, with an ablation study on 6https://en.wikipedia.org/wiki/ Jaccard_index WebNLG to show the beneﬁts of using DART for data augmentation.\\n3.1 Models We investigate several state-of-the-art Data-to-Text generation models. We report results of the following models on DART-testset: (1) BidirectionalLSTM with attention, for which we use 2-layer bi-LSTM for encoder, with 300 dimensional word embeddings (without using pretrained word vectors), 512 hidden units and 0.3 dropout rate for the decoder. (2) Transformer (Vaswani et al., 2017), previously used by Castro Ferreira et al. (2019) on the WebNLG dataset. The input is formed by linearizing the unordered triple set. (3) BART (Lewis et al., 2020), for which we report results of both BART-base and BART-large. (4) T5 (Raffel et al., 2020): we add the same preﬁx \"translate Graph to English:\" to the input, as it is used in Ribeiro et al.\\n(2020). We report results of T5-small, T5-base and T5-large models. For both BART and T5 models, we use implementations of Ribeiro et al. (2020), with same hyperparameter setting.\\n3.2 Evaluation Metrics We use a variety of automatic metrics and human evaluation (Section 4) to evaluate the quality of the generated text. We report BLEU, METEOR, and TER which are used in the ofﬁcial WebNLG challenge. However, these measures have limitations in considering the semantic meanings of words or phrases (Novikova et al., 2017a), therefore we also report MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020), and BLEURT (Sellam et al., 2020) that incorporate semantics rather than surface forms using contextual embeddings. Furthermore, we include PARENT (Dhingra et al., 2019) which explicitly aligns n-grams from the reference and generated text to the data contents.\\n3.3 Results DART Our experimental results on DART are summarized in Table 3. The T5-large model has the highest performance among all models with a BLEU score of 50.66. We attribute this to T5’s generalization and transfer learning ability due to pretraining on multi-tasks. We can see that in general, pretrained models outperform others by a large margin, and increasing the model size seems to further boost the performance on DART. However, language models such as BART and T5 are pretrained by reconstructing text and, as a result, we found that their output on DART often contains hallucinated words (Parikh et al., 2020; Harkous et al., 2020; Reiter, 2020), as shown in Figure 11.\\nIn addition, while the pretrained model shows better text generation quality due to its generalization ability from pretraining, it does not fully capture the hierarchical ontology nature of the triple sets in their linearized input, therefore making DART more challenging. We suspect that models that are better at exploiting the ontology structure preserved in the input tripleset will achieve better performance on DART.\\nWebNLG Furthermore, we investigate if DART can improve pretrained models’ performance on other Data-to-Text generation tasks.\\nTo this end, we ﬁnetune the baseline transformer model, BART-[base, large] and T5-[small, base, large] on the WebNLG 2017 dataset, and augment the training by adding instances in the DART training set. The experimental results can be found in Table 4. We report performances of some competitive models that are not pretrained, as well as the state-of-the-art performances of pretrained models on the WebNLG 2017 dataset by Ribeiro et al.\\n(2020). On the bottom panel, we include results of experiments augmented with DART instances whose triplesets are generated with table ontology annotation, paired with human written sentences.\\nWe are able to achieve new state-of-the-art results on all WebNLG 2017 test set splits (seen, unseen and all) by ﬁnetuning T5-large on DART. We observe that using DART for data augmentation consistently improves the performance across all models, including the baseline transformer model that is not pretrained. Furthermore, we observe that more improvement is shown on unseen split of the test set, due to DART’s open-domain nature.\\nSee Figure 12 of the Appendix for example model outputs aligned with their human references.\\n3.4 Ablation Study We also conduct an ablation study on the WebNLG dataset to investigate what part of DART contributes most to improving the Data-to-Text tasks in general. We report results of the study in Table 6 of the Appendix. We divide DART into 4 partitions, where declarative sentence (auto-generated) partition and human annotated sentence partition contain instances whose triplesets are extracted from Wikipedia tables based on ontology. E2E partition contains instances converted from the E2E BLEU ↑ METEOR ↑ TER ↓ MoverScore ↑ BERTScore(F1) ↑ BLEURT ↑ PARENT ↑ LSTM with Attention 29.66 0.27 0.63 0.31 0.90 -0.13 0.35 End-to-End Transformer 27.24 0.25 0.65 0.25 0.89 -0.29 0.28 BART-base 47.11 0.38 0.46 0.51 0.95 0.37 0.55 BART-large 48.56 0.39 0.45 0.52 0.95 0.41 0.57 T5-small 47.69 0.39 0.46 0.52 0.95 0.40 0.56 T5-base 49.21 0.40 0.44 0.53 0.95 0.43 0.57 T5-large 50.66 0.40 0.43 0.54 0.95 0.44 0.58 Table 3: Model results on the test set of DART ↑: Higher is better. ↓: Lower is better.\\nBLEU ↑ METEOR ↑ TER ↓ SEEN UNSEEN ALL SEEN UNSEEN ALL SEEN UNSEEN ALL Pipeline Transformer† (Castro Ferreira et al., 2019) 56.28 23.04 42.41 0.42 0.21 0.32 0.39 0.63 0.50 Pipeline GRU† (Castro Ferreira et al., 2019) 56.09 25.12 42.73 0.42 0.22 0.33 0.39 0.64 0.51 MELBOURNE (Gardent et al., 2017) 54.52 33.27 45.13 0.41 0.33 0.37 0.40 0.55 0.47 BestPlan † (Moryossef et al., 2019) 53.30 34.41 47.24 0.44 0.34 0.39 0.47 0.56 0.51 DualEnc (Zhao et al., 2020) 63.45 36.73 51.42 0.46 0.37 0.41 0.34 0.55 0.44 PlanEnc (Zhao et al., 2020) 64.42 38.23 52.78 0.45 0.37 0.41 0.33 0.53 0.42 Ribeiro et al. (2020) BART-base ‡ 63.02 41.74 53.36 0.45 0.35 0.40 0.33 0.52 0.42 BART-large ‡ 63.71 44.17 54.95 0.46 0.39 0.42 0.33 0.51 0.41 T5-small ‡ 65.30 45.58 56.57 0.46 0.39 0.43 0.32 0.49 0.40 T5-base ‡ 64.89 52.86 59.44 0.46 0.42 0.44 0.33 0.42 0.37 T5-large ‡ 64.89 54.01 59.95 0.46 0.43 0.44 0.34 0.41 0.37 + DART BART-base 62.36 46.21 55.14 0.44 0.37 0.41 0.34 0.45 0.39 BART-large 64.51 50.20 58.06 0.46 0.40 0.43 0.32 0.44 0.38 T5-small 65.05 47.81 57.32 0.46 0.40 0.43 0.33 0.46 0.39 T5-base 65.42 50.71 58.80 0.46 0.41 0.44 0.32 0.43 0.37 T5-large 65.82 56.01 61.44 0.46 0.43 0.45 0.32 0.38 0.35 Table 4: The WebNLG 2017 results on the test set.\\n†: We report results from Zhao et al. (2020) who use the evaluation scripts that are strictly the same as the ofﬁcial challenge. ‡: We report results calculated with the model outputs on the WebNLG 2017 testset released by Ribeiro et al. (2020).\\nTripleset source Sentence source % ﬂuent % faithful % (ﬂuent+ mostly ﬂuent) % (faithful+ mostly faithful) WikiTableQuestions (§ 2.1) human-written reference 75% 81% 96% 99% BART-base 74% 57% 93% 84% T5-base 72% 54% 94% 76% WikiSQL (§ 2.2) auto-generated reference 59% 56% 87% 88% BART-base 66% 51% 92% 83% T5-base 75% 65% 97% 90% Table 5: Human evaluation over references and model outputs.\\ndataset, and WebNLG partition keeps the original data format. In general, we observe that adding DART instances that contain human written sentences brings most improvement, especially on unseen split. While adding E2E partition boosts the scores on seen test split and deteriorates the performance on unseen test split. This trend is consistent across all models. Comparing results of declarative sentence partition and human written sentence partition, we see that for most of the models, DART instances with human written sentences have better quality as it brings more improvement to the task.\\n4 Human Evaluation In Table 5, we perform human evaluation on DART based on two criteria: (1) ﬂuency if a sentence is natural and grammatical, and (2) semantic faithfulness if a sentence is supported by the input triples. We deﬁned three levels of ﬂuency: ﬂuent, mostly ﬂuent, and not ﬂuent, and the same for semantic faithfulness. We ask 5 internal annotators to evaluate on 100 triplesets sampled from declarative sentence partition and another 100 triplesets sampled from human written sentence partition. Each tripleset is paired with 3 sentences, one of them is the reference sentence, and the other two are outputs of BART-base and T5-base models.\\nThe results in Table 5 attest to the high quality of our annotations since the human written references achieve highest ﬂuency and faithfulness comparing to outputs of two strong baseline models. The evaluation on faithfulness also demonstrates that there is a considerable gap between the DART reference and the outputs of the state-of-the-art pretrained model, showing that there is a large room for improvement. We also noticed that the auto-generated declarative sentences are not as ﬂuent or faithful as the model outputs because they are generated with a rule-based system. However, we decided to release this partition, along with other partitions of DART because it demonstrates an economic way to obtain large amounts of DART instances and it also shows beneﬁts for generalization due to the diverse topics it contains.\\n5 Related Work Data-to-Text Data-to-Text generation aims to produce natural language output from structured input. Applications include generating sports commentaries (Chen and Mooney, 2008; Wiseman et al., 2017), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012), biographical texts (Lebret et al., 2016; Liu et al., 2018), knowledge-base descriptions (Gardent et al., 2017), dialogue response generation (Wen et al., 2015, 2016), and commonsense reasoning (Lin et al., 2020). Yet, most existing datasets are restricted to speciﬁc domains and applications. In contrast, a major source of DART is from Wikipedia tables covering various domains and topics.\\nRepresentation of Data The input of the Datato-Text datasets take different formats, including slot-value pairs, Abstract Meaning Representation (AMR) (Song et al., 2017; Ribeiro et al., 2019), Minimal Recursion Semantics (MRS) (Hajdik et al., 2019), Resource Description Framework (RDF triples) (Gardent et al., 2017), and logic forms (Chen et al., 2020b). There are also studies of converting tabular data to RDF triples in the Semantic Web community (Kellogg et al., 2015).\\nRecently, some open-domain table-to-text datasets have been proposed including WikiTableText (Bao et al., 2018), LogicNLP (Chen et al., 2020a), and ToTTo (Parikh et al., 2020), whose inputs are rows or entire tables. In ToTTo, highlighted cells are also provided as input, and the authors found using only highlighted cells with ﬂat row and column headers led to higher performance than using the entire table.\\nIn contrast, DART is constructed by ﬁrst annotating the tree-structured table ontology that encodes the semantic dependencies among table headers, and we could ﬂexibly incorporate additional contexts such as the table title to the ontology tree.\\nWe then use an automatic procedure to extract connected components from the tree to form the input of a DART instance. Our annotation framework not only provides a ﬂexible way of incorporating any contexts to the representation of tables, but also encodes hierarchical relationships among table headers and contexts, ensuring the extracted triples are logically consistent and can be described in text without loss of information.\\nModel Traditional Data-to-Text models break the generation progress into different stages such as signal analysis, data interpretation, document planning, microplanning, and realization (Reiter and Dale, 2000; Reiter, 2007).\\nRecently, neural encoder-decoder models based on attention and copy mechanisms have shown promising results (Gehrmann et al., 2018; Puduppully et al., 2018, 2019; Castro Ferreira et al., 2019). Furthermore, recent progress on pretrained models such as GPT-2 (Radford et al., 2018), BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) has shown effective results for text generation tasks on machine translation, summarization, and conversation response generation. Chen et al. (2020c); Peng et al. (2020); Kale (2020) also ﬁnetune pretrained models on Data-to-Text tasks.\\n6 Conclusion In this paper, we introduce DART, an open-domain corpus for structured data record to text generation.\\nDART’s ontology-preserving representation of data inputs differentiates itself from other open-domain Data-to-Text corpora. We found that DART introduces new challenges to several state-of-the-art Data-to-Text models due to its open-domain nature and its ontology structure of the semantic triple input. Furthermore, we found that using it for data augmentation improves other Data-to-Text tasks.\\nFor future work, we will explore more controlled, high-ﬁdelity generation that better incorporates the ontology hierarchy of data.\\n7 Ethics Statement Our dataset is constructed by accumulating and processing resources from various existing datasets that are open to the public. In addition, we collect annotations on structure of tabular data and human written sentences that describe data records.\\nThe existing resources that we utilize mainly consist of (1) tabular data from Wikipedia, (2) information of restaurants presented with dialogueact meaning representation and its textual description (E2E), and (3) information of various entities and their relationship that are in 15 different categories of DBPedia, which is a knowledge base built on contents created in various Wikimedia projects (WebNLG). It is possible that there are biases in these resources, either in the tabular data or the textual description written by humans.\\nFor additional annotations we collected, we have two groups of annotators participating: internal annotators who are the authors of this work, and external annotators recruited from the Amazon Mechanical Turk platform. On MTurk, we use a pay rate of $15 per hour approximately based on our estimation of the time it takes to complete our annotation tasks. In total, it took 125 hours to complete all tasks on the Amazon Mechanical Turk platform.\\nThere are three annotation tasks: (1) Annotators are asked to specify ontological structure of the table by indicating relationship between table column headers, (2) Annotators are asked to write descriptions that are ﬂuent and semantically faithful to the data records presented to them, and (3) Annotators are asked to evaluate sentences that are either references or model generated outputs. We acknowledge that it is also possible to have biases in the sentences written by the annotators, or in the data records that are presented to them.\\nWe conducted experiments on our own dataset and the WebNLG dataset using BART and T5, two large-scale pretrained models. Both models are trained on large amounts of textual data such as news, books, and web text, which may contain any kinds of biases. As a result, it is possible to insert those biases into the models.\\nIn total, we conducted 43 experiments: 7 on DART and 36 for our ablation study on the WebNLG dataset. We use a single NVIDIA V100 GPU for all experiments and each experiment took from 5 to 40 hours depending on the model size.\\nAcknowledgement The authors would like to thank the anonymous reviewers for their discussion and feedback.\\nReferences Junwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuanhua Lv, Ming Zhou, and Tiejun Zhao. 2018. Tableto-text: Describing table region with natural language. In AAAI.\\nThiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. 2019. Neural datato-text generation: A comparison between pipeline and end-to-end architectures. In EMNLP.\\nAlison J Cawsey, Bonnie L Webber, and Ray B Jones.\\n1997. Natural language generation in health care.\\nDavid L Chen and Raymond J Mooney. 2008. Learning to sportscast: a test of grounded language acquisition. In ICML.\\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and William Yang Wang. 2020a.\\nLogical natural language generation from open-domain tables. In ACL.\\nZhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, and William Yang Wang. 2020b.\\nLogic2Text: Highﬁdelity natural language generation from logical forms. In Findings of EMNLP.\\nZhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, and William Yang Wang. 2020c. Few-shot nlg with pre-trained language model. In ACL.\\nDorottya Demszky, Kelvin Guu, and Percy Liang. 2018.\\nTransforming question answering datasets into natural language inference datasets.\\narXiv preprint arXiv:1809.02922.\\nBhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William Cohen. 2019. Handling divergent reference texts when evaluating table-to-text generation. In ACL.\\nOndˇrej Dušek, David M. Howcroft, and Verena Rieser.\\n2019. Semantic noise matters for neural natural language generation. In INLG.\\nHady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. T-REx: A large scale alignment of natural language with knowledge base triples. In LREC.\\nAngela Fan, Claire Gardent, Chloé Braud, and Antoine Bordes. 2019. Using local knowledge graph construction to scale Seq2Seq models to multidocument inputs. In EMNLP-IJCNLP.\\nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In INLG.\\nSebastian Gehrmann, Falcon Dai, Henry Elder, and Alexander Rush. 2018. End-to-end content and plan selection for data-to-text generation. In INLG.\\nValerie Hajdik, Jan Buys, Michael Wayne Goodman, and Emily M Bender. 2019. Neural text generation from rich semantic representations. In NAACL.\\nHamza Harkous, Isabel Groves, and Amir Saffari. 2020.\\nHave your text and use it too! end-to-end neural data-to-text generation with semantic ﬁdelity. arXiv preprint arXiv:2004.06577.\\nMihir Kale. 2020. Text-to-text pre-training for data-totext tasks. arXiv preprint arXiv:2005.10433.\\nGregg Kellogg, Ivan Herman, and Jeremy Tandy.\\n2015.\\nGenerating RDF from tabular data on the web.\\nW3C recommendation, W3C.\\nHttps://www.w3.org/TR/2015/REC-csv2rdf20151217/.\\nIoannis Konstas and Mirella Lapata. 2012. Unsupervised concept-to-text generation with hypergraphs.\\nIn NAACL.\\nRémi Lebret, David Grangier, and Michael Auli. 2016.\\nNeural text generation from structured data with application to the biography domain. In EMNLP.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.\\n2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In ACL.\\nPercy Liang, Michael I Jordan, and Dan Klein. 2009.\\nLearning semantic correspondences with less supervision. In ACL.\\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of EMNLP.\\nTianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, and Zhifang Sui. 2018. Table-to-text generation by structure-aware seq2seq learning. In AAAI.\\nAmit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.\\nStep-by-step: Separating planning from realization in neural data-to-text generation. In NAACL.\\nJekaterina Novikova, Ondˇrej Dušek, Amanda Cercas Curry, and Verena Rieser. 2017a. Why we need new evaluation metrics for nlg. In EMNLP.\\nJekaterina Novikova, Ondrej Dusek, and Verena Rieser.\\n2017b. The E2E dataset: New challenges for end-toend generation. In SIGDIAL.\\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A controlled table-totext generation dataset. In EMNLP.\\nPanupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. In ACL.\\nBaolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun Li, Jinchao Li, Michael Zeng, and Jianfeng Gao.\\n2020.\\nFew-shot natural language generation for task-oriented dialog. In arXiv.\\nRatish Puduppully, Li Dong, and Mirella Lapata. 2018.\\nData-to-text generation with content selection and planning. In AAAI.\\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019.\\nData-to-text generation with entity modeling.\\nIn ACL.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.\\nImproving language understanding by generative pre-training.\\nTechnical report, OpenAI.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67.\\nEhud Reiter. 2007.\\nAn architecture for data-to-text systems. In Proceedings of the Eleventh European Workshop on Natural Language Generation (ENLG 07).\\nEhud Reiter. 2020. Openai gpt system: What does it do? Technical report, Arria.\\nEhud Reiter and Robert Dale. 2000. Building natural language generation systems. Cambridge university press.\\nLeonardo F. R. Ribeiro, Claire Gardent, and Iryna Gurevych. 2019.\\nEnhancing AMR-to-text generation with dual graph representations. In EMNLP.\\nLeonardo F. R. Ribeiro, Martin Schmitt, Hinrich Schütze, and Iryna Gurevych. 2020. Investigating pretrained language models for graph-to-text generation. arXiv.\\nThibault Sellam, Dipanjan Das, and Ankur P Parikh.\\n2020.\\nBLEURT: Learning robust metrics for text generation. In ACL.\\nLinfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2017. Amr-to-text generation with synchronous node replacement grammar.\\nIn ACL.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS.\\nPavlos Vougiouklis, Hady ElSahar, Lucie-Aimée Kaffee, Christophe Gravier, Frédérique Laforest, Jonathon S. Hare, and Elena Simperl. 2018. Neural wikipedian: Generating textual summaries from knowledge base triples. Journal of Web Semantics, 52-53:1 – 15.\\nTsung-Hsien Wen, Milica Gaši´c, Nikola Mrkši´c, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. 2016.\\nConditional generation and snapshot learning in neural dialogue systems. In EMNLP.\\nTsung-Hsien Wen, Milica Gaši´c, Nikola Mrkši´c, PeiHao Su, David Vandyke, and Steve Young. 2015.\\nSemantically conditioned LSTM-based natural language generation for spoken dialogue systems. In EMNLP.\\nSam Wiseman, Stuart Shieber, and Alexander Rush.\\n2017.\\nChallenges in data-to-document generation.\\nIn EMNLP.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2020.\\nBERTScore: Evaluating text generation with BERT. In ICLR.\\nChao Zhao, Marilyn Walker, and Snigdha Chaturvedi.\\n2020. Bridging the structural gap between encoding and decoding for data-to-text generation. In ACL.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In EMNLP.\\nVictor Zhong, Caiming Xiong, and Richard Socher.\\n2017.\\nSeq2sql: Generating structured queries from natural language using reinforcement learning.\\nCoRR, abs/1709.00103.\\nAppendix The Appendix contains the following contents: • Results of the ablation study on WebNLG 2017 testset.\\n• Statistics of the table ontology annotations.\\n• Examples of tables that help illustrate DART’s annotation procedure.\\n• Examples of model outputs.\\nModel Experiment BLEU ↑ METEOR ↑ TER ↓ SEEN UNSEEN ALL SEEN UNSEEN ALL SEEN UNSEEN ALL Baseline Transformer [1] webnlg 49.81 5.51 31.81 0.39 0.09 0.24 0.47 0.86 0.64 [2] webnlg+dart_decl_sents 52.31 8.96 39.98 0.40 0.07 0.25 0.45 0.79 0.60 [3] webnlg+dart_human_annotated 53.68 7.02 36.36 0.40 0.09 0.26 0.43 0.79 0.59 [4] webnlg+dart_ontology 53.40 8.54 38.51 0.41 0.08 0.26 0.44 0.80 0.60 [5] webnlg+dart_e2e 51.76 5.92 32.36 0.40 0.09 0.25 0.45 0.86 0.63 [6] webnlg+dart_full 54.99 8.64 39.11 0.40 0.08 0.25 0.42 0.81 0.60 BART-base [1] webnlg 63.02 41.74 53.36 0.45 0.35 0.40 0.33 0.52 0.42 [2] webnlg+dart_decl_sents 62.71 42.51 53.64 0.45 0.36 0.40 0.34 0.51 0.41 [3] webnlg+dart_human_annotated 62.36 46.21 55.14 0.44 0.37 0.41 0.34 0.45 0.39 [4] webnlg+dart_ontology 62.62 46.74 55.54 0.44 0.38 0.41 0.34 0.45 0.39 [5] webnlg+dart_e2e 64.00 35.07 51.17 0.45 0.33 0.40 0.33 0.61 0.46 [6] webnlg+dart_full 63.66 45.48 55.52 0.45 0.37 0.41 0.33 0.47 0.40 BART-large [1] webnlg 63.71 44.17 54.95 0.46 0.39 0.42 0.33 0.51 0.41 [2] webnlg+dart_decl_sents 65.18 46.79 56.79 0.46 0.39 0.42 0.32 0.48 0.40 [3] webnlg+dart_human_annotated 64.51 50.20 58.06 0.46 0.40 0.43 0.32 0.44 0.38 [4] webnlg+dart_ontology 64.19 49.62 57.65 0.46 0.39 0.43 0.33 0.45 0.38 [5] webnlg+dart_e2e 65.06 30.17 48.24 0.46 0.33 0.40 0.32 0.69 0.49 [6] webnlg+dart_full 65.24 47.96 57.44 0.46 0.39 0.43 0.32 0.46 0.39 T5-small [1] webnlg 65.30 45.58 56.57 0.46 0.39 0.43 0.32 0.49 0.40 [2] webnlg+dart_decl_sents 64.18 46.61 56.27 0.46 0.39 0.43 0.33 0.48 0.40 [3] webnlg+dart_human_annotated 65.05 47.81 57.32 0.46 0.40 0.43 0.33 0.46 0.39 [4] webnlg+dart_ontology 65.17 47.49 57.24 0.46 0.39 0.43 0.32 0.47 0.39 [5] webnlg+dart_e2e 65.56 41.28 54.56 0.46 0.38 0.42 0.32 0.54 0.42 [6] webnlg+dart_full 64.70 47.56 57.01 0.46 0.39 0.43 0.33 0.47 0.39 T5-base [1] webnlg 64.89 52.86 59.44 0.46 0.42 0.44 0.33 0.42 0.37 [2] webnlg+dart_decl_sents 65.44 50.80 58.81 0.46 0.41 0.44 0.32 0.43 0.37 [3] webnlg+dart_human_annotated 65.42 50.71 58.80 0.46 0.41 0.44 0.32 0.43 0.37 [4] webnlg+dart_ontology 65.17 51.49 59.04 0.46 0.41 0.44 0.33 0.43 0.37 [5] webnlg+dart_e2e 65.11 49.64 58.19 0.46 0.41 0.44 0.33 0.46 0.39 [6] webnlg+dart_full 65.99 51.68 59.50 0.46 0.42 0.44 0.32 0.43 0.37 T5-large [1] webnlg 64.89 54.01 59.95 0.46 0.43 0.44 0.34 0.41 0.37 [2] webnlg+dart_decl_sents 65.97 53.00 60.12 0.46 0.42 0.44 0.32 0.41 0.36 [3] webnlg+dart_human_annotated 65.82 56.01 61.44 0.46 0.43 0.45 0.32 0.38 0.35 [4] webnlg+dart_ontology 65.53 55.20 60.90 0.46 0.42 0.44 0.32 0.38 0.35 [5] webnlg+dart_e2e 66.27 54.13 60.76 0.46 0.43 0.45 0.32 0.41 0.36 [6] webnlg+dart_full 65.78 54.35 60.64 0.46 0.42 0.44 0.32 0.39 0.35 Table 6: Results of ablation study on WebNLG 2017 testset. dart_decl_sents refers to DART partition that contains auto-generated declarative sentences mentioned in Section 2.2, dart_human_annotated refers to partition that contains human written sentences mentioned in Section 2.1, dart_ontology is the combination of dart_decl_sents and dart_human_annotated, and dart_e2e refers to DART partition containing instances extracted from E2E dataset, the process of which is mentioned in Section 2.3. Note that dart_full is the combination of dart_ontology and dart_e2e.\\nTables Ontology depth (min, med, max) Nodes in ontology (min, med, max) Branching factor (mean) WikiTableQuestions 2060 1, 1, 4 2, 6, 25 4.0 WikiSQL 3563 1, 1, 4 3, 7, 25 5.1 Table 7: Properties of the ontology in the WikiTableQuestions and WikiSQL samples in DART. Branching factor refers to the average number of children across all non-leaf nodes in a table’s ontology.\\nFigure 3: Distribution of column ontology depths in the WikiTableQuestions and WikiSQL samples in DART v1.1.1.\\n<entry category=\"MISC\" eid=\"Id5\" size=\"3\"> <modifiedtripleset> <mtriple>Apertura 2006 | JORNADA_OR_OTHER | Semifinals Ida</mtriple> <mtriple>Semifinals Ida | AWAY_TEAM | América</mtriple> <mtriple>Semifinals Ida | HOME_TEAM | Chivas</mtriple> </modifiedtripleset> <lex comment=\"WikiTableQuestions\" lid=\"Id1\"> Chivas and América will compete in the semifinals of the Apertura 2006 tournament.\\n</lex> </entry> <entry category=\"MISC\" eid=\"Id76\" size=\"6\"> <modifiedtripleset> <mtriple>Terry Jenkins | ROUND | 1st Round</mtriple> <mtriple>Terry Jenkins | YEAR | 2014</mtriple> <mtriple>[TABLECONTEXT] | [TITLE] | PDC World Darts Championship</mtriple> <mtriple>1st Round | OPPONENT | Per Laursen</mtriple> <mtriple>1st Round | RESULT | Lost</mtriple> <mtriple>[TABLECONTEXT] | PLAYER | Terry Jenkins</mtriple> </modifiedtripleset> <lex comment=\"WikiTableQuestions\" lid=\"Id1\"> Terry Jenkins lost the game with Per Laursen in the 1st Round of 2014 PDC World Darts Championship </lex> </entry> Figure 4: Examples of DART instance Figure 5: An example of the data cleaning. The top left table had a missing column name and the table title was not speciﬁc to the data; our internal annotators add the missing column name “Year” and linked the rest of the columns to the “Year” column. The bottom left table had repeat column names in the table; our internal annotators disambiguate the columns by making the column names more speciﬁc.\\nFigure 6: A WikiTableQuestions table that uses [TITLE] in the ontology.\\nFigure 7: A manually annotated table from WikiTableQuestions with a sentence that uses the table title.\\nFigure 8: A manually annotated table from WikiTableQuestions. Annotators created a table ontology, and they wrote sentences encapsulating the information in the orange cells for a given row. Whenever a sentence referenced the table title, that sentence was also highlighted green.\\nFigure 9: An example of collected MTurk-generated sentences for WikiTableQuestions. Internal annotators went through the generated sentences and checked for both sentence coherence and title usage. Below the generated sentences, ‘y’ meant the sentence references the table title, ‘n’ meant the sentence did not use the table title, ‘x’ meant the sentence was nonsensical.\\nFigure 10: Automatically generated declarative sentences from WikiSQL with human validation. Annotators went through the generated sentences and checked for both sentence coherence and title use. Below the generated sentences, ‘y’ meant the sentence references the table title, ‘n’ meant the sentence did not use the table title, ‘x’ meant the sentence was nonsensical.\\n- Sample 1 Input triples: <H> Peru Earthquake <R> scale of disaster <T> 250k homeless <H> Peru Earthquake <R> year <T> 2007 BART-base output: 250k people were killed in the 2007 philippine earthquake .\\n- Sample 2 Input triples: <H> [TABLECONTEXT] <R> game <T> 3 <H> 3 <R> attendance <T> 10 637 <H> [TABLECONTEXT] <R> [title] <T> 2006 Minnesota Swarm season BART-base output: the minnesota swarm played in front of a crowd of 10 , 684 people .\\n- Sample 3 Input triples: <H> Andrew Phelps McCormick <R> state <T> TX <H> Andrew Phelps McCormick <R> active <T> 1892-1916 T5-base output: andrew phelps mccormick was active from 1892 to 1616 in texas .\\nFigure 11: Examples of hallucinated outputs of pretrained models trained on DART - Sample 1 Input triples: <H> Andrew Rayel <R> associated Band/associated Musical Artist <T> Christian Burns <H> Andrew Rayel <R> associated Band/associated Musical Artist <T> Jonathan Mendelsohn reference: andrew rayel , is associated with musical artist jonathan mendelsohn and christian burns .\\ntrain on WebNLG - BART-base output: christian mendelsohn and andrew rayel are both associated with the same band , christian burns .\\ntrain on DART - BART-base output: andrew rayel is associated with christian burns and jonathan mendelsohn .\\n- Sample 2 Input triples: <H> Indie rock <R> stylistic Origin <T> New wave music reference: the stylistic origin of indie rock is new wave music .\\ntrain on WebNLG - BART-base output: the alternative rock genre is new wave .\\ntrain on DART - BART-base output: indie rock is influenced by new wave music .\\n- Sample 3 Input triples: <H> Abradab <R> associated Band/associated Musical Artist <T> Magik rapper <H> Abradab <R> associated Band/associated Musical Artist <T> Kaliber 44 reference: abradab , an artist for the band kaliber 44 , is associated with magik ( rapper ) .\\ntrain on WebNLG - BART-base output: magiber 44 is the creator of abradab , which is also associated with the magik rapper .\\ntrain on DART - BART-base output: magik rapper and kaliber 44 are the associated musicians of abradab .\\n- Sample 4 Input triples: <H> Alfa Romeo 164 <R> assembly <T> Milan <H> Alfa Romeo 164 <R> related Mean Of Transportation <T> Saab 9000 reference: the alfa romeo 164 , which is assembled in milan , is a related means of transportation to saab 9000 , in that they are both cars .\\ntrain on WebNLG - T5-base output: alfa romeo 164 is a transport vehicle for saab 9000 and is found in milan .\\ntrain on DART - T5-base output: alfa romeo 164 ( assembled in milan ) is a related transport vehicle to saab 9000 .\\n- Sample 5 Input triples: <H> Akeem Ayers <R> former Team <T> Tennessee Titans <H> Akeem Ayers <R> draft Pick <T> 39 reference: akeem ayers ’ former team was tennessee titans and he was number 39 in the draft pick .\\ntrain on WebNLG - T5-large output: akeem ayers was drafted with the 39th pick by the tennessee titans .\\ntrain on DART - T5-large output: akeem ayers , a former player of the tennessee titans , was the 39th draft pick .\\nFigure 12: Examples of model outputs - with or without DART data augmentation '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document['text'] = text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_file(filename, content):\n",
    "    pathlib.Path(TXT_BASE+filename).write_bytes(content.encode('utf-8').strip())\n",
    "\n",
    "def write_json_file(filename, content):\n",
    "    pathlib.Path(JSON_BASE+filename).write_bytes(content.encode('utf-8').strip())\n",
    "\n",
    "def save_cleaned_file(document):\n",
    "    filename = document['title']+'.json'\n",
    "    filename_txt = document['title']+'.txt'\n",
    "    json_object = json.dumps(document) \n",
    "    write_json_file(filename,json_object)\n",
    "    write_text_file(filename_txt,document['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_cleaned_file(document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
