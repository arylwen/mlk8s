{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# knowledge graph index pdf pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import openai\n",
    "os.environ['OPENAI_API_KEY'] = \"EMPTY\"\n",
    "os.environ['OPENAI_API_BASE'] = \"http://10.0.0.222:30307/v1\"\n",
    "openai.api_key = \"EMPTY\"\n",
    "openai.api_base = \"http://10.0.0.222:30307/v1\"\n",
    "\n",
    "model = \"Writer/camel-5b-hf\"\n",
    "#model = \"mosaicml/mpt-7b-instruct\"\n",
    "#model = \"mosaicml/mpt-30b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "#kron extensions to llama_index to support openai compatible api\n",
    "sys.path.append('../llama_index')\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import pathlib\n",
    "import fitz\n",
    "\n",
    "PDF_BASE = '/home/arylwen/datasets/documents/pdf/'\n",
    "JSON_BASE = '/home/arylwen/datasets/documents/json_raw/'\n",
    "doc_path = '/home/arylwen/datasets/documents/pdf/2007.02871.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = fitz.open(doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'format': 'PDF 1.5',\n",
       " 'title': '',\n",
       " 'author': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'producer': 'pdfTeX-1.40.21',\n",
       " 'creationDate': 'D:20210413005313Z',\n",
       " 'modDate': 'D:20210413005313Z',\n",
       " 'trapped': '',\n",
       " 'encryption': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if document.metadata['title'].strip() == '':\n",
    "    pdf_name= doc_path.split('/')[-1]\n",
    "    document.metadata['title'] = pdf_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DART: Open-Domain Structured Data Record to Text Generation\n",
      "Linyong Nan1\n",
      "Dragomir Radev1,2\n",
      "Rui Zhang3\n",
      "Amrit Rau1\n",
      "Abhinand Sivaprasad1\n",
      "Chiachun Hsieh4\n",
      "Xiangru Tang1\n",
      "Aadit Vyas1\n",
      "Neha Verma1\n",
      "Pranav Krishna5\n",
      "Yangxiaokang Liu1\n",
      "Nadia Irwanto1\n",
      "Jessica Pan1\n",
      "Faiaz Rahman1\n",
      "Ahmad Zaidi1\n",
      "Murori Mutuma1\n",
      "Yasin Tarabar1\n",
      "Ankit Gupta1\n",
      "Tao Yu1\n",
      "Yi Chern Tan1\n",
      "Xi Victoria Lin2∗\n",
      "Caiming Xiong2\n",
      "Richard Socher2\n",
      "Nazneen Fatema Rajani2\n",
      "1 Yale University\n",
      "2 Salesforce Research\n",
      "3 Penn State University\n",
      "4 The University of Hong Kong\n",
      "5 MIT\n",
      "{linyong.nan, dragomir.radev}@yale.edu, rmz5227@psu.edu, nazneen.rajani@salesforce.com\n",
      "Abstract\n",
      "We present DART, an open domain structured\n",
      "DAta Record to Text generation dataset with\n",
      "over 82k instances (DARTs). Data-to-Text an-\n",
      "notations can be a costly process, especially\n",
      "when dealing with tables which are the ma-\n",
      "jor source of structured data and contain non-\n",
      "trivial structures. To this end, we propose a\n",
      "procedure of extracting semantic triples from\n",
      "tables that encodes their structures by exploit-\n",
      "ing the semantic dependencies among table\n",
      "headers and the table title. Our dataset con-\n",
      "struction framework effectively merged hetero-\n",
      "geneous sources from open domain semantic\n",
      "parsing and dialogue-act-based meaning rep-\n",
      "resentation tasks by utilizing techniques such\n",
      "as: tree ontology annotation, question-answer\n",
      "pair to declarative sentence conversion, and\n",
      "predicate uniﬁcation, all with minimum post-\n",
      "editing. We present systematic evaluation on\n",
      "DART as well as new state-of-the-art results\n",
      "on WebNLG 2017 to show that DART (1)\n",
      "poses new challenges to existing data-to-text\n",
      "datasets and (2) facilitates out-of-domain gen-\n",
      "eralization. Our data and code can be found\n",
      "at https://github.com/Yale-LILY/\n",
      "dart.\n",
      "1\n",
      "Introduction\n",
      "Automatically generating textual descriptions from\n",
      "structured data improves the accessibility of knowl-\n",
      "edge bases to lay users. Such applications include\n",
      "explaining data records to non-experts (Cawsey\n",
      "et al., 1997), writing sports news (Chen and\n",
      "Mooney, 2008), summarizing information in mul-\n",
      "tiple documents (Fan et al., 2019), and generating\n",
      "dialogue responses (Wen et al., 2015).\n",
      "While signiﬁcant progress has been made in this\n",
      "ﬁeld, there are still several issues with existing\n",
      "Data-to-Text datasets. First, they adopt a ﬂat ontol-\n",
      "ogy structure of the data, such as slot-value pairs\n",
      "for data records (Lebret et al., 2016; Novikova et al.,\n",
      "∗Now at Facebook AI.\n",
      "2017b) or ﬂat schema for tables (Wiseman et al.,\n",
      "2017; Chen et al., 2020a; Parikh et al., 2020). This\n",
      "ﬂat structure is not powerful enough to encode rich\n",
      "semantic relationships in the ontology of the struc-\n",
      "tured data, especially tables, whose representation\n",
      "can be further improved with these semantic knowl-\n",
      "edge. Second, some of the datasets only focus on\n",
      "a small number of domains or knowledge graphs,\n",
      "therefore providing limited number of predicates\n",
      "and data ontologies. For example, E2E (Novikova\n",
      "et al., 2017b) on restaurants and WebNLG (Gar-\n",
      "dent et al., 2017) on 15 categories from DBPedia.\n",
      "Furthermore, some of them only have loose align-\n",
      "ments between data input and sentence due to the\n",
      "nature of the task (Wiseman et al., 2017) and the\n",
      "automatic generation procedure (Vougiouklis et al.,\n",
      "2018; Elsahar et al., 2018).\n",
      "To address some of these issues and to encour-\n",
      "age further research in natural language generation\n",
      "from structured data, we introduce DART, a large\n",
      "and open-domain structured DAta-Record-to-Text\n",
      "generation corpus. The goal of DART is to har-\n",
      "vest the diverse predicates occurred in Wikipedia\n",
      "tables, which is signiﬁcantly richer than those de-\n",
      "ﬁned in the domain speciﬁc ontologies E2E and\n",
      "WebNLG were built on (Table 2). We also in-\n",
      "troduce novel tree ontology annotation on tables,\n",
      "which converts a ﬂat table schema into a tree struc-\n",
      "tured semantic frame. The tree ontology reﬂects\n",
      "the core and auxiliary relations in the table schema,\n",
      "and naturally occurs across many domains. As a\n",
      "result, DART provides high-quality sentence an-\n",
      "notations to tree structured semantic frames ex-\n",
      "tracted from various data sources, including Wik-\n",
      "iSQL (Zhong et al., 2017) and WikiTableQuestions\n",
      "(Pasupat and Liang, 2015), two open-domain ques-\n",
      "tion answering datasets, as well as E2E (Novikova\n",
      "et al., 2017b) and WebNLG (Gardent et al., 2017)\n",
      "(Figure 1). We evaluated several state-of-the-art\n",
      "data-to-text models on DART, and found that while\n",
      "these models achieve impressive performance on\n",
      "arXiv:2007.02871v2  [cs.CL]  12 Apr 2021\n",
      "domain-speciﬁc datasets, their performance suffers\n",
      "on DART due to its open-domain nature and richer\n",
      "semantic structures.\n",
      "Our contributions are as follows. (1) We present\n",
      "a large and open-domain corpus for structured data\n",
      "record to text generation, annotated with tree on-\n",
      "tologies converted from the table. This hierarchical\n",
      "input differentiates our corpus from existing data-\n",
      "to-text corpora. (2) We benchmark several state-\n",
      "of-the-art data-to-text models to show that DART\n",
      "introduces new generalization challenges. (3) We\n",
      "demonstrate that using DART for data augmenta-\n",
      "tion improves the performance of existing models\n",
      "on the WebNLG 2017 dataset. We expect the re-\n",
      "sults to generalize to other data-to-text datasets\n",
      "given the open-domain nature of DART.\n",
      "2\n",
      "DART Data Collection\n",
      "As shown in Figure 1, DART is constructed from\n",
      "three different sources: (1) human annotation on\n",
      "Wikipedia tables from two table semantic parsing\n",
      "and question answering datasets WikiSQL and Wik-\n",
      "iTableQuestions (§ 2.1), (2) automatic conversion\n",
      "of questions in WikiSQL to declarative sentences\n",
      "(§ 2.2), and (3) incorporation of existing datasets\n",
      "including WebNLG 2017 and Cleaned E2E (§ 2.3).\n",
      "After collecting the ⟨triple-set, sentence⟩ pairs from\n",
      "various data sources, we manually canonicalized\n",
      "the predicates and show that DART covers a broad\n",
      "range of topics (§ 2.4). Finally, we discuss the data\n",
      "split in § 2.5.\n",
      "2.1\n",
      "Tree Ontology and Sentence Annotation\n",
      "on Tables\n",
      "Tables are a major source of structured data that\n",
      "contain a wealth of information complementary\n",
      "to text and knowledge graphs.\n",
      "We aim to col-\n",
      "lect ⟨triple-set, sentence⟩ pairs from open-domain\n",
      "Wikipedia tables.\n",
      "However, table schema are\n",
      "ﬂat, making them not directly usable for building\n",
      "subject-predicate-object triples to capture rich rela-\n",
      "tionships in the data.\n",
      "As shown in Figure 2, we propose a two-stage an-\n",
      "notation process that involves two groups of anno-\n",
      "tators: internal annotators and Amazon Mechanical\n",
      "Turk1 workers. In the ﬁrst stage, skilled internal an-\n",
      "notators specify the parent of every column header\n",
      "to construct a tree-structured ontology for each ta-\n",
      "ble. In the second stage, both internal and external\n",
      "annotators provide a sentential description of the\n",
      "1https://www.mturk.com/\n",
      "highlighted cells in a row that are automatically-\n",
      "chosen based on the ontology.\n",
      "Tree Ontology Annotation\n",
      "For each column in\n",
      "a given table, our internal annotators labeled its\n",
      "ontological parent. In Figure 2, for example, the an-\n",
      "notator would provide the sequence {NULL, TEAM,\n",
      "STADIUM, STADIUM, TEAM} as the parent of each\n",
      "column — column TEAM has no parent, STADIUM\n",
      "has parent TEAM, and so on. In many cases, the\n",
      "relationship between a parent column and its child\n",
      "column can be conceptualized as a \"has-a\" relation-\n",
      "ship. For tables that are malformed or have dupli-\n",
      "cate or missing column names (as shown in Figure\n",
      "5 of the Appendix), annotators either changed or\n",
      "added appropriate column names in order to ﬁt\n",
      "these patterns. For each table we generate an ontol-\n",
      "ogy tree whose root is always [TABLECONTEXT].\n",
      "This root node either has (1) one child node [TI-\n",
      "TLE] in the cases where the table title is the subject\n",
      "of entire table, or (2) column header node(s) and\n",
      "a [TITLE] node as children, as shown in Figure 2.\n",
      "This is because in some tables, the table title itself\n",
      "is more appropriate to be the root of the ontology\n",
      "tree (example shown in Figure 6 of the Appendix).\n",
      "In these cases, annotators assigned the special to-\n",
      "ken [TITLE] as the parent of the relevant column\n",
      "nodes. For other tables, title usually provides im-\n",
      "portant context for understanding the table’s rows\n",
      "(example shown in Figure 7 of the Appendix). In\n",
      "such cases, [TITLE] is made a child of [TABLE-\n",
      "CONTEXT] together with the column headers that\n",
      "are appropriate.\n",
      "We evaluate the quality of the initial tree on-\n",
      "tology annotation and made corrections with the\n",
      "following procedure: (1) reject and request correc-\n",
      "tions from the original annotators if the provided\n",
      "ontology is disconnected or contains a cycle, (2)\n",
      "verify that all column headers appear as a node in\n",
      "the tree. For many tables, the determination of an\n",
      "ontology is a subjective process with many \"cor-\n",
      "rect\" answers - for example, swapping the positions\n",
      "of TEAM and CITY in the tree in Figure 2 produces\n",
      "an equally valid ontology for the referenced table.\n",
      "If there are multiple ways to construct an ontology\n",
      "based on annotators’ decisions of attribute relation-\n",
      "ships among column headers, we manually unify\n",
      "the annotations for similar tables (for examples,\n",
      "tables about athletes in different sports). The on-\n",
      "tologies exhibit a great deal of structural variety.\n",
      "Relevant statistics are summarized in Table 7 and\n",
      "Figure 3 of the Appendix.\n",
      "Figure 1: DART data collection pipeline. MR: Meaning Representation.\n",
      "Input Unit\n",
      "Examples\n",
      "Vocab Size\n",
      "Words per SR\n",
      "Sents per SR\n",
      "Tables\n",
      "WikiTableText\n",
      "Row\n",
      "13,318\n",
      "—\n",
      "13.9\n",
      "1.0\n",
      "4,962\n",
      "LogicNLG\n",
      "Table\n",
      "37,015\n",
      "122K\n",
      "13.8\n",
      "1.0\n",
      "7,392\n",
      "ToTTo\n",
      "Highlighted Cells\n",
      "136,161\n",
      "136K\n",
      "17.4\n",
      "1.0\n",
      "83,141\n",
      "DART\n",
      "Triple Set\n",
      "82,191\n",
      "33.2K\n",
      "21.6\n",
      "1.5\n",
      "5,623\n",
      "Table 1: DART compared with other open-domain table-to-text datasets. DART takes triple sets as input by\n",
      "incorporating the ontology of table headers and title, and its surface realizations tend to be longer with more than\n",
      "single sentence verbalization. SR: Surface Realization.\n",
      "DART: 62,659 train / 6,980 dev / 12,552 test\n",
      "WikiTableQuestions\n",
      "WikiSQL\n",
      "WebNLG\n",
      "Cleaned E2E\n",
      "Internal\n",
      "MTurk\n",
      "Internal\n",
      "Declarative\n",
      "Domains\n",
      "Wikipedia (open-domain)\n",
      "15 DBPedia Categories\n",
      "Restaurants\n",
      "Unique Predicates\n",
      "1,950\n",
      "1,403\n",
      "493\n",
      "2,008\n",
      "347\n",
      "7\n",
      "Unique Triples\n",
      "13,505\n",
      "5,541\n",
      "1,648\n",
      "7,787\n",
      "3,220\n",
      "946\n",
      "Tripleset-Sentence Pairs\n",
      "4,902\n",
      "2,120\n",
      "772\n",
      "4,204\n",
      "27,731\n",
      "42,462\n",
      "Triples per Tripleset (min, med, max)\n",
      "1, 3, 10\n",
      "1, 3, 7\n",
      "1, 2, 7\n",
      "1, 2, 10\n",
      "1, 3, 7\n",
      "1, 4, 7\n",
      "Vocab Size\n",
      "13.4K\n",
      "8.9K\n",
      "3.0K\n",
      "10.7K\n",
      "8.0K\n",
      "3.0K\n",
      "Words per SR\n",
      "15.2\n",
      "16.5\n",
      "14.0\n",
      "12.6\n",
      "22.5\n",
      "22.9\n",
      "Sentences per SR\n",
      "1.0\n",
      "1.1\n",
      "1.0\n",
      "1.0\n",
      "1.4\n",
      "1.6\n",
      "Table 2: Statistics of DART decomposed by different collection methods. DART exhibits a great deal of topical\n",
      "variety in terms of the number of unique predicates, the number of unique triples, and the vocabulary size.\n",
      "Connected Component Extraction\n",
      "After we\n",
      "annotated the ontology, we automatically choose\n",
      "a subset of cells for a selected table row to form\n",
      "the triple set. Randomly selecting cells leads to\n",
      "poor quality annotation as the selected data could\n",
      "lack a subject, lack cohesion, or would require in-\n",
      "formation not encoded in the ontology to form a\n",
      "coherent sentence. For example, in Figure 2, if only\n",
      "two nodes CITY and CAPACITY were highlighted\n",
      "then a coherent sentence cannot be produced as\n",
      "there is no direct logical relationship (functional\n",
      "dependency) between them. To solve these issues,\n",
      "instead of randomly selecting cells in a row, we\n",
      "extract connected components from the ontology.\n",
      "The extracted components have two controllable\n",
      "properties: size and shape. To create variation in\n",
      "size, we randomly sampled between [2, 5]. The\n",
      "shape is determined by two numbers: the number\n",
      "of sibling node pairs and parent-child node pairs.\n",
      "Increasing the number of sibling node pairs creates\n",
      "a wider tree, while increasing the latter creates a\n",
      "deeper tree. We created a sliding scale between\n",
      "width and depth using an expansion parameter, p.\n",
      "We recursively visit a node if it has children with\n",
      "probability p and otherwise move to a sibling if it\n",
      "exists. If p = 1, the search becomes a DFS and if\n",
      "p = 0, it becomes BFS. We found that randomly\n",
      "selecting p from 0.5 to 0.7 created a reasonable\n",
      "variation in extracted component shapes. This en-\n",
      "sures the balance between breadth and depth of\n",
      "ontology coverage of the selected cells, therefore\n",
      "ensuring the quality of the sentence annotation.\n",
      "Sentence Annotation\n",
      "Given the table, title, and\n",
      "connected highlighted cells of a row, annotators\n",
      "Figure 2: Overview of our human annotation procedure. Top panel: We collect the parent-child relations between\n",
      "columns from internal annotators (yellow is parent, green is child). Then, we collect a surface realization of the\n",
      "cells highlighted in orange. Middle panel: We use the provided parent-child relations to construct an ontology tree\n",
      "on the columns, then select the nodes corresponding to the highlighted cells. We gather a connected subtree by\n",
      "collecting all nodes leading up to the highlighted cells’ lowest common ancestor. Bottom panel: We extract a set of\n",
      "triples from the subtree as shown. This triple-set is paired with the provided realization to form a DART instance.\n",
      "were asked to write a description of the highlighted\n",
      "cells. We encouraged the annotators to use di-\n",
      "verse vocabulary and syntactic structures. To en-\n",
      "sure quality, internal annotators reviewed every\n",
      "crowd sourced sentence for correctness. They ei-\n",
      "ther rewrote or discarded the sentences that were\n",
      "nonsensical or incorrect. In some cases, they also\n",
      "changed cell highlighting patterns to match the sen-\n",
      "tence provided.\n",
      "Build Tripleset-Sentence Pairs\n",
      "Finally, we con-\n",
      "vert the highlighted cells to triplesets. For a row R,\n",
      "we start with the table’s column ontology T. We\n",
      "ﬁrst place the cell values in R in their correspond-\n",
      "ing slots in T, e.g. in Figure 2 we ﬁll TEAM with\n",
      "\"Amsterdam Admirals\". We then check that the\n",
      "nodes of T corresponding to the highlighted cells\n",
      "in R form a connected subtree. If not, we walk up\n",
      "the tree and highlight each traversed node up un-\n",
      "til the lowest common ancestor of the highlighted\n",
      "nodes (inclusive) to form a connected subtree. For\n",
      "each node N in the tree except the root node, we\n",
      "can extract the triple (parent (N), title (N), N).\n",
      "For example, since STADIUM is highlighted in Fig-\n",
      "ure 2, we extract the triple (Amsterdam Admirals,\n",
      "STADIUM, Olympisch Stadion). A small number\n",
      "of triple-sets contained more than 10 triples. We\n",
      "discarded these because their associated surface\n",
      "realizations were of poor quality. The numbers\n",
      "of tripleset-sentence pairs annotated by different\n",
      "annotators are shown in Table 2.\n",
      "2.2\n",
      "Automatically Converting Questions to\n",
      "Declarative Sentences\n",
      "High quality natural language questions in open\n",
      "domain semantic parsing datasets such as Wik-\n",
      "iSQL and QA2D techniques found in automati-\n",
      "cally constructing NLI datasets (Demszky et al.,\n",
      "2018) present themselves as an attractive opportu-\n",
      "nity to semi-automatically construct an abundance\n",
      "of declarative sentences and align to table cells. We\n",
      "leveraged rule-based QA2D technique2 together\n",
      "with manual screening to combine WikiSQL ques-\n",
      "tions and SQL-retrieved-answers into declarative\n",
      "sentences and manually ﬁltered out bad sentences.\n",
      "We only execute SQL queries without aggregate\n",
      "commands3 to retrieve answers corresponding to\n",
      "questions answerable by single rows. An example\n",
      "of such conversion is as follows:\n",
      "2We use the rule-based model from https://github.\n",
      "com/kelvinguu/qanli (Demszky et al., 2018). The neu-\n",
      "ral model code is not released.\n",
      "3MAX, MIN, COUNT, SUM, AVG, JOIN, INTER-\n",
      "SECT, UNION, GROUP BY, ORDER BY.\n",
      "Question:\n",
      "In which year did Greece hold its\n",
      "last Summer Olympics?\n",
      "Answer: 2004\n",
      "Declarative Sentence: Greece held its last Summer\n",
      "Olympics in 2004.\n",
      "Alignment with table cells is done at two\n",
      "stages. We ﬁrst align sentences with corresponding\n",
      "rows by changing SQL commands to SELECT\n",
      "* and use string matching to obtain columns\n",
      "and column headers relevant to the answer and\n",
      "WHERE condition. After manually ﬁltering out\n",
      "bad sentences, bad alignments, or tables without\n",
      "ontology annotations, we were able to get 4,204\n",
      "sentences. Finally, the corresponding table cells\n",
      "are then converted into triples in the same way as\n",
      "we described in Section 2.1.\n",
      "Examples of produced declarative sentences can\n",
      "be found in Figure 10 of the Appendix.\n",
      "2.3\n",
      "Incorporating Existing Datasets\n",
      "Since they provide a large amount of strictly\n",
      "aligned data-text pairs with high quality sentences,\n",
      "we incorporate the following existing datasets in\n",
      "the same ⟨triple-set, sentence⟩ pair format with\n",
      "some modiﬁcations.\n",
      "WebNLG 2017\n",
      "An instance of the WebNLG\n",
      "dataset contains a set of triples extracted from DB-\n",
      "pedia and the target text written by human. We\n",
      "include the WebNLG 2017 dataset4 consisting of\n",
      "27731 triple-set sentence pairs with up to 7 RDF\n",
      "triples in a triple set covering 15 domains.\n",
      "Cleaned E2E\n",
      "The original E2E dataset includes\n",
      "dialogue act meaning representations (MR) and\n",
      "natural language references in the restaurant do-\n",
      "main. Later, Dušek et al. (2019) provide Cleaned\n",
      "E2E5 by automatically ﬁxing the dialogue acts to\n",
      "account for omissions and hallucinations in the\n",
      "text.\n",
      "We incorporate Cleaned E2E because of\n",
      "its strict alignment between the meaning repre-\n",
      "sentation and the text. To convert the MR to a\n",
      "triple-set, we take the NAME slot (present in al-\n",
      "most all the MRs) as the subject. For example,\n",
      "the MR (NAME[ALIMENTUM], AREA[CITY CEN-\n",
      "TRE], FAMILYFRIENDLY[NO]) is converted to the\n",
      "4https://gitlab.com/shimorina/\n",
      "webnlg-dataset/-/tree/master/webnlg_\n",
      "challenge_2017\n",
      "5https://github.com/tuetschek/\n",
      "e2e-cleaning\n",
      "triple-set {(ALIMENTUM, AREA, CITY CENTRE),\n",
      "(ALIMENTUM, FAMILYFRIENDLY, NO)}. We drop\n",
      "MRs which do not contain the NAME slot.\n",
      "2.4\n",
      "Predicate Uniﬁcation\n",
      "We canonicalized the predicates in our triple sets\n",
      "such that those of the same meaning are also repre-\n",
      "sented the same. We manually constructed a predi-\n",
      "cate mapping table to achieve this. As an example,\n",
      "our predicate mapping maps \"Hometown,\" \"Home\n",
      "Town,\" and \"Home Town/City\" to the uniﬁed pred-\n",
      "icate \"HOMETOWN.\"\n",
      "After unifying predicates, we evaluated the di-\n",
      "versity of DART by counting the number of unique\n",
      "predicates in its partitions. As shown in Table 2, we\n",
      "see that the Wikipedia partition of DART contains\n",
      "much more unique predicates than the WebNLG\n",
      "and Cleaned E2E partitions combined, despite hav-\n",
      "ing smaller number of ⟨triple-set, sentence⟩ pairs.\n",
      "This contributes signiﬁcantly to the domain di-\n",
      "versity of DART. In addition, we can see that\n",
      "DART exhibits a great deal of topical variety in\n",
      "terms of number of unique triples and vocabulary\n",
      "size.\n",
      "2.5\n",
      "Dataset Split\n",
      "For WebNLG 2017 and Cleaned E2E, we use their\n",
      "original data splits. For our annotation on Wik-\n",
      "iTableQuestions and WikiSQL, random splitting\n",
      "will make train, dev, and test splits contain similar\n",
      "tables and similar ⟨triple-set, sentence⟩ examples.\n",
      "Therefore, to increase the generalization challenge,\n",
      "we compare the table title and the table header to\n",
      "ﬁnd similar tables, and make sure the model is eval-\n",
      "uated on test split tables that are least similar to\n",
      "those used for training. We ﬁrst sample some ta-\n",
      "bles as a seed test set, and then compute Jaccard\n",
      "similarity6 with remaining tables based on the titles\n",
      "and the headers. If a table has a Jaccard similarity\n",
      "greater than 0.5 with any of the tables in the test\n",
      "set, we add it into the test set. A similar process\n",
      "is repeated to create the dev set, and the remain-\n",
      "ing tables form the training set. This results in\n",
      "62,659/6,980/12,552 sentences in the train/dev/test\n",
      "sets, respectively.\n",
      "3\n",
      "Experimental Results\n",
      "We conduct experiments on DART and the\n",
      "WebNLG 2017 dataset, with an ablation study on\n",
      "6https://en.wikipedia.org/wiki/\n",
      "Jaccard_index\n",
      "WebNLG to show the beneﬁts of using DART for\n",
      "data augmentation.\n",
      "3.1\n",
      "Models\n",
      "We investigate several state-of-the-art Data-to-Text\n",
      "generation models. We report results of the fol-\n",
      "lowing models on DART-testset: (1) Bidirectional-\n",
      "LSTM with attention, for which we use 2-layer\n",
      "bi-LSTM for encoder, with 300 dimensional word\n",
      "embeddings (without using pretrained word vec-\n",
      "tors), 512 hidden units and 0.3 dropout rate for the\n",
      "decoder. (2) Transformer (Vaswani et al., 2017),\n",
      "previously used by Castro Ferreira et al. (2019) on\n",
      "the WebNLG dataset. The input is formed by lin-\n",
      "earizing the unordered triple set. (3) BART (Lewis\n",
      "et al., 2020), for which we report results of both\n",
      "BART-base and BART-large. (4) T5 (Raffel et al.,\n",
      "2020): we add the same preﬁx \"translate Graph to\n",
      "English:\" to the input, as it is used in Ribeiro et al.\n",
      "(2020). We report results of T5-small, T5-base and\n",
      "T5-large models. For both BART and T5 models,\n",
      "we use implementations of Ribeiro et al. (2020),\n",
      "with same hyperparameter setting.\n",
      "3.2\n",
      "Evaluation Metrics\n",
      "We use a variety of automatic metrics and human\n",
      "evaluation (Section 4) to evaluate the quality of the\n",
      "generated text. We report BLEU, METEOR, and\n",
      "TER which are used in the ofﬁcial WebNLG chal-\n",
      "lenge. However, these measures have limitations\n",
      "in considering the semantic meanings of words or\n",
      "phrases (Novikova et al., 2017a), therefore we also\n",
      "report MoverScore (Zhao et al., 2019), BERTScore\n",
      "(Zhang et al., 2020), and BLEURT (Sellam et al.,\n",
      "2020) that incorporate semantics rather than sur-\n",
      "face forms using contextual embeddings. Further-\n",
      "more, we include PARENT (Dhingra et al., 2019)\n",
      "which explicitly aligns n-grams from the reference\n",
      "and generated text to the data contents.\n",
      "3.3\n",
      "Results\n",
      "DART\n",
      "Our experimental results on DART are\n",
      "summarized in Table 3. The T5-large model has\n",
      "the highest performance among all models with a\n",
      "BLEU score of 50.66. We attribute this to T5’s gen-\n",
      "eralization and transfer learning ability due to pre-\n",
      "training on multi-tasks. We can see that in general,\n",
      "pretrained models outperform others by a large\n",
      "margin, and increasing the model size seems to\n",
      "further boost the performance on DART. However,\n",
      "language models such as BART and T5 are pre-\n",
      "trained by reconstructing text and, as a result, we\n",
      "found that their output on DART often contains\n",
      "hallucinated words (Parikh et al., 2020; Harkous\n",
      "et al., 2020; Reiter, 2020), as shown in Figure 11.\n",
      "In addition, while the pretrained model shows bet-\n",
      "ter text generation quality due to its generalization\n",
      "ability from pretraining, it does not fully capture\n",
      "the hierarchical ontology nature of the triple sets\n",
      "in their linearized input, therefore making DART\n",
      "more challenging. We suspect that models that\n",
      "are better at exploiting the ontology structure pre-\n",
      "served in the input tripleset will achieve better per-\n",
      "formance on DART.\n",
      "WebNLG\n",
      "Furthermore,\n",
      "we\n",
      "investigate\n",
      "if\n",
      "DART can improve pretrained models’ perfor-\n",
      "mance on other Data-to-Text generation tasks.\n",
      "To this end, we ﬁnetune the baseline transformer\n",
      "model, BART-[base, large] and T5-[small, base,\n",
      "large] on the WebNLG 2017 dataset, and augment\n",
      "the training by adding instances in the DART train-\n",
      "ing set. The experimental results can be found in\n",
      "Table 4. We report performances of some competi-\n",
      "tive models that are not pretrained, as well as the\n",
      "state-of-the-art performances of pretrained models\n",
      "on the WebNLG 2017 dataset by Ribeiro et al.\n",
      "(2020). On the bottom panel, we include results\n",
      "of experiments augmented with DART instances\n",
      "whose triplesets are generated with table ontology\n",
      "annotation, paired with human written sentences.\n",
      "We are able to achieve new state-of-the-art results\n",
      "on all WebNLG 2017 test set splits (seen, unseen\n",
      "and all) by ﬁnetuning T5-large on DART. We\n",
      "observe that using DART for data augmentation\n",
      "consistently improves the performance across all\n",
      "models, including the baseline transformer model\n",
      "that is not pretrained. Furthermore, we observe\n",
      "that more improvement is shown on unseen split of\n",
      "the test set, due to DART’s open-domain nature.\n",
      "See Figure 12 of the Appendix for example model\n",
      "outputs aligned with their human references.\n",
      "3.4\n",
      "Ablation Study\n",
      "We also conduct an ablation study on the WebNLG\n",
      "dataset to investigate what part of DART con-\n",
      "tributes most to improving the Data-to-Text tasks\n",
      "in general. We report results of the study in Table 6\n",
      "of the Appendix. We divide DART into 4 partitions,\n",
      "where declarative sentence (auto-generated) parti-\n",
      "tion and human annotated sentence partition con-\n",
      "tain instances whose triplesets are extracted from\n",
      "Wikipedia tables based on ontology. E2E parti-\n",
      "tion contains instances converted from the E2E\n",
      "BLEU ↑\n",
      "METEOR ↑\n",
      "TER ↓\n",
      "MoverScore ↑\n",
      "BERTScore(F1) ↑\n",
      "BLEURT ↑\n",
      "PARENT ↑\n",
      "LSTM with Attention\n",
      "29.66\n",
      "0.27\n",
      "0.63\n",
      "0.31\n",
      "0.90\n",
      "-0.13\n",
      "0.35\n",
      "End-to-End Transformer\n",
      "27.24\n",
      "0.25\n",
      "0.65\n",
      "0.25\n",
      "0.89\n",
      "-0.29\n",
      "0.28\n",
      "BART-base\n",
      "47.11\n",
      "0.38\n",
      "0.46\n",
      "0.51\n",
      "0.95\n",
      "0.37\n",
      "0.55\n",
      "BART-large\n",
      "48.56\n",
      "0.39\n",
      "0.45\n",
      "0.52\n",
      "0.95\n",
      "0.41\n",
      "0.57\n",
      "T5-small\n",
      "47.69\n",
      "0.39\n",
      "0.46\n",
      "0.52\n",
      "0.95\n",
      "0.40\n",
      "0.56\n",
      "T5-base\n",
      "49.21\n",
      "0.40\n",
      "0.44\n",
      "0.53\n",
      "0.95\n",
      "0.43\n",
      "0.57\n",
      "T5-large\n",
      "50.66\n",
      "0.40\n",
      "0.43\n",
      "0.54\n",
      "0.95\n",
      "0.44\n",
      "0.58\n",
      "Table 3: Model results on the test set of DART ↑: Higher is better. ↓: Lower is better.\n",
      "BLEU ↑\n",
      "METEOR ↑\n",
      "TER ↓\n",
      "SEEN\n",
      "UNSEEN\n",
      "ALL\n",
      "SEEN\n",
      "UNSEEN\n",
      "ALL\n",
      "SEEN\n",
      "UNSEEN\n",
      "ALL\n",
      "Pipeline Transformer† (Castro Ferreira et al., 2019)\n",
      "56.28\n",
      "23.04\n",
      "42.41\n",
      "0.42\n",
      "0.21\n",
      "0.32\n",
      "0.39\n",
      "0.63\n",
      "0.50\n",
      "Pipeline GRU† (Castro Ferreira et al., 2019)\n",
      "56.09\n",
      "25.12\n",
      "42.73\n",
      "0.42\n",
      "0.22\n",
      "0.33\n",
      "0.39\n",
      "0.64\n",
      "0.51\n",
      "MELBOURNE (Gardent et al., 2017)\n",
      "54.52\n",
      "33.27\n",
      "45.13\n",
      "0.41\n",
      "0.33\n",
      "0.37\n",
      "0.40\n",
      "0.55\n",
      "0.47\n",
      "BestPlan † (Moryossef et al., 2019)\n",
      "53.30\n",
      "34.41\n",
      "47.24\n",
      "0.44\n",
      "0.34\n",
      "0.39\n",
      "0.47\n",
      "0.56\n",
      "0.51\n",
      "DualEnc (Zhao et al., 2020)\n",
      "63.45\n",
      "36.73\n",
      "51.42\n",
      "0.46\n",
      "0.37\n",
      "0.41\n",
      "0.34\n",
      "0.55\n",
      "0.44\n",
      "PlanEnc (Zhao et al., 2020)\n",
      "64.42\n",
      "38.23\n",
      "52.78\n",
      "0.45\n",
      "0.37\n",
      "0.41\n",
      "0.33\n",
      "0.53\n",
      "0.42\n",
      "Ribeiro et al. (2020)\n",
      "BART-base ‡\n",
      "63.02\n",
      "41.74\n",
      "53.36\n",
      "0.45\n",
      "0.35\n",
      "0.40\n",
      "0.33\n",
      "0.52\n",
      "0.42\n",
      "BART-large ‡\n",
      "63.71\n",
      "44.17\n",
      "54.95\n",
      "0.46\n",
      "0.39\n",
      "0.42\n",
      "0.33\n",
      "0.51\n",
      "0.41\n",
      "T5-small ‡\n",
      "65.30\n",
      "45.58\n",
      "56.57\n",
      "0.46\n",
      "0.39\n",
      "0.43\n",
      "0.32\n",
      "0.49\n",
      "0.40\n",
      "T5-base ‡\n",
      "64.89\n",
      "52.86\n",
      "59.44\n",
      "0.46\n",
      "0.42\n",
      "0.44\n",
      "0.33\n",
      "0.42\n",
      "0.37\n",
      "T5-large ‡\n",
      "64.89\n",
      "54.01\n",
      "59.95\n",
      "0.46\n",
      "0.43\n",
      "0.44\n",
      "0.34\n",
      "0.41\n",
      "0.37\n",
      "+ DART\n",
      "BART-base\n",
      "62.36\n",
      "46.21\n",
      "55.14\n",
      "0.44\n",
      "0.37\n",
      "0.41\n",
      "0.34\n",
      "0.45\n",
      "0.39\n",
      "BART-large\n",
      "64.51\n",
      "50.20\n",
      "58.06\n",
      "0.46\n",
      "0.40\n",
      "0.43\n",
      "0.32\n",
      "0.44\n",
      "0.38\n",
      "T5-small\n",
      "65.05\n",
      "47.81\n",
      "57.32\n",
      "0.46\n",
      "0.40\n",
      "0.43\n",
      "0.33\n",
      "0.46\n",
      "0.39\n",
      "T5-base\n",
      "65.42\n",
      "50.71\n",
      "58.80\n",
      "0.46\n",
      "0.41\n",
      "0.44\n",
      "0.32\n",
      "0.43\n",
      "0.37\n",
      "T5-large\n",
      "65.82\n",
      "56.01\n",
      "61.44\n",
      "0.46\n",
      "0.43\n",
      "0.45\n",
      "0.32\n",
      "0.38\n",
      "0.35\n",
      "Table 4: The WebNLG 2017 results on the test set.\n",
      "†: We report results from Zhao et al. (2020) who use the\n",
      "evaluation scripts that are strictly the same as the ofﬁcial challenge. ‡: We report results calculated with the model\n",
      "outputs on the WebNLG 2017 testset released by Ribeiro et al. (2020).\n",
      "Tripleset source\n",
      "Sentence source\n",
      "% ﬂuent\n",
      "% faithful\n",
      "% (ﬂuent+\n",
      "mostly ﬂuent)\n",
      "% (faithful+\n",
      "mostly faithful)\n",
      "WikiTableQuestions (§ 2.1)\n",
      "human-written reference\n",
      "75%\n",
      "81%\n",
      "96%\n",
      "99%\n",
      "BART-base\n",
      "74%\n",
      "57%\n",
      "93%\n",
      "84%\n",
      "T5-base\n",
      "72%\n",
      "54%\n",
      "94%\n",
      "76%\n",
      "WikiSQL (§ 2.2)\n",
      "auto-generated reference\n",
      "59%\n",
      "56%\n",
      "87%\n",
      "88%\n",
      "BART-base\n",
      "66%\n",
      "51%\n",
      "92%\n",
      "83%\n",
      "T5-base\n",
      "75%\n",
      "65%\n",
      "97%\n",
      "90%\n",
      "Table 5: Human evaluation over references and model outputs.\n",
      "dataset, and WebNLG partition keeps the original\n",
      "data format. In general, we observe that adding\n",
      "DART instances that contain human written sen-\n",
      "tences brings most improvement, especially on un-\n",
      "seen split. While adding E2E partition boosts the\n",
      "scores on seen test split and deteriorates the perfor-\n",
      "mance on unseen test split. This trend is consistent\n",
      "across all models. Comparing results of declarative\n",
      "sentence partition and human written sentence par-\n",
      "tition, we see that for most of the models, DART\n",
      "instances with human written sentences have better\n",
      "quality as it brings more improvement to the task.\n",
      "4\n",
      "Human Evaluation\n",
      "In Table 5, we perform human evaluation on\n",
      "DART based on two criteria: (1) ﬂuency if a sen-\n",
      "tence is natural and grammatical, and (2) semantic\n",
      "faithfulness if a sentence is supported by the input\n",
      "triples. We deﬁned three levels of ﬂuency: ﬂuent,\n",
      "mostly ﬂuent, and not ﬂuent, and the same for se-\n",
      "mantic faithfulness. We ask 5 internal annotators to\n",
      "evaluate on 100 triplesets sampled from declarative\n",
      "sentence partition and another 100 triplesets sam-\n",
      "pled from human written sentence partition. Each\n",
      "tripleset is paired with 3 sentences, one of them\n",
      "is the reference sentence, and the other two are\n",
      "outputs of BART-base and T5-base models.\n",
      "The results in Table 5 attest to the high quality of\n",
      "our annotations since the human written references\n",
      "achieve highest ﬂuency and faithfulness comparing\n",
      "to outputs of two strong baseline models. The eval-\n",
      "uation on faithfulness also demonstrates that there\n",
      "is a considerable gap between the DART reference\n",
      "and the outputs of the state-of-the-art pretrained\n",
      "model, showing that there is a large room for im-\n",
      "provement. We also noticed that the auto-generated\n",
      "declarative sentences are not as ﬂuent or faithful\n",
      "as the model outputs because they are generated\n",
      "with a rule-based system. However, we decided to\n",
      "release this partition, along with other partitions of\n",
      "DART because it demonstrates an economic way\n",
      "to obtain large amounts of DART instances and it\n",
      "also shows beneﬁts for generalization due to the\n",
      "diverse topics it contains.\n",
      "5\n",
      "Related Work\n",
      "Data-to-Text\n",
      "Data-to-Text generation aims to\n",
      "produce natural language output from structured\n",
      "input. Applications include generating sports com-\n",
      "mentaries (Chen and Mooney, 2008; Wiseman\n",
      "et al., 2017), weather forecasts (Liang et al., 2009;\n",
      "Konstas and Lapata, 2012), biographical texts (Le-\n",
      "bret et al., 2016; Liu et al., 2018), knowledge-base\n",
      "descriptions (Gardent et al., 2017), dialogue re-\n",
      "sponse generation (Wen et al., 2015, 2016), and\n",
      "commonsense reasoning (Lin et al., 2020). Yet,\n",
      "most existing datasets are restricted to speciﬁc do-\n",
      "mains and applications. In contrast, a major source\n",
      "of DART is from Wikipedia tables covering various\n",
      "domains and topics.\n",
      "Representation of Data\n",
      "The input of the Data-\n",
      "to-Text datasets take different formats, including\n",
      "slot-value pairs, Abstract Meaning Representa-\n",
      "tion (AMR) (Song et al., 2017; Ribeiro et al.,\n",
      "2019), Minimal Recursion Semantics (MRS) (Ha-\n",
      "jdik et al., 2019), Resource Description Framework\n",
      "(RDF triples) (Gardent et al., 2017), and logic\n",
      "forms (Chen et al., 2020b). There are also stud-\n",
      "ies of converting tabular data to RDF triples in the\n",
      "Semantic Web community (Kellogg et al., 2015).\n",
      "Recently, some open-domain table-to-text datasets\n",
      "have been proposed including WikiTableText (Bao\n",
      "et al., 2018), LogicNLP (Chen et al., 2020a), and\n",
      "ToTTo (Parikh et al., 2020), whose inputs are rows\n",
      "or entire tables. In ToTTo, highlighted cells are\n",
      "also provided as input, and the authors found using\n",
      "only highlighted cells with ﬂat row and column\n",
      "headers led to higher performance than using the\n",
      "entire table.\n",
      "In contrast, DART is constructed by ﬁrst annotat-\n",
      "ing the tree-structured table ontology that encodes\n",
      "the semantic dependencies among table headers,\n",
      "and we could ﬂexibly incorporate additional con-\n",
      "texts such as the table title to the ontology tree.\n",
      "We then use an automatic procedure to extract con-\n",
      "nected components from the tree to form the input\n",
      "of a DART instance. Our annotation framework\n",
      "not only provides a ﬂexible way of incorporating\n",
      "any contexts to the representation of tables, but\n",
      "also encodes hierarchical relationships among ta-\n",
      "ble headers and contexts, ensuring the extracted\n",
      "triples are logically consistent and can be described\n",
      "in text without loss of information.\n",
      "Model\n",
      "Traditional Data-to-Text models break\n",
      "the generation progress into different stages such\n",
      "as signal analysis, data interpretation, document\n",
      "planning, microplanning, and realization (Reiter\n",
      "and Dale, 2000; Reiter, 2007).\n",
      "Recently, neu-\n",
      "ral encoder-decoder models based on attention\n",
      "and copy mechanisms have shown promising re-\n",
      "sults (Gehrmann et al., 2018; Puduppully et al.,\n",
      "2018, 2019; Castro Ferreira et al., 2019). Further-\n",
      "more, recent progress on pretrained models such\n",
      "as GPT-2 (Radford et al., 2018), BART (Lewis\n",
      "et al., 2020) and T5 (Raffel et al., 2020) has shown\n",
      "effective results for text generation tasks on ma-\n",
      "chine translation, summarization, and conversation\n",
      "response generation. Chen et al. (2020c); Peng\n",
      "et al. (2020); Kale (2020) also ﬁnetune pretrained\n",
      "models on Data-to-Text tasks.\n",
      "6\n",
      "Conclusion\n",
      "In this paper, we introduce DART, an open-domain\n",
      "corpus for structured data record to text generation.\n",
      "DART’s ontology-preserving representation of data\n",
      "inputs differentiates itself from other open-domain\n",
      "Data-to-Text corpora. We found that DART in-\n",
      "troduces new challenges to several state-of-the-art\n",
      "Data-to-Text models due to its open-domain nature\n",
      "and its ontology structure of the semantic triple\n",
      "input. Furthermore, we found that using it for data\n",
      "augmentation improves other Data-to-Text tasks.\n",
      "For future work, we will explore more controlled,\n",
      "high-ﬁdelity generation that better incorporates the\n",
      "ontology hierarchy of data.\n",
      "7\n",
      "Ethics Statement\n",
      "Our dataset is constructed by accumulating and\n",
      "processing resources from various existing datasets\n",
      "that are open to the public. In addition, we collect\n",
      "annotations on structure of tabular data and human\n",
      "written sentences that describe data records.\n",
      "The existing resources that we utilize mainly\n",
      "consist of (1) tabular data from Wikipedia, (2) in-\n",
      "formation of restaurants presented with dialogue-\n",
      "act meaning representation and its textual descrip-\n",
      "tion (E2E), and (3) information of various entities\n",
      "and their relationship that are in 15 different cate-\n",
      "gories of DBPedia, which is a knowledge base built\n",
      "on contents created in various Wikimedia projects\n",
      "(WebNLG). It is possible that there are biases in\n",
      "these resources, either in the tabular data or the\n",
      "textual description written by humans.\n",
      "For additional annotations we collected, we have\n",
      "two groups of annotators participating: internal\n",
      "annotators who are the authors of this work, and\n",
      "external annotators recruited from the Amazon Me-\n",
      "chanical Turk platform. On MTurk, we use a pay\n",
      "rate of $15 per hour approximately based on our\n",
      "estimation of the time it takes to complete our anno-\n",
      "tation tasks. In total, it took 125 hours to complete\n",
      "all tasks on the Amazon Mechanical Turk platform.\n",
      "There are three annotation tasks: (1) Annotators\n",
      "are asked to specify ontological structure of the\n",
      "table by indicating relationship between table col-\n",
      "umn headers, (2) Annotators are asked to write\n",
      "descriptions that are ﬂuent and semantically faith-\n",
      "ful to the data records presented to them, and (3)\n",
      "Annotators are asked to evaluate sentences that are\n",
      "either references or model generated outputs. We\n",
      "acknowledge that it is also possible to have biases\n",
      "in the sentences written by the annotators, or in the\n",
      "data records that are presented to them.\n",
      "We conducted experiments on our own dataset\n",
      "and the WebNLG dataset using BART and T5, two\n",
      "large-scale pretrained models. Both models are\n",
      "trained on large amounts of textual data such as\n",
      "news, books, and web text, which may contain any\n",
      "kinds of biases. As a result, it is possible to insert\n",
      "those biases into the models.\n",
      "In total, we conducted 43 experiments: 7 on\n",
      "DART and 36 for our ablation study on the\n",
      "WebNLG dataset. We use a single NVIDIA V100\n",
      "GPU for all experiments and each experiment took\n",
      "from 5 to 40 hours depending on the model size.\n",
      "Acknowledgement\n",
      "The authors would like to thank the anonymous\n",
      "reviewers for their discussion and feedback.\n",
      "References\n",
      "Junwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuan-\n",
      "hua Lv, Ming Zhou, and Tiejun Zhao. 2018. Table-\n",
      "to-text: Describing table region with natural lan-\n",
      "guage. In AAAI.\n",
      "Thiago Castro Ferreira, Chris van der Lee, Emiel van\n",
      "Miltenburg, and Emiel Krahmer. 2019. Neural data-\n",
      "to-text generation: A comparison between pipeline\n",
      "and end-to-end architectures. In EMNLP.\n",
      "Alison J Cawsey, Bonnie L Webber, and Ray B Jones.\n",
      "1997. Natural language generation in health care.\n",
      "David L Chen and Raymond J Mooney. 2008. Learn-\n",
      "ing to sportscast: a test of grounded language acqui-\n",
      "sition. In ICML.\n",
      "Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\n",
      "William Yang Wang. 2020a.\n",
      "Logical natural lan-\n",
      "guage generation from open-domain tables. In ACL.\n",
      "Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou\n",
      "Zhou, Yunkai Zhang, Sairam Sundaresan, and\n",
      "William Yang Wang. 2020b.\n",
      "Logic2Text: High-\n",
      "ﬁdelity natural language generation from logical\n",
      "forms. In Findings of EMNLP.\n",
      "Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu,\n",
      "and William Yang Wang. 2020c. Few-shot nlg with\n",
      "pre-trained language model. In ACL.\n",
      "Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018.\n",
      "Transforming question answering datasets into nat-\n",
      "ural language inference datasets.\n",
      "arXiv preprint\n",
      "arXiv:1809.02922.\n",
      "Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh,\n",
      "Ming-Wei Chang, Dipanjan Das, and William Co-\n",
      "hen. 2019. Handling divergent reference texts when\n",
      "evaluating table-to-text generation. In ACL.\n",
      "Ondˇrej Dušek, David M. Howcroft, and Verena Rieser.\n",
      "2019. Semantic noise matters for neural natural lan-\n",
      "guage generation. In INLG.\n",
      "Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci,\n",
      "Christophe Gravier,\n",
      "Jonathon Hare,\n",
      "Frederique\n",
      "Laforest, and Elena Simperl. 2018. T-REx: A large\n",
      "scale alignment of natural language with knowledge\n",
      "base triples. In LREC.\n",
      "Angela Fan, Claire Gardent, Chloé Braud, and An-\n",
      "toine Bordes. 2019. Using local knowledge graph\n",
      "construction to scale Seq2Seq models to multi-\n",
      "document inputs. In EMNLP-IJCNLP.\n",
      "Claire Gardent, Anastasia Shimorina, Shashi Narayan,\n",
      "and Laura Perez-Beltrachini. 2017. The WebNLG\n",
      "challenge: Generating text from RDF data. In INLG.\n",
      "Sebastian Gehrmann, Falcon Dai, Henry Elder, and\n",
      "Alexander Rush. 2018. End-to-end content and plan\n",
      "selection for data-to-text generation. In INLG.\n",
      "Valerie Hajdik, Jan Buys, Michael Wayne Goodman,\n",
      "and Emily M Bender. 2019. Neural text generation\n",
      "from rich semantic representations. In NAACL.\n",
      "Hamza Harkous, Isabel Groves, and Amir Saffari. 2020.\n",
      "Have your text and use it too! end-to-end neural\n",
      "data-to-text generation with semantic ﬁdelity. arXiv\n",
      "preprint arXiv:2004.06577.\n",
      "Mihir Kale. 2020. Text-to-text pre-training for data-to-\n",
      "text tasks. arXiv preprint arXiv:2005.10433.\n",
      "Gregg Kellogg, Ivan Herman, and Jeremy Tandy.\n",
      "2015.\n",
      "Generating\n",
      "RDF\n",
      "from\n",
      "tabular\n",
      "data\n",
      "on the web.\n",
      "W3C recommendation,\n",
      "W3C.\n",
      "Https://www.w3.org/TR/2015/REC-csv2rdf-\n",
      "20151217/.\n",
      "Ioannis Konstas and Mirella Lapata. 2012. Unsuper-\n",
      "vised concept-to-text generation with hypergraphs.\n",
      "In NAACL.\n",
      "Rémi Lebret, David Grangier, and Michael Auli. 2016.\n",
      "Neural text generation from structured data with ap-\n",
      "plication to the biography domain. In EMNLP.\n",
      "Mike\n",
      "Lewis,\n",
      "Yinhan\n",
      "Liu,\n",
      "Naman\n",
      "Goyal,\n",
      "Mar-\n",
      "jan Ghazvininejad, Abdelrahman Mohamed, Omer\n",
      "Levy,\n",
      "Ves\n",
      "Stoyanov,\n",
      "and\n",
      "Luke\n",
      "Zettlemoyer.\n",
      "2020. BART: Denoising sequence-to-sequence pre-\n",
      "training for natural language generation, translation,\n",
      "and comprehension. In ACL.\n",
      "Percy Liang, Michael I Jordan, and Dan Klein. 2009.\n",
      "Learning semantic correspondences with less super-\n",
      "vision. In ACL.\n",
      "Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\n",
      "Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang\n",
      "Ren. 2020. CommonGen: A constrained text gener-\n",
      "ation challenge for generative commonsense reason-\n",
      "ing. In Findings of EMNLP.\n",
      "Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,\n",
      "and Zhifang Sui. 2018. Table-to-text generation by\n",
      "structure-aware seq2seq learning. In AAAI.\n",
      "Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.\n",
      "Step-by-step: Separating planning from realization\n",
      "in neural data-to-text generation. In NAACL.\n",
      "Jekaterina Novikova, Ondˇrej Dušek, Amanda Cercas\n",
      "Curry, and Verena Rieser. 2017a. Why we need new\n",
      "evaluation metrics for nlg. In EMNLP.\n",
      "Jekaterina Novikova, Ondrej Dusek, and Verena Rieser.\n",
      "2017b. The E2E dataset: New challenges for end-to-\n",
      "end generation. In SIGDIAL.\n",
      "Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann,\n",
      "Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and\n",
      "Dipanjan Das. 2020. ToTTo: A controlled table-to-\n",
      "text generation dataset. In EMNLP.\n",
      "Panupong Pasupat and Percy Liang. 2015. Composi-\n",
      "tional semantic parsing on semi-structured tables. In\n",
      "ACL.\n",
      "Baolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun\n",
      "Li, Jinchao Li, Michael Zeng, and Jianfeng Gao.\n",
      "2020.\n",
      "Few-shot natural language generation for\n",
      "task-oriented dialog. In arXiv.\n",
      "Ratish Puduppully, Li Dong, and Mirella Lapata. 2018.\n",
      "Data-to-text generation with content selection and\n",
      "planning. In AAAI.\n",
      "Ratish Puduppully, Li Dong, and Mirella Lapata. 2019.\n",
      "Data-to-text generation with entity modeling.\n",
      "In\n",
      "ACL.\n",
      "Alec Radford, Karthik Narasimhan, Tim Salimans, and\n",
      "Ilya Sutskever. 2018.\n",
      "Improving language under-\n",
      "standing by generative pre-training.\n",
      "Technical re-\n",
      "port, OpenAI.\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\n",
      "Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\n",
      "Wei Li, and Peter J. Liu. 2020. Exploring the lim-\n",
      "its of transfer learning with a uniﬁed text-to-text\n",
      "transformer. Journal of Machine Learning Research,\n",
      "21(140):1–67.\n",
      "Ehud Reiter. 2007.\n",
      "An architecture for data-to-text\n",
      "systems. In Proceedings of the Eleventh European\n",
      "Workshop on Natural Language Generation (ENLG\n",
      "07).\n",
      "Ehud Reiter. 2020. Openai gpt system: What does it\n",
      "do? Technical report, Arria.\n",
      "Ehud Reiter and Robert Dale. 2000. Building natural\n",
      "language generation systems. Cambridge university\n",
      "press.\n",
      "Leonardo F. R. Ribeiro, Claire Gardent, and Iryna\n",
      "Gurevych. 2019.\n",
      "Enhancing AMR-to-text genera-\n",
      "tion with dual graph representations. In EMNLP.\n",
      "Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich\n",
      "Schütze, and Iryna Gurevych. 2020. Investigating\n",
      "pretrained language models for graph-to-text gener-\n",
      "ation. arXiv.\n",
      "Thibault Sellam, Dipanjan Das, and Ankur P Parikh.\n",
      "2020.\n",
      "BLEURT: Learning robust metrics for text\n",
      "generation. In ACL.\n",
      "Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo\n",
      "Wang, and Daniel Gildea. 2017. Amr-to-text gener-\n",
      "ation with synchronous node replacement grammar.\n",
      "In ACL.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\n",
      "Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. In NeurIPS.\n",
      "Pavlos Vougiouklis,\n",
      "Hady ElSahar,\n",
      "Lucie-Aimée\n",
      "Kaffee, Christophe Gravier, Frédérique Laforest,\n",
      "Jonathon S. Hare, and Elena Simperl. 2018. Neu-\n",
      "ral wikipedian: Generating textual summaries from\n",
      "knowledge base triples. Journal of Web Semantics,\n",
      "52-53:1 – 15.\n",
      "Tsung-Hsien Wen,\n",
      "Milica Gaši´c,\n",
      "Nikola Mrkši´c,\n",
      "Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes,\n",
      "David Vandyke, and Steve Young. 2016.\n",
      "Condi-\n",
      "tional generation and snapshot learning in neural di-\n",
      "alogue systems. In EMNLP.\n",
      "Tsung-Hsien Wen, Milica Gaši´c, Nikola Mrkši´c, Pei-\n",
      "Hao Su, David Vandyke, and Steve Young. 2015.\n",
      "Semantically conditioned LSTM-based natural lan-\n",
      "guage generation for spoken dialogue systems. In\n",
      "EMNLP.\n",
      "Sam Wiseman, Stuart Shieber, and Alexander Rush.\n",
      "2017.\n",
      "Challenges in data-to-document generation.\n",
      "In EMNLP.\n",
      "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\n",
      "Weinberger, and Yoav Artzi. 2020.\n",
      "BERTScore:\n",
      "Evaluating text generation with BERT. In ICLR.\n",
      "Chao Zhao, Marilyn Walker, and Snigdha Chaturvedi.\n",
      "2020. Bridging the structural gap between encoding\n",
      "and decoding for data-to-text generation. In ACL.\n",
      "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\n",
      "tian M Meyer, and Steffen Eger. 2019. MoverScore:\n",
      "Text generation evaluating with contextualized em-\n",
      "beddings and earth mover distance. In EMNLP.\n",
      "Victor Zhong, Caiming Xiong, and Richard Socher.\n",
      "2017.\n",
      "Seq2sql:\n",
      "Generating structured queries\n",
      "from natural language using reinforcement learning.\n",
      "CoRR, abs/1709.00103.\n",
      "Appendix\n",
      "The Appendix contains the following contents:\n",
      "• Results of the ablation study on WebNLG 2017 testset.\n",
      "• Statistics of the table ontology annotations.\n",
      "• Examples of tables that help illustrate DART’s annotation procedure.\n",
      "• Examples of model outputs.\n",
      "Model\n",
      "Experiment\n",
      "BLEU ↑\n",
      "METEOR ↑\n",
      "TER ↓\n",
      "SEEN\n",
      "UNSEEN\n",
      "ALL\n",
      "SEEN\n",
      "UNSEEN\n",
      "ALL\n",
      "SEEN\n",
      "UNSEEN\n",
      "ALL\n",
      "Baseline\n",
      "Transformer\n",
      "[1] webnlg\n",
      "49.81\n",
      "5.51\n",
      "31.81\n",
      "0.39\n",
      "0.09\n",
      "0.24\n",
      "0.47\n",
      "0.86\n",
      "0.64\n",
      "[2] webnlg+dart_decl_sents\n",
      "52.31\n",
      "8.96\n",
      "39.98\n",
      "0.40\n",
      "0.07\n",
      "0.25\n",
      "0.45\n",
      "0.79\n",
      "0.60\n",
      "[3] webnlg+dart_human_annotated\n",
      "53.68\n",
      "7.02\n",
      "36.36\n",
      "0.40\n",
      "0.09\n",
      "0.26\n",
      "0.43\n",
      "0.79\n",
      "0.59\n",
      "[4] webnlg+dart_ontology\n",
      "53.40\n",
      "8.54\n",
      "38.51\n",
      "0.41\n",
      "0.08\n",
      "0.26\n",
      "0.44\n",
      "0.80\n",
      "0.60\n",
      "[5] webnlg+dart_e2e\n",
      "51.76\n",
      "5.92\n",
      "32.36\n",
      "0.40\n",
      "0.09\n",
      "0.25\n",
      "0.45\n",
      "0.86\n",
      "0.63\n",
      "[6] webnlg+dart_full\n",
      "54.99\n",
      "8.64\n",
      "39.11\n",
      "0.40\n",
      "0.08\n",
      "0.25\n",
      "0.42\n",
      "0.81\n",
      "0.60\n",
      "BART-base\n",
      "[1] webnlg\n",
      "63.02\n",
      "41.74\n",
      "53.36\n",
      "0.45\n",
      "0.35\n",
      "0.40\n",
      "0.33\n",
      "0.52\n",
      "0.42\n",
      "[2] webnlg+dart_decl_sents\n",
      "62.71\n",
      "42.51\n",
      "53.64\n",
      "0.45\n",
      "0.36\n",
      "0.40\n",
      "0.34\n",
      "0.51\n",
      "0.41\n",
      "[3] webnlg+dart_human_annotated\n",
      "62.36\n",
      "46.21\n",
      "55.14\n",
      "0.44\n",
      "0.37\n",
      "0.41\n",
      "0.34\n",
      "0.45\n",
      "0.39\n",
      "[4] webnlg+dart_ontology\n",
      "62.62\n",
      "46.74\n",
      "55.54\n",
      "0.44\n",
      "0.38\n",
      "0.41\n",
      "0.34\n",
      "0.45\n",
      "0.39\n",
      "[5] webnlg+dart_e2e\n",
      "64.00\n",
      "35.07\n",
      "51.17\n",
      "0.45\n",
      "0.33\n",
      "0.40\n",
      "0.33\n",
      "0.61\n",
      "0.46\n",
      "[6] webnlg+dart_full\n",
      "63.66\n",
      "45.48\n",
      "55.52\n",
      "0.45\n",
      "0.37\n",
      "0.41\n",
      "0.33\n",
      "0.47\n",
      "0.40\n",
      "BART-large\n",
      "[1] webnlg\n",
      "63.71\n",
      "44.17\n",
      "54.95\n",
      "0.46\n",
      "0.39\n",
      "0.42\n",
      "0.33\n",
      "0.51\n",
      "0.41\n",
      "[2] webnlg+dart_decl_sents\n",
      "65.18\n",
      "46.79\n",
      "56.79\n",
      "0.46\n",
      "0.39\n",
      "0.42\n",
      "0.32\n",
      "0.48\n",
      "0.40\n",
      "[3] webnlg+dart_human_annotated\n",
      "64.51\n",
      "50.20\n",
      "58.06\n",
      "0.46\n",
      "0.40\n",
      "0.43\n",
      "0.32\n",
      "0.44\n",
      "0.38\n",
      "[4] webnlg+dart_ontology\n",
      "64.19\n",
      "49.62\n",
      "57.65\n",
      "0.46\n",
      "0.39\n",
      "0.43\n",
      "0.33\n",
      "0.45\n",
      "0.38\n",
      "[5] webnlg+dart_e2e\n",
      "65.06\n",
      "30.17\n",
      "48.24\n",
      "0.46\n",
      "0.33\n",
      "0.40\n",
      "0.32\n",
      "0.69\n",
      "0.49\n",
      "[6] webnlg+dart_full\n",
      "65.24\n",
      "47.96\n",
      "57.44\n",
      "0.46\n",
      "0.39\n",
      "0.43\n",
      "0.32\n",
      "0.46\n",
      "0.39\n",
      "T5-small\n",
      "[1] webnlg\n",
      "65.30\n",
      "45.58\n",
      "56.57\n",
      "0.46\n",
      "0.39\n",
      "0.43\n",
      "0.32\n",
      "0.49\n",
      "0.40\n",
      "[2] webnlg+dart_decl_sents\n",
      "64.18\n",
      "46.61\n",
      "56.27\n",
      "0.46\n",
      "0.39\n",
      "0.43\n",
      "0.33\n",
      "0.48\n",
      "0.40\n",
      "[3] webnlg+dart_human_annotated\n",
      "65.05\n",
      "47.81\n",
      "57.32\n",
      "0.46\n",
      "0.40\n",
      "0.43\n",
      "0.33\n",
      "0.46\n",
      "0.39\n",
      "[4] webnlg+dart_ontology\n",
      "65.17\n",
      "47.49\n",
      "57.24\n",
      "0.46\n",
      "0.39\n",
      "0.43\n",
      "0.32\n",
      "0.47\n",
      "0.39\n",
      "[5] webnlg+dart_e2e\n",
      "65.56\n",
      "41.28\n",
      "54.56\n",
      "0.46\n",
      "0.38\n",
      "0.42\n",
      "0.32\n",
      "0.54\n",
      "0.42\n",
      "[6] webnlg+dart_full\n",
      "64.70\n",
      "47.56\n",
      "57.01\n",
      "0.46\n",
      "0.39\n",
      "0.43\n",
      "0.33\n",
      "0.47\n",
      "0.39\n",
      "T5-base\n",
      "[1] webnlg\n",
      "64.89\n",
      "52.86\n",
      "59.44\n",
      "0.46\n",
      "0.42\n",
      "0.44\n",
      "0.33\n",
      "0.42\n",
      "0.37\n",
      "[2] webnlg+dart_decl_sents\n",
      "65.44\n",
      "50.80\n",
      "58.81\n",
      "0.46\n",
      "0.41\n",
      "0.44\n",
      "0.32\n",
      "0.43\n",
      "0.37\n",
      "[3] webnlg+dart_human_annotated\n",
      "65.42\n",
      "50.71\n",
      "58.80\n",
      "0.46\n",
      "0.41\n",
      "0.44\n",
      "0.32\n",
      "0.43\n",
      "0.37\n",
      "[4] webnlg+dart_ontology\n",
      "65.17\n",
      "51.49\n",
      "59.04\n",
      "0.46\n",
      "0.41\n",
      "0.44\n",
      "0.33\n",
      "0.43\n",
      "0.37\n",
      "[5] webnlg+dart_e2e\n",
      "65.11\n",
      "49.64\n",
      "58.19\n",
      "0.46\n",
      "0.41\n",
      "0.44\n",
      "0.33\n",
      "0.46\n",
      "0.39\n",
      "[6] webnlg+dart_full\n",
      "65.99\n",
      "51.68\n",
      "59.50\n",
      "0.46\n",
      "0.42\n",
      "0.44\n",
      "0.32\n",
      "0.43\n",
      "0.37\n",
      "T5-large\n",
      "[1] webnlg\n",
      "64.89\n",
      "54.01\n",
      "59.95\n",
      "0.46\n",
      "0.43\n",
      "0.44\n",
      "0.34\n",
      "0.41\n",
      "0.37\n",
      "[2] webnlg+dart_decl_sents\n",
      "65.97\n",
      "53.00\n",
      "60.12\n",
      "0.46\n",
      "0.42\n",
      "0.44\n",
      "0.32\n",
      "0.41\n",
      "0.36\n",
      "[3] webnlg+dart_human_annotated\n",
      "65.82\n",
      "56.01\n",
      "61.44\n",
      "0.46\n",
      "0.43\n",
      "0.45\n",
      "0.32\n",
      "0.38\n",
      "0.35\n",
      "[4] webnlg+dart_ontology\n",
      "65.53\n",
      "55.20\n",
      "60.90\n",
      "0.46\n",
      "0.42\n",
      "0.44\n",
      "0.32\n",
      "0.38\n",
      "0.35\n",
      "[5] webnlg+dart_e2e\n",
      "66.27\n",
      "54.13\n",
      "60.76\n",
      "0.46\n",
      "0.43\n",
      "0.45\n",
      "0.32\n",
      "0.41\n",
      "0.36\n",
      "[6] webnlg+dart_full\n",
      "65.78\n",
      "54.35\n",
      "60.64\n",
      "0.46\n",
      "0.42\n",
      "0.44\n",
      "0.32\n",
      "0.39\n",
      "0.35\n",
      "Table 6: Results of ablation study on WebNLG 2017 testset. dart_decl_sents refers to DART partition that contains\n",
      "auto-generated declarative sentences mentioned in Section 2.2, dart_human_annotated refers to partition that\n",
      "contains human written sentences mentioned in Section 2.1, dart_ontology is the combination of dart_decl_sents\n",
      "and dart_human_annotated, and dart_e2e refers to DART partition containing instances extracted from E2E\n",
      "dataset, the process of which is mentioned in Section 2.3. Note that dart_full is the combination of dart_ontology\n",
      "and dart_e2e.\n",
      "Tables\n",
      "Ontology depth\n",
      "(min, med, max)\n",
      "Nodes in ontology\n",
      "(min, med, max)\n",
      "Branching factor\n",
      "(mean)\n",
      "WikiTableQuestions\n",
      "2060\n",
      "1, 1, 4\n",
      "2, 6, 25\n",
      "4.0\n",
      "WikiSQL\n",
      "3563\n",
      "1, 1, 4\n",
      "3, 7, 25\n",
      "5.1\n",
      "Table 7: Properties of the ontology in the WikiTableQuestions and WikiSQL samples in DART. Branching factor\n",
      "refers to the average number of children across all non-leaf nodes in a table’s ontology.\n",
      "Figure 3:\n",
      "Distribution of column ontology depths in the WikiTableQuestions and WikiSQL samples in\n",
      "DART v1.1.1.\n",
      "<entry category=\"MISC\" eid=\"Id5\" size=\"3\">\n",
      "<modifiedtripleset>\n",
      "<mtriple>Apertura 2006 | JORNADA_OR_OTHER | Semifinals Ida</mtriple>\n",
      "<mtriple>Semifinals Ida | AWAY_TEAM | América</mtriple>\n",
      "<mtriple>Semifinals Ida | HOME_TEAM | Chivas</mtriple>\n",
      "</modifiedtripleset>\n",
      "<lex comment=\"WikiTableQuestions\" lid=\"Id1\">\n",
      "Chivas and América will compete in the semifinals of the Apertura 2006 tournament.\n",
      "</lex>\n",
      "</entry>\n",
      "<entry category=\"MISC\" eid=\"Id76\" size=\"6\">\n",
      "<modifiedtripleset>\n",
      "<mtriple>Terry Jenkins | ROUND | 1st Round</mtriple>\n",
      "<mtriple>Terry Jenkins | YEAR | 2014</mtriple>\n",
      "<mtriple>[TABLECONTEXT] | [TITLE] | PDC World Darts Championship</mtriple>\n",
      "<mtriple>1st Round | OPPONENT | Per Laursen</mtriple>\n",
      "<mtriple>1st Round | RESULT | Lost</mtriple>\n",
      "<mtriple>[TABLECONTEXT] | PLAYER | Terry Jenkins</mtriple>\n",
      "</modifiedtripleset>\n",
      "<lex comment=\"WikiTableQuestions\" lid=\"Id1\">\n",
      "Terry Jenkins lost the game with Per Laursen in\n",
      "the 1st Round of 2014 PDC World Darts Championship\n",
      "</lex>\n",
      "</entry>\n",
      "Figure 4: Examples of DART instance\n",
      "Figure 5: An example of the data cleaning. The top left table had a missing column name and the table title was\n",
      "not speciﬁc to the data; our internal annotators add the missing column name “Year” and linked the rest of the\n",
      "columns to the “Year” column. The bottom left table had repeat column names in the table; our internal annotators\n",
      "disambiguate the columns by making the column names more speciﬁc.\n",
      "Figure 6: A WikiTableQuestions table that uses [TITLE] in the ontology.\n",
      "Figure 7: A manually annotated table from WikiTableQuestions with a sentence that uses the table title.\n",
      "Figure 8: A manually annotated table from WikiTableQuestions. Annotators created a table ontology, and they\n",
      "wrote sentences encapsulating the information in the orange cells for a given row. Whenever a sentence referenced\n",
      "the table title, that sentence was also highlighted green.\n",
      "Figure 9: An example of collected MTurk-generated sentences for WikiTableQuestions. Internal annotators went\n",
      "through the generated sentences and checked for both sentence coherence and title usage. Below the generated\n",
      "sentences, ‘y’ meant the sentence references the table title, ‘n’ meant the sentence did not use the table title, ‘x’\n",
      "meant the sentence was nonsensical.\n",
      "Figure 10: Automatically generated declarative sentences from WikiSQL with human validation. Annotators went\n",
      "through the generated sentences and checked for both sentence coherence and title use. Below the generated\n",
      "sentences, ‘y’ meant the sentence references the table title, ‘n’ meant the sentence did not use the table title, ‘x’\n",
      "meant the sentence was nonsensical.\n",
      "- Sample 1 -\n",
      "Input triples:\n",
      "<H> Peru Earthquake <R> scale of disaster <T> 250k homeless\n",
      "<H> Peru Earthquake <R> year <T> 2007\n",
      "BART-base output: 250k people were killed in the 2007 philippine earthquake .\n",
      "- Sample 2 -\n",
      "Input triples:\n",
      "<H> [TABLECONTEXT] <R> game <T> 3\n",
      "<H> 3 <R> attendance <T> 10 637\n",
      "<H> [TABLECONTEXT] <R> [title] <T> 2006 Minnesota Swarm season\n",
      "BART-base output: the minnesota swarm played in front of a crowd of 10 , 684 people .\n",
      "- Sample 3 -\n",
      "Input triples:\n",
      "<H> Andrew Phelps McCormick <R> state <T> TX\n",
      "<H> Andrew Phelps McCormick <R> active <T> 1892-1916\n",
      "T5-base output: andrew phelps mccormick was active from 1892 to 1616 in texas .\n",
      "Figure 11: Examples of hallucinated outputs of pretrained models trained on DART\n",
      "- Sample 1 -\n",
      "Input triples:\n",
      "<H> Andrew Rayel <R> associated Band/associated Musical Artist <T> Christian Burns\n",
      "<H> Andrew Rayel <R> associated Band/associated Musical Artist <T> Jonathan Mendelsohn\n",
      "reference:\n",
      "andrew rayel , is associated with musical artist jonathan mendelsohn and christian burns .\n",
      "train on WebNLG - BART-base output:\n",
      "christian mendelsohn and andrew rayel are both associated with the same band , christian burns .\n",
      "train on DART - BART-base output:\n",
      "andrew rayel is associated with christian burns and jonathan mendelsohn .\n",
      "- Sample 2 -\n",
      "Input triples:\n",
      "<H> Indie rock <R> stylistic Origin <T> New wave music\n",
      "reference: the stylistic origin of indie rock is new wave music .\n",
      "train on WebNLG - BART-base output:\n",
      "the alternative rock genre is new wave .\n",
      "train on DART - BART-base output:\n",
      "indie rock is influenced by new wave music .\n",
      "- Sample 3 -\n",
      "Input triples:\n",
      "<H> Abradab <R> associated Band/associated Musical Artist <T> Magik rapper\n",
      "<H> Abradab <R> associated Band/associated Musical Artist <T> Kaliber 44\n",
      "reference:\n",
      "abradab , an artist for the band kaliber 44 , is associated with magik ( rapper ) .\n",
      "train on WebNLG - BART-base output:\n",
      "magiber 44 is the creator of abradab , which is also associated with the magik rapper .\n",
      "train on DART - BART-base output:\n",
      "magik rapper and kaliber 44 are the associated musicians of abradab .\n",
      "- Sample 4 -\n",
      "Input triples:\n",
      "<H> Alfa Romeo 164 <R> assembly <T> Milan\n",
      "<H> Alfa Romeo 164 <R> related Mean Of Transportation <T> Saab 9000\n",
      "reference:\n",
      "the alfa romeo 164 , which is assembled in milan , is a related means of transportation to saab 9000 ,\n",
      "in that they are both cars .\n",
      "train on WebNLG - T5-base output:\n",
      "alfa romeo 164 is a transport vehicle for saab 9000 and is found in milan .\n",
      "train on DART - T5-base output:\n",
      "alfa romeo 164 ( assembled in milan ) is a related transport vehicle to saab 9000 .\n",
      "- Sample 5 -\n",
      "Input triples:\n",
      "<H> Akeem Ayers <R> former Team <T> Tennessee Titans\n",
      "<H> Akeem Ayers <R> draft Pick <T> 39\n",
      "reference:\n",
      "akeem ayers ’ former team was tennessee titans and he was number 39 in the draft pick .\n",
      "train on WebNLG - T5-large output:\n",
      "akeem ayers was drafted with the 39th pick by the tennessee titans .\n",
      "train on DART - T5-large output:\n",
      "akeem ayers , a former player of the tennessee titans , was the 39th draft pick .\n",
      "Figure 12: Examples of model outputs - with or without DART data augmentation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_pdf_content(document):\n",
    "    content = \"\"\n",
    "    for page in document:\n",
    "        content += page.get_text()\n",
    "    return content\n",
    "\n",
    "print(get_pdf_content(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pymupdf_extractable(document,content):\n",
    "    print(document.metadata['creator'])\n",
    "    acceptable_creator = ['Springer','Pages', 'LaTeX with hyperref']\n",
    "    words_threshold = 20\n",
    "    if (document.metadata['creator'] in acceptable_creator) and (len(content) > words_threshold):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def write_json_file(filename, content):\n",
    "    pathlib.Path(JSON_BASE+filename).write_bytes(content.encode('utf-8').strip())\n",
    "\n",
    "def save_content(document,content):\n",
    "    document_dict = dict()\n",
    "    filename = document.metadata['title']+'.json'\n",
    "    document_dict['title'] = document.metadata['title']\n",
    "    document_dict['text'] = content\n",
    "    document_dict['extraction_date'] = str(datetime.utcnow())\n",
    "    document_dict['num_pages'] = document.page_count\n",
    "    json_object = json.dumps(document_dict) \n",
    "    write_json_file(filename,json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX with hyperref\n"
     ]
    }
   ],
   "source": [
    "content = get_pdf_content(document)\n",
    "if is_pymupdf_extractable(document,content):\n",
    "    save_content(document,content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
