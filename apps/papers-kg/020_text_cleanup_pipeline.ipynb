{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pathlib\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = 'ArxivHealthcareNLP'\n",
    "#CORPUS = 'arxiv_cl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_properties(filepath, sep='=', comment_char='#'):\n",
    "    '''\n",
    "    Read the file passed as parameter as a properties file.\n",
    "    '''\n",
    "    props = {}\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            if l and not l.startswith(comment_char):\n",
    "                key_value = l.split(sep)\n",
    "                key = key_value[0].strip()\n",
    "                value = sep.join(key_value[1:]).strip().strip('\"') \n",
    "                props[key] = value \n",
    "    return props\n",
    "\n",
    "corpus_properties = load_properties(f\"corpora/{CORPUS}.properties\")\n",
    "corpus_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_BASE = corpus_properties['corpus_base']\n",
    "JSON_RAW_BASE = f'{CORPUS_BASE}/json_raw/'\n",
    "TXT_BASE = f'{CORPUS_BASE}/text_cleaned/'\n",
    "JSON_BASE = f'{CORPUS_BASE}/json_cleaned/'\n",
    "\n",
    "if not os.path.exists(JSON_BASE):\n",
    "    print(f'{JSON_BASE} does not exist. Creating.')\n",
    "    os.makedirs(JSON_BASE)\n",
    "\n",
    "if not os.path.exists(TXT_BASE):\n",
    "    print(f'{TXT_BASE} does not exist. Creating.')\n",
    "    os.makedirs(TXT_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "json_files = [f for f in listdir(JSON_RAW_BASE) if isfile(join(JSON_RAW_BASE, f))]\n",
    "len(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    json_content = pathlib.Path(filename).read_bytes()\n",
    "    print(len(json_content))\n",
    "    return json_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(raw_text):\n",
    "    text = raw_text\n",
    "\n",
    "    # convert split words on line break, e.g. post-\\nediting\n",
    "    text = text.replace('-\\n', '')\n",
    "    # remove lone new lines in the middle of the sentence - leave only the new lines after .(dot)\n",
    "    one_new_line = r'(?<![\\.\\n])\\n(?!\\n)'\n",
    "    text = re.sub(one_new_line, ' ', text)\n",
    "    # encoded sequences will always generate sequences larger than the chunk size\n",
    "    latexit = r'<latexit.*latexit> *'\n",
    "    text = re.sub(latexit, ' ', text)\n",
    "    # remove urls - not needed for KG\n",
    "    text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "    # remove email addresses - not needed for KG\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    # remove references\n",
    "    text = re.sub(r'doi\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'abs\\/.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'URL\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'url\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'arXiv\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    # remove everythng between parathesis\n",
    "    text = re.sub(r'\\(([^)]+)\\)', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\[([^]]+)\\]', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\{([^}]+)\\}', '', text, flags=re.MULTILINE)   \n",
    "    # remove Figure captions:\n",
    "    text = re.sub(r'(Figure [0-9]*:*)', '', text)\n",
    "    # remove Table captions:\n",
    "    text = re.sub(r'(Table [0-9]*:*)', '', text)\n",
    "    # remove numbers\n",
    "    numbers = r'[0-9]'\n",
    "    text = re.sub(numbers, '', text)\n",
    "    # lists\n",
    "    lists = r'(i\\))|(ii\\))|(iii\\))|(xvii)|(xviii)'\n",
    "    text = re.sub(lists, '', text)\n",
    "    lists = r'(I\\))|(II\\))|(III\\.)|(XVII)|(XVIII)'\n",
    "    text = re.sub(lists, '', text)\n",
    "    #[] () .% -, .+ at the beginning of the line TODO |( \\. )\n",
    "    group_of_characters = r'(\\(\\))|(\\[\\])|(\\.%)|(, \\.)|(â€“,)|(\\-\\-)|(, \\:\\.)'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    #( ) [ ] [,][, ]\n",
    "    group_of_characters = r'(\\( \\))|(\\[ \\])|(\\[,\\])|(\\[, \\])'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    group_of_characters = r'^(\\.+\\s)|^(,+\\s)|^\\.'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    #stray abbreviations\n",
    "    words = r'(vol\\.)|(no\\.)|(pp\\.)|(Rec@)|(Fig\\.)|(\\b\\.v\\b)|(\\sv\\s)|(\\sb\\s)|(Aug\\.)|(Jan)|(Nov\\.)'\n",
    "    text = re.sub(words, '', text)\n",
    "    #stray commas TODO - ( \\, )|\n",
    "    group_of_characters_2 = r'(,(\\s*)\\n)'\n",
    "    text = re.sub(group_of_characters_2, '', text)\n",
    "    #stray characters\n",
    "    special_characters = r'[â€“â™£â™¥â™ â™¦â€¢ï¿½Â±ğ›¹ğ´âˆ’ğ‘¦âœ“âŸ¨âŸ©â„ğ‘¥\"ğœƒâ€¦ğ·ğ‘‹ğ‘£ğ‘¥\"ğ‘ğ’†\"Î¸Î´âˆˆÃ—ğ‘Ÿğ‘¡ğ‘¨ğ’—âˆ—ğ’™ğ¶ğ¿ğ‘†ğµğ‘€ğ‘†ğ¾ğ‘™ğ‘–ğ‘˜ğ‘’ğ‘ âˆ€â†‘â†“ğ‘‘â†’â€ ğ‘ğ‘”ğ‘›âŠ¤âˆšÏ€â™¢â™¡ï£´ï£±ï£³ï£«ï£­ÂµÎ»ğ‘šğ‘ğ‘ƒÎµğ‘§ğ”„ğœ‹â—¦ğ»ğ‘‡â„•âˆ…âˆ‘ï¸âŠ‚âˆ¥â€²âœ—â‡’Â¬âˆ§â–½ğ‘¯ğ‘»â†”â„­â†¦ğŸğğ‘«]'\n",
    "    text = re.sub(special_characters, '', text)\n",
    "    #leave '-' as part of e.g. bert \n",
    "    special_characters = r'[\\&%=_\\Ë†â‰¥+|Ëœ!#$<>â€”â‰¤Â¯ğ’Šğœ½ğ‹ğ’“âŠ¥âˆ¥Î¦â‰ âˆğšºğœ»âˆ¼ÏƒÎ²Â·]'\n",
    "    text = re.sub(special_characters, '', text)\n",
    "    special_characters = r'[é¹åŸå®éªŒå®¤é¹åŸå®éªŒå®¤æ¨å‡ºé¢å‘ä¸­æ–‡åŒ»ç–—æ–‡æœ¬å¤„ç†çš„é¢„è®­ç»ƒæ¨¡å‹é˜»å¡æ€§ç¡çœ å‘¼å¸æš‚åœä¸‹è¿°å“ªä¸€é¡¹ä¸ç¬¦åˆSLEè¡€æ¶²ç³»ç»Ÿæ”¹å˜å›ç­”é€‰é¡¹é€‰é¡¹Aè¡€å°æ¿å‡å°‘é€‰é¡¹B è‡ªç»†èƒå‡å°‘í‘†å»å°¿è¿‡í‘†]'\n",
    "   text = re.sub(special_characters, '', text)\n",
    "    special_characters = r'[é€‰é¡¹Cè‡ªèº«å…ç–«æº¶è¡€è´«è¡€é€‰é¡¹Dæ­£è‰²ç´ ç»†èƒè´«è¡€é€‰é¡¹Eç±»ç™½è¡€ç—…æ ·æ”¹å˜SLEæ˜¯ä¸€ç§è‡ªèº«å…ç–«ç–¾ç—…å…¶è¡€æ¶²ç³»ç»Ÿæ”¹å˜åŒ…æ‹¬è¡€å°æ¿å‡å°‘è‡ªèº«å…ç–«æº¶è¡€è´«è¡€æ­£è‰²ç´ ç»†èƒè´«è¡€ç­‰è€Œç±»ç™½è¡€ç—…æ ·æ”¹å˜æ˜¯æŒ‡éª¨é«“ç°å¤§é‡å¹¼ç¨šç»†èƒä¸SLEæ— å…³å› æ­¤é€‰é¡¹Eä¸ç¬¦åˆSLEè¡€æ¶²ç³»ç»Ÿæ”¹å˜]'\n",
    "    text = re.sub(special_characters, '', text)\n",
    "    #fi  \n",
    "    special_characters = r'[\\ufb01]'\n",
    "    text = re.sub(special_characters, 'fi', text)\n",
    "    #ffi \n",
    "    special_characters = r'[\\ufb03]'\n",
    "    text = re.sub(special_characters, 'ffi', text)\n",
    "    #empty lines\n",
    "    empty_line = r'\\.\\n(\\.+)'\n",
    "    text = re.sub(empty_line, '\\.\\n', text)\n",
    "    #paragraph names, e.g. A., B., C. - llms are picking them up as topics\n",
    "    paragraph_names = r'\\n(\\s*)[a-zA-Z]\\.'\n",
    "    text = re.sub(paragraph_names, '\\n', text)\n",
    "    #try again to remove numbers\n",
    "    numbers = r'[0-9]'\n",
    "    text = re.sub(numbers, '', text)\n",
    "    #stray letters\n",
    "    group_of_characters_3 = r'(\\bD\\b)|( B )|( o )|(\\bs\\b)|( Xn )|(\\bX\\b)|(\\by\\b)|( m )|(\\bi\\b)|( c )|(\\br\\b)|(\\bk\\b)|(\\bd\\b)|(\\bt\\b)|(\\bvt\\b)|(\\bxn\\b)|(\\bXn\\b)|(\\be\\b)|(\\bL\\b)'\n",
    "    text = re.sub(group_of_characters_3, '', text)\n",
    "    group_of_characters_3 = r'( m )|( b )|(\\bx\\b)|(\\bS\\b)|( F )|( g )|(\\bC\\b)|( Z )|( z )|( Xu )|( R )|( \\/ )|( w )|( U= )|( V )|( M )|(dx)|(\\bxt\\b)|(\\bTn\\b)|(\\btn\\b)'\n",
    "    text = re.sub(group_of_characters_3, '', text)\n",
    "    group_of_characters_3 = r'(\\bT\\b)|(\\bP\\b)|(\\bMC\\b)|(\\b.-\\b)|(\\b-.\\b)|(\\bK\\b)|(\\bp\\b)|(\\bl\\b)|(\\b-.\\b)|( Xu )|( R )|(\\b\\/\\b)|( w )|( U= )|( V )|( M )|(dx)|(\\bxt\\b)|(\\bTn\\b)'\n",
    "    text = re.sub(group_of_characters_3, '', text)\n",
    "    group_of_characters_3 = r'(\\bJ\\.\\b)|(\\bM\\.\\b)|(\\bA\\.\\b)|(\\bH\\b)|(\\bv\\b)|(\\bE\\b)|(\\bth\\b)|(\\bexp\\b)|(\\bFv\\b)|(\\bFs\\b)|(\\bQ\\b)|(\\bxv\\b)'\n",
    "    text = re.sub(group_of_characters_3, '', text)\n",
    "    #stray words\n",
    "    words = r'(kk)|(kknum)|(pp\\.)|(Rec@)|(Fig\\.)|(\\.v)|(\\sv\\s)|(\\sb\\s)|(Apr\\.)|(Feb\\.)|(Nov\\.)|(Inf\\.)|(CoRR)|(ACM\\, \\,)|(Vol\\.)|(No\\.)|(Surv\\.)'\n",
    "    text = re.sub(words, '', text)\n",
    "    #again [] () {} .% -, .+ \n",
    "    group_of_characters = r'(\\(\\))|(\\[\\])|(\\{\\})|(\\.%)|(, \\.)|(â€“,)|(\\-\\-)|(, \\:\\.)|(\\(\\.\\))|(\\(\\, \\))'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    # again ( ) [ ] [,][, ] -\n",
    "    group_of_characters = r'(\\( \\))|(\\[ \\])|(\\[,\\])|(\\[, \\])|(\\/)|( \\- )|(\\- )|( \\-)|( \\-\\,)|( \\-\\.)|(\\.\\- )'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    #final commas\n",
    "    commas = r'(\\s+\\, )'\n",
    "    text = re.sub(commas, ', ', text)\n",
    "    commas = r'(\\, \\,)'\n",
    "    text = re.sub(commas, ', ', text)\n",
    "    # commas at the end of the line\n",
    "    commas = r'(\\s*\\,\\s+\\n)|(\\s*\\,\\s*\\.\\s*\\n)|(\\\\\\.)|(\\.\\s*\\.)|(\\.)+|(\\. )+'\n",
    "    text = re.sub(commas, '.', text)\n",
    "    # stray dots (\\.){2}|\n",
    "    commas = r'(\\. ){2,25}'\n",
    "    text = re.sub(commas, '.', text)\n",
    "    commas = r'(\\. ){2,25}'\n",
    "    text = re.sub(commas, '.', text)\n",
    "    commas = r'(\\.){2,25}'\n",
    "    text = re.sub(commas, '.', text)\n",
    "    commas = r'(\\-){2,25}' #--\n",
    "    text = re.sub(commas, '', text)\n",
    "    commas = r'(\\.\\s+\\.)'  #.  .\n",
    "    text = re.sub(commas, '.', text)\n",
    "    commas = r'(\\:\\s+\\:)'  #:  :\n",
    "    text = re.sub(commas, ':', text)\n",
    "    commas = r'(\\,\\s+\\,)'  #,  ,\n",
    "    text = re.sub(commas, ',', text)\n",
    "    commas = r'(\\s*\\,{2,20}\\s*)'  # ,,    ,,,    ,, \n",
    "    text = re.sub(commas, ' ', text)\n",
    "    commas = r'(\\s+\\;*\\s*\\:\\s)'  #   ;  :  \n",
    "    text = re.sub(commas, ' ', text)\n",
    "    commas = r'[^\\S\\r\\n]{2,30}'  # 2+ spaces, no new line\n",
    "    text = re.sub(commas, ' ', text)\n",
    "    # try again - remove numbers\n",
    "    numbers = r'[0-9]'\n",
    "    text = re.sub(numbers, '', text)\n",
    "    # Academic stopwords\n",
    "    words = r'(Furthermore)|(Moreover)|(However)|(What)|(Overall)'\n",
    "    text = re.sub(words, '', text)\n",
    "    #arxiv coref hack: we shows up as topic; match only full words\n",
    "    words = r'(\\bWe\\b)'\n",
    "    text = re.sub(words, 'Authors', text)\n",
    "    words = r'(\\bwe\\b)'\n",
    "    text = re.sub(words, 'authors', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_file(filename, content):\n",
    "    pathlib.Path(TXT_BASE+filename).write_bytes(content.encode('utf-8').strip())\n",
    "\n",
    "def write_json_file(filename, content):\n",
    "    pathlib.Path(JSON_BASE+filename).write_bytes(content.encode('utf-8').strip())\n",
    "\n",
    "def save_cleaned_file(document):\n",
    "    filename = document['title']+'.json'\n",
    "    filename_txt = document['title']+'.txt'\n",
    "    json_object = json.dumps(document) \n",
    "    write_json_file(filename,json_object)\n",
    "    write_text_file(filename_txt,document['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for json_file in json_files:\n",
    "    file_name = join(JSON_RAW_BASE, json_file)\n",
    "    doc_string = read_text_file(file_name) \n",
    "    doc_string = doc_string.decode(encoding = 'utf-8')\n",
    "    #print(doc_string)\n",
    "    document = json.loads(doc_string)\n",
    "    #print(document)\n",
    "    text = document['text']\n",
    "    #print(text)\n",
    "    document[\"text\"] = cleanup_text(text)\n",
    "    save_cleaned_file(document)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
