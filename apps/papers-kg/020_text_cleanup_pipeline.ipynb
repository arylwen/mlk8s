{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pathlib\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = 'ArxivHealthcareNLP'\n",
    "#CORPUS = 'arxiv_cl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_properties(filepath, sep='=', comment_char='#'):\n",
    "    '''\n",
    "    Read the file passed as parameter as a properties file.\n",
    "    '''\n",
    "    props = {}\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            if l and not l.startswith(comment_char):\n",
    "                key_value = l.split(sep)\n",
    "                key = key_value[0].strip()\n",
    "                value = sep.join(key_value[1:]).strip().strip('\"') \n",
    "                props[key] = value \n",
    "    return props\n",
    "\n",
    "corpus_properties = load_properties(f\"corpora/{CORPUS}.properties\")\n",
    "corpus_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_BASE = corpus_properties['corpus_base']\n",
    "JSON_RAW_BASE = f'{CORPUS_BASE}/json_raw/'\n",
    "TXT_BASE = f'{CORPUS_BASE}/text_cleaned/'\n",
    "JSON_BASE = f'{CORPUS_BASE}/json_cleaned/'\n",
    "\n",
    "if not os.path.exists(JSON_BASE):\n",
    "    print(f'{JSON_BASE} does not exist. Creating.')\n",
    "    os.makedirs(JSON_BASE)\n",
    "\n",
    "if not os.path.exists(TXT_BASE):\n",
    "    print(f'{TXT_BASE} does not exist. Creating.')\n",
    "    os.makedirs(TXT_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "json_files = [f for f in listdir(JSON_RAW_BASE) if isfile(join(JSON_RAW_BASE, f))]\n",
    "len(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    json_content = pathlib.Path(filename).read_bytes()\n",
    "    print(len(json_content))\n",
    "    return json_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(raw_text):\n",
    "    text = raw_text\n",
    "\n",
    "    # convert split words on line break, e.g. post-\\nediting\n",
    "    text = text.replace('-\\n', '')\n",
    "    # remove lone new lines in the middle of the sentence - leave only the new lines after .(dot)\n",
    "    one_new_line = r'(?<![\\.\\n])\\n(?!\\n)'\n",
    "    text = re.sub(one_new_line, ' ', text)\n",
    "    # encoded sequences will always generate sequences larger than the chunk size\n",
    "    latexit = r'<latexit.*latexit> *'\n",
    "    text = re.sub(latexit, ' ', text)\n",
    "    # remove urls - not needed for KG\n",
    "    text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "    # remove email addresses - not needed for KG\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    # remove references\n",
    "    text = re.sub(r'doi\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'abs\\/.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'URL\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'url\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'arXiv\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    # remove everythng between parathesis\n",
    "    text = re.sub(r'\\(([^)]+)\\)', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\[([^]]+)\\]', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\{([^}]+)\\}', '', text, flags=re.MULTILINE)   \n",
    "    # remove Figure captions:\n",
    "    text = re.sub(r'(Figure [0-9]*:*)', '', text)\n",
    "    # remove Table captions:\n",
    "    text = re.sub(r'(Table [0-9]*:*)', '', text)\n",
    "    # remove numbers\n",
    "    numbers = r'[0-9]'\n",
    "    text = re.sub(numbers, '', text)\n",
    "    # lists\n",
    "    lists = r'(i\\))|(ii\\))|(iii\\))|(xvii)|(xviii)'\n",
    "    text = re.sub(lists, '', text)\n",
    "    lists = r'(I\\))|(II\\))|(III\\.)|(XVII)|(XVIII)'\n",
    "    text = re.sub(lists, '', text)\n",
    "    #[] () .% -, .+ at the beginning of the line TODO |( \\. )\n",
    "    group_of_characters = r'(\\(\\))|(\\[\\])|(\\.%)|(, \\.)|(–,)|(\\-\\-)|(, \\:\\.)'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    #( ) [ ] [,][, ]\n",
    "    group_of_characters = r'(\\( \\))|(\\[ \\])|(\\[,\\])|(\\[, \\])'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    group_of_characters = r'^(\\.+\\s)|^(,+\\s)|^\\.'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    #stray abbreviations\n",
    "    words = r'(vol\\.)|(no\\.)|(pp\\.)|(Rec@)|(Fig\\.)|(\\b\\.v\\b)|(\\sv\\s)|(\\sb\\s)|(Aug\\.)|(Jan)|(Nov\\.)'\n",
    "    text = re.sub(words, '', text)\n",
    "    #stray commas TODO - ( \\, )|\n",
    "    group_of_characters_2 = r'(,(\\s*)\\n)'\n",
    "    text = re.sub(group_of_characters_2, '', text)\n",
    "    #stray characters\n",
    "    special_characters = r'[–♣♥♠♦•�±𝛹𝐴−𝑦✓⟨⟩ℎ𝑥\"𝜃…𝐷𝑋𝑣𝑥\"𝑐𝒆\"θδ∈×𝑟𝑡𝑨𝒗∗𝒙𝐶𝐿𝑆𝐵𝑀𝑆𝐾𝑙𝑖𝑘𝑒𝑠∀↑↓𝑑→†𝑎𝑔𝑛⊤√π♢♡µλ𝑚𝑝𝑃ε𝑧𝔄𝜋◦𝐻𝑇ℕ∅∑︁⊂∥′✗⇒¬∧▽𝑯𝑻↔ℭ↦𝟏𝐝𝑫]'\n",
    "    text = re.sub(special_characters, '', text)\n",
    "    #leave '-' as part of e.g. bert \n",
    "    special_characters = r'[\\&%=_\\ˆ≥+|˜!#$<>—≤¯𝒊𝜽𝝋𝒓⊥∥Φ≠∞𝚺𝜻∼σβ·]'\n",
    "    text = re.sub(special_characters, '', text)\n",
    "    special_characters = r'[鹏城实验室鹏城实验室推出面向中文医疗文本处理的预训练模型阻塞性睡眠呼吸暂停下述哪一项不符合SLE血液系统改变回答选项选项A血小板减少选项B 自细胞减少푆去尿过푆]'\n",
    "   text = re.sub(special_characters, '', text)\n",
    "    special_characters = r'[选项C自身免疫溶血贫血选项D正色素细胞贫血选项E类白血病样改变SLE是一种自身免疫疾病其血液系统改变包括血小板减少自身免疫溶血贫血正色素细胞贫血等而类白血病样改变是指骨髓现大量幼稚细胞与SLE无关因此选项E不符合SLE血液系统改变]'\n",
    "    text = re.sub(special_characters, '', text)\n",
    "    #fi  \n",
    "    special_characters = r'[\\ufb01]'\n",
    "    text = re.sub(special_characters, 'fi', text)\n",
    "    #ffi \n",
    "    special_characters = r'[\\ufb03]'\n",
    "    text = re.sub(special_characters, 'ffi', text)\n",
    "    #empty lines\n",
    "    empty_line = r'\\.\\n(\\.+)'\n",
    "    text = re.sub(empty_line, '\\.\\n', text)\n",
    "    #paragraph names, e.g. A., B., C. - llms are picking them up as topics\n",
    "    paragraph_names = r'\\n(\\s*)[a-zA-Z]\\.'\n",
    "    text = re.sub(paragraph_names, '\\n', text)\n",
    "    #try again to remove numbers\n",
    "    numbers = r'[0-9]'\n",
    "    text = re.sub(numbers, '', text)\n",
    "    #stray letters\n",
    "    group_of_characters_3 = r'(\\bD\\b)|( B )|( o )|(\\bs\\b)|( Xn )|(\\bX\\b)|(\\by\\b)|( m )|(\\bi\\b)|( c )|(\\br\\b)|(\\bk\\b)|(\\bd\\b)|(\\bt\\b)|(\\bvt\\b)|(\\bxn\\b)|(\\bXn\\b)|(\\be\\b)|(\\bL\\b)'\n",
    "    text = re.sub(group_of_characters_3, '', text)\n",
    "    group_of_characters_3 = r'( m )|( b )|(\\bx\\b)|(\\bS\\b)|( F )|( g )|(\\bC\\b)|( Z )|( z )|( Xu )|( R )|( \\/ )|( w )|( U= )|( V )|( M )|(dx)|(\\bxt\\b)|(\\bTn\\b)|(\\btn\\b)'\n",
    "    text = re.sub(group_of_characters_3, '', text)\n",
    "    group_of_characters_3 = r'(\\bT\\b)|(\\bP\\b)|(\\bMC\\b)|(\\b.-\\b)|(\\b-.\\b)|(\\bK\\b)|(\\bp\\b)|(\\bl\\b)|(\\b-.\\b)|( Xu )|( R )|(\\b\\/\\b)|( w )|( U= )|( V )|( M )|(dx)|(\\bxt\\b)|(\\bTn\\b)'\n",
    "    text = re.sub(group_of_characters_3, '', text)\n",
    "    group_of_characters_3 = r'(\\bJ\\.\\b)|(\\bM\\.\\b)|(\\bA\\.\\b)|(\\bH\\b)|(\\bv\\b)|(\\bE\\b)|(\\bth\\b)|(\\bexp\\b)|(\\bFv\\b)|(\\bFs\\b)|(\\bQ\\b)|(\\bxv\\b)'\n",
    "    text = re.sub(group_of_characters_3, '', text)\n",
    "    #stray words\n",
    "    words = r'(kk)|(kknum)|(pp\\.)|(Rec@)|(Fig\\.)|(\\.v)|(\\sv\\s)|(\\sb\\s)|(Apr\\.)|(Feb\\.)|(Nov\\.)|(Inf\\.)|(CoRR)|(ACM\\, \\,)|(Vol\\.)|(No\\.)|(Surv\\.)'\n",
    "    text = re.sub(words, '', text)\n",
    "    #again [] () {} .% -, .+ \n",
    "    group_of_characters = r'(\\(\\))|(\\[\\])|(\\{\\})|(\\.%)|(, \\.)|(–,)|(\\-\\-)|(, \\:\\.)|(\\(\\.\\))|(\\(\\, \\))'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    # again ( ) [ ] [,][, ] -\n",
    "    group_of_characters = r'(\\( \\))|(\\[ \\])|(\\[,\\])|(\\[, \\])|(\\/)|( \\- )|(\\- )|( \\-)|( \\-\\,)|( \\-\\.)|(\\.\\- )'\n",
    "    text = re.sub(group_of_characters, '', text)\n",
    "    #final commas\n",
    "    commas = r'(\\s+\\, )'\n",
    "    text = re.sub(commas, ', ', text)\n",
    "    commas = r'(\\, \\,)'\n",
    "    text = re.sub(commas, ', ', text)\n",
    "    # commas at the end of the line\n",
    "    commas = r'(\\s*\\,\\s+\\n)|(\\s*\\,\\s*\\.\\s*\\n)|(\\\\\\.)|(\\.\\s*\\.)|(\\.)+|(\\. )+'\n",
    "    text = re.sub(commas, '.', text)\n",
    "    # stray dots (\\.){2}|\n",
    "    commas = r'(\\. ){2,25}'\n",
    "    text = re.sub(commas, '.', text)\n",
    "    commas = r'(\\. ){2,25}'\n",
    "    text = re.sub(commas, '.', text)\n",
    "    commas = r'(\\.){2,25}'\n",
    "    text = re.sub(commas, '.', text)\n",
    "    commas = r'(\\-){2,25}' #--\n",
    "    text = re.sub(commas, '', text)\n",
    "    commas = r'(\\.\\s+\\.)'  #.  .\n",
    "    text = re.sub(commas, '.', text)\n",
    "    commas = r'(\\:\\s+\\:)'  #:  :\n",
    "    text = re.sub(commas, ':', text)\n",
    "    commas = r'(\\,\\s+\\,)'  #,  ,\n",
    "    text = re.sub(commas, ',', text)\n",
    "    commas = r'(\\s*\\,{2,20}\\s*)'  # ,,    ,,,    ,, \n",
    "    text = re.sub(commas, ' ', text)\n",
    "    commas = r'(\\s+\\;*\\s*\\:\\s)'  #   ;  :  \n",
    "    text = re.sub(commas, ' ', text)\n",
    "    commas = r'[^\\S\\r\\n]{2,30}'  # 2+ spaces, no new line\n",
    "    text = re.sub(commas, ' ', text)\n",
    "    # try again - remove numbers\n",
    "    numbers = r'[0-9]'\n",
    "    text = re.sub(numbers, '', text)\n",
    "    # Academic stopwords\n",
    "    words = r'(Furthermore)|(Moreover)|(However)|(What)|(Overall)'\n",
    "    text = re.sub(words, '', text)\n",
    "    #arxiv coref hack: we shows up as topic; match only full words\n",
    "    words = r'(\\bWe\\b)'\n",
    "    text = re.sub(words, 'Authors', text)\n",
    "    words = r'(\\bwe\\b)'\n",
    "    text = re.sub(words, 'authors', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_file(filename, content):\n",
    "    pathlib.Path(TXT_BASE+filename).write_bytes(content.encode('utf-8').strip())\n",
    "\n",
    "def write_json_file(filename, content):\n",
    "    pathlib.Path(JSON_BASE+filename).write_bytes(content.encode('utf-8').strip())\n",
    "\n",
    "def save_cleaned_file(document):\n",
    "    filename = document['title']+'.json'\n",
    "    filename_txt = document['title']+'.txt'\n",
    "    json_object = json.dumps(document) \n",
    "    write_json_file(filename,json_object)\n",
    "    write_text_file(filename_txt,document['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for json_file in json_files:\n",
    "    file_name = join(JSON_RAW_BASE, json_file)\n",
    "    doc_string = read_text_file(file_name) \n",
    "    doc_string = doc_string.decode(encoding = 'utf-8')\n",
    "    #print(doc_string)\n",
    "    document = json.loads(doc_string)\n",
    "    #print(document)\n",
    "    text = document['text']\n",
    "    #print(text)\n",
    "    document[\"text\"] = cleanup_text(text)\n",
    "    save_cleaned_file(document)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
