{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pathlib\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n",
    "#execute before first launch\n",
    "#python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = 'ArxivHealthcareNLP'\n",
    "#CORPUS = 'arxiv_cl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_properties(filepath, sep='=', comment_char='#'):\n",
    "    '''\n",
    "    Read the file passed as parameter as a properties file.\n",
    "    '''\n",
    "    props = {}\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            if l and not l.startswith(comment_char):\n",
    "                key_value = l.split(sep)\n",
    "                key = key_value[0].strip()\n",
    "                value = sep.join(key_value[1:]).strip().strip('\"') \n",
    "                props[key] = value \n",
    "    return props\n",
    "\n",
    "corpus_properties = load_properties(f\"corpora/{CORPUS}.properties\")\n",
    "corpus_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_BASE = corpus_properties['corpus_base']\n",
    "TXT_BASE = f'{CORPUS_BASE}/text_cleaned/'\n",
    "TXT_COREF_BASE = f'{CORPUS_BASE}/text_coref_resolved/'\n",
    "\n",
    "if not os.path.exists(TXT_COREF_BASE):\n",
    "    print(f'{TXT_COREF_BASE} does not exist. Creating.')\n",
    "    os.makedirs(TXT_COREF_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "txt_files = [f for f in listdir(TXT_BASE) if isfile(join(TXT_BASE, f))]\n",
    "len(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    txt_content = pathlib.Path(filename).read_bytes()\n",
    "    txt_content = txt_content.decode(\"utf-8\")\n",
    "    print(f'File length: {len(txt_content)}')\n",
    "    return txt_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "from fastcoref import spacy_component\n",
    "import spacy\n",
    "\n",
    "def resolve_corefs(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\n",
    "        \"fastcoref\", \n",
    "        config={'model_architecture': 'LingMessCoref', \n",
    "                'model_path': 'biu-nlp/lingmess-coref', \n",
    "                'device': 'cpu'}\n",
    "    )\n",
    "    doc = nlp(text, component_cfg={\"fastcoref\": {'resolve_text': True}})\n",
    "    #print(doc._.coref_clusters)\n",
    "    txt_resolved = doc._.resolved_text\n",
    "    return txt_resolved, doc._.coref_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_file(filename, content):\n",
    "    pathlib.Path(filename).write_bytes(content.encode('utf-8').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastcoref max_doc_len is 4096\n",
    "# TODO - split a large file in a memory effecive way; add overlap for context\n",
    "# split texts in 4096 or less; 3200 tokens x 5chars/token\n",
    "MAX_SLICE_LEN = 12000 # tokens are smaller than words\n",
    "\n",
    "def split_large_paragraphs(text):\n",
    "    texts = text.split('.')\n",
    "    print(f'Sentences: {len(texts)}')\n",
    "    slices = []\n",
    "    slice = ''\n",
    "    for txt in texts:\n",
    "        if len(slice) + len(txt) < MAX_SLICE_LEN:\n",
    "            slice = slice + '.' + txt\n",
    "        else:\n",
    "            slices.extend([slice])\n",
    "            slice = ''\n",
    "    # add last pending slice        \n",
    "    slices.append(slice)\n",
    "\n",
    "    return slices\n",
    "\n",
    "def split_large_file(text):\n",
    "    texts = text.split('\\n')\n",
    "    print(f'Paragraphs: {len(texts)}')\n",
    "    slices = []\n",
    "    slice = ''\n",
    "    for txt in texts:\n",
    "        if(len(txt) < MAX_SLICE_LEN):\n",
    "            if len(slice) + len(txt) < MAX_SLICE_LEN:\n",
    "                slice = slice + '\\n' + txt\n",
    "            else:\n",
    "                slices.extend([slice])\n",
    "                slice = ''\n",
    "        else:\n",
    "            if len(slice) > 0:   \n",
    "                slices.extend([slice])\n",
    "                slice = ''\n",
    "            # large paragraphs\n",
    "            p_slices = split_large_paragraphs(text)\n",
    "            print(f'large paragraphs: {len(p_slices)} slices')\n",
    "            slices.extend(p_slices)\n",
    " \n",
    "    # add last pending slice    \n",
    "    if len(slice) > 0:    \n",
    "        slices.append(slice)\n",
    "\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for txt_file in txt_files:\n",
    "    resolved_file_name = join(TXT_COREF_BASE, txt_file)\n",
    "    if os.path.exists(resolved_file_name):\n",
    "        print(f'Skipping exiting resolved file: {resolved_file_name}')\n",
    "    else:\n",
    "        # read cleaned file\n",
    "        file_name = join(TXT_BASE, txt_file)\n",
    "        print(f'Processing file: {resolved_file_name}')\n",
    "        txt = read_text_file(file_name) \n",
    "        #print(txt)\n",
    "        splits = split_large_file(txt)\n",
    "        print(f'Splits: {len(splits)}')\n",
    "        resolved_txt = ''\n",
    "        for split in splits: \n",
    "            try:\n",
    "                # resolve corefs\n",
    "                resolved_split, coref_clusters = resolve_corefs(split)\n",
    "                print(f'Found {len(coref_clusters)} coref clusters.')\n",
    "                resolved_txt = resolved_txt + '\\n' + resolved_split\n",
    "            except IndexError as e:\n",
    "                print(f'Error processig split. Adding unchanged. \\n{split}')\n",
    "                # mostly references\n",
    "                resolved_txt = resolved_txt + '\\n' + split\n",
    "        # write the file with the resolved corefs\n",
    "        write_text_file(resolved_file_name, resolved_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
