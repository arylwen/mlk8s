{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file_utf(filename):\n",
    "    text_bytes = pathlib.Path(filename).read_bytes()\n",
    "    print(len(text_bytes))\n",
    "    text_bytes = text_bytes.decode(encoding = 'utf-8')\n",
    "    return text_bytes\n",
    "\n",
    "def read_text_file_ascii(filename):\n",
    "    text = pathlib.Path(filename).read_bytes().decode(encoding = 'utf-8')\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    return text\n",
    "\n",
    "def write_text_file_utf(filename, content):\n",
    "    pathlib.Path(filename).write_bytes(content.encode('utf-8').strip())\n",
    "\n",
    "def write_text_file_ascii(filename, content):\n",
    "    pathlib.Path(filename).write_bytes(content.encode('ascii').strip())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    - split words\n",
    "    - latex encoded sequences\n",
    "    - utf specials as fi, ffi\n",
    "'''\n",
    "def utf_cleanup(raw_text):\n",
    "    text = raw_text\n",
    "    #----------------- general text cleanup ---------------------------\n",
    "    # convert split words on line break, e.g. post-\\nediting\n",
    "    text = text.replace('-\\n', '')\n",
    "    # encoded sequences will always generate sequences larger than the chunk size\n",
    "    latexit = r'<latexit.*latexit> *'\n",
    "    text = re.sub(latexit, ' ', text)\n",
    "    # sepcial characters part of words\n",
    "    #fi  \n",
    "    special_characters = r'[\\ufb01]'\n",
    "    text = re.sub(special_characters, 'fi', text)\n",
    "    #ffi \n",
    "    special_characters = r'[\\ufb03]'\n",
    "    text = re.sub(special_characters, 'ffi', text)\n",
    "    #ﬀ\n",
    "    special_characters = r'[ﬀ]'\n",
    "    text = re.sub(special_characters, 'ff', text)\n",
    "    #ﬂ\n",
    "    special_characters = r'[ﬂ]'\n",
    "    text = re.sub(special_characters, 'fl', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(raw_text):\n",
    "    text = raw_text\n",
    "\n",
    "    #----------------- general text cleanup ---------------------------\n",
    "    # convert split words on line break, e.g. post-\\nediting\n",
    "    #text = text.replace('-\\n', '')\n",
    "\n",
    "    # encoded sequences will always generate sequences larger than the chunk size\n",
    "    #latexit = r'<latexit.*latexit> *'\n",
    "    #text = re.sub(latexit, ' ', text)\n",
    "    # remove numbers\n",
    "    #numbers = r'[0-9]'\n",
    "    numbers = r'\\b\\d+\\b'\n",
    "    text = re.sub(numbers, '', text)\n",
    "    # remove urls - not needed for KG\n",
    "    text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "    # remove email addresses - not needed for KG\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    # remove lines less than 20 characters  ----too wide net: ^.{1,20}$\n",
    "    text = re.sub(r'^[\\w\\-\\s():<>?,]{1,45}$', '', text, flags=re.MULTILINE)\n",
    "    # remove single dots\n",
    "    text = re.sub(r'^[.]$', '', text, flags=re.MULTILINE)\n",
    "    # remove words longer than 20 characters, but not paragraph lines (ending with .) \n",
    "    text = re.sub(r'\\b\\w{20,}\\b', '', text)    \n",
    "    # remove empty lines\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)    \n",
    "\n",
    "    #--------------- arxiv specific text cleanup ----------------------------------\n",
    "    # remove references\n",
    "    text = re.sub(r'doi\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'abs\\/.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'URL\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'url\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'arXiv\\:.*\\n?', '\\n', text, flags=re.MULTILINE)\n",
    "    # remove Figure captions:\n",
    "    text = re.sub(r'(Figure [0-9]*:*)', ' ', text)\n",
    "    # remove Table captions:\n",
    "    text = re.sub(r'(Table [0-9]*:*)', ' ', text)\n",
    "    # no code and math ,.- have special treatment\n",
    "    special_characters = r'[\\!\\\"\\#\\$\\%\\&\\*\\+\\/\\:\\;\\<\\=\\>\\?\\\\\\^\\_\\|\\(\\)\\[\\]\\{\\}]'\n",
    "    text = re.sub(special_characters, ' ', text)\n",
    "    # single-letter-comma\n",
    "    special_characters = r'\\b(\\w,)'\n",
    "    text = re.sub(special_characters, ' ', text)\n",
    "     # multiple commas\n",
    "    special_characters = r'(,{2,} {0,})|(, {1,})'\n",
    "    text = re.sub(special_characters, ',', text)\n",
    "    special_characters = r'(,{2,} {0,})|(, {1,})'\n",
    "    text = re.sub(special_characters, ',', text)\n",
    "    # single-letter-space\n",
    "    special_characters = r'\\b(\\w\\s)'\n",
    "    text = re.sub(special_characters, ' ', text)\n",
    "    # single-letter-space\n",
    "    special_characters = r'\\b(\\w\\.)'\n",
    "    text = re.sub(special_characters, '.', text)\n",
    "    # dagling dashes\n",
    "    #special_characters = r'(\\ \\-\\b)|(\\b- )|( - )'\n",
    "    #text = re.sub(special_characters, ',', text)\n",
    "    # multiple spaces\n",
    "    special_characters = r'( {2,})'\n",
    "    text = re.sub(special_characters, ' ', text)\n",
    "    # multiple .\n",
    "    special_characters = r'(\\.{2,})'\n",
    "    text = re.sub(special_characters, '.', text)\n",
    "\n",
    "    # remove lone new lines in the middle of the sentence - leave only the new lines after .(dot)\n",
    "    one_new_line = r'(?<![\\.\\n])\\n(?!\\n)'\n",
    "    text = re.sub(one_new_line, ' ', text)\n",
    "\n",
    "    # dagling dashes - after lone new lines to not lose compose words\n",
    "    special_characters = r'(\\ \\-\\b)|(\\b- )|( - )'\n",
    "    text = re.sub(special_characters, ',', text)\n",
    "\n",
    "    # CC AA BB GCRE2 SFR\n",
    "    text = re.sub(r'(\\b[A-Z]{1,3}[0-9]*[A-Z]{0,3}\\b\\s){3,}', ' ', text)\n",
    "\n",
    "    #stray punctuation\n",
    "    text = re.sub(r'(\\, {1,}\\,?\\.)', ' ', text)\n",
    "\n",
    "    # stray character at the begining of the line\n",
    "    special_characters = r'(\\n\\. )|(\\n\\- )|(\\n {1,})'\n",
    "    text = re.sub(special_characters, '\\n', text)\n",
    " \n",
    "    text = re.sub(r'(\\,\\.)', '.', text)\n",
    "    text = re.sub(r'(\\. ?){1,}', '.', text)\n",
    "\n",
    "    # multiple spaces\n",
    "    special_characters = r'( {2,})'\n",
    "    text = re.sub(special_characters, ' ', text)\n",
    "\n",
    "    #multiple '. '\n",
    "    text = re.sub(r'(\\. ){2,}', ' ', text)\n",
    "\n",
    "    #lone dots on a line '. '\n",
    "    text = re.sub(r'\\n *\\. *\\n', '\\n', text)\n",
    "\n",
    "    #stray punctuation\n",
    "    text = re.sub(r'(\\-\\.)', '', text)\n",
    "\n",
    "    # stray character at the begining of the line\n",
    "    special_characters = r'(\\n\\. )|(\\n\\- )|(\\n {1,})'\n",
    "    text = re.sub(special_characters, '\\n', text)\n",
    "\n",
    "    #math\n",
    "    special_characters = r'(\\bdx\\b)|(\\bdy\\b)|(\\bdx2\\b)'\n",
    "    text = re.sub(special_characters, '', text)\n",
    "\n",
    "    #things\n",
    "    special_characters = r'(\\.and\\b)'\n",
    "    text = re.sub(special_characters, 'and', text)\n",
    "    text = re.sub(r'CoRR', '', text)\n",
    "    text = re.sub(r'URL', '', text)\n",
    "\n",
    "    #too many commas\n",
    "    text = re.sub(r'( ,){2,}', ', ', text)\n",
    "    #space befoe dot\n",
    "    text = re.sub(r'( \\.)', '.', text)\n",
    "\n",
    "    # remove lines less than 40 characters; keep sentences \n",
    "    text = re.sub(r'^[\\w\\-\\s():<>?,]{1,40}$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    #lone dots on a line - good paragraph breaks?\n",
    "    text = re.sub(r'(\\n *\\.+ *)', '\\n\\n', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Academic stopwords\n",
    "    words = r'(Furthermore)|(Moreover)|(However)|(What)|(Overall)|(Nonetheless)|(Although)|(particularly)|(Essentially)'\n",
    "    text = re.sub(words, '', text)\n",
    "    words = r'(Recently)|(Particularly)|(Additionally)|(Finally)|(Now)|(Both)|(Secondly)|(In general)'\n",
    "    text = re.sub(words, '', text)\n",
    "    words = r'(\\bOur\\b)|(\\bThe\\b)|(\\bthe\\b)|(\\balso\\b)|(wo cond\\.)|(arXiv preprint)|(IEEE\\.)|(arXiv)'\n",
    "    text = re.sub(words, '', text)\n",
    "    # stray character at the begining of the line after removing academic stopwords\n",
    "    special_characters = r'(\\n\\. )|(\\n\\- )|(\\n {1,})'\n",
    "    text = re.sub(special_characters, '\\n', text)\n",
    "    # multiple spaces\n",
    "    special_characters = r'( {2,})'\n",
    "    text = re.sub(special_characters, ' ', text)\n",
    "    #arxiv coref hack: we shows up as topic; match only full words\n",
    "    words = r'(\\bWe\\b)'\n",
    "    text = re.sub(words, 'Authors', text)\n",
    "    words = r'(\\bwe\\b)'\n",
    "    text = re.sub(words, 'authors', text)\n",
    "  \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172292\n"
     ]
    }
   ],
   "source": [
    "filename = 'samples/galactica.txt'\n",
    "dirname = os.path.dirname(filename)\n",
    "basename = os.path.basename(filename)\n",
    "ascii_file_name = f'{dirname}/ascii_cleaned_{basename}'\n",
    "text = read_text_file_utf(filename)\n",
    "text = utf_cleanup(text)\n",
    "# convert to ascii - we could ignore the other non-ascii\n",
    "text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "# rest of cleanup\n",
    "text = cleanup_text(text)\n",
    "write_text_file_ascii(ascii_file_name, text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
