{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook uses arxiv metadata from https://www.kaggle.com/datasets/Cornell-University/arxiv?resource=download.\n",
    "Current version is 139. Check for a later version before running.\n",
    "'''\n",
    "#based on https://jrashford.com/2023/02/13/how-to-scrape-mastodon-timelines-using-python-and-pandas/\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORPUS = 'ArxivHealthcareNLP'\n",
    "#CORPUS = 'arxiv_cl'\n",
    "CORPUS = 'aiml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_properties(filepath, sep='=', comment_char='#'):\n",
    "    '''\n",
    "    Read the file passed as parameter as a properties file.\n",
    "    '''\n",
    "    props = {}\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            if l and not l.startswith(comment_char):\n",
    "                key_value = l.split(sep)\n",
    "                key = key_value[0].strip()\n",
    "                value = sep.join(key_value[1:]).strip().strip('\"') \n",
    "                props[key] = value \n",
    "    return props\n",
    "'''\n",
    "Save a dictionary as a properties file; use to remember the latest processed id.\n",
    "TODO store comments\n",
    "'''\n",
    "def save_properties(properties, filepath, sep='=', comment_char='#'):\n",
    "    with open(filepath, 'w') as f: \n",
    "        for key, value in properties.items(): \n",
    "            f.write('%s %s %s\\n' % (key, sep, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_properties = load_properties(f\"corpora/{CORPUS}.properties\")\n",
    "corpus_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT = corpus_properties['account']\n",
    "LATEST = int(corpus_properties['latest'])\n",
    "CORPUS_BASE = corpus_properties['corpus_base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_BASE = f'{CORPUS_BASE}/pdf'\n",
    "if not os.path.exists(PDF_BASE):\n",
    "    print(f'{PDF_BASE} does not exist. Creating.')\n",
    "    os.makedirs(PDF_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_lookup(acct):\n",
    "    URL = f'https://mastodon.social/api/v1/accounts/lookup'\n",
    "    params = {\n",
    "        'acct': acct\n",
    "    }\n",
    "\n",
    "    r = requests.get(URL, params=params)\n",
    "    user = json.loads(r.text)\n",
    "    \n",
    "    return user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = user_lookup(acct=ACCOUNT)\n",
    "user_id = user['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = f'https://mastodon.social/api/v1/accounts/{user_id}/statuses'\n",
    "params = {\n",
    "    'limit': 40,\n",
    "    'since_id':  LATEST\n",
    "#    'min_id':  LATEST\n",
    "}\n",
    "\n",
    "results = []\n",
    "LATEST = 0\n",
    "\n",
    "while True:\n",
    "    print(params)\n",
    "    r = requests.get(URL, params=params)\n",
    "    toots = json.loads(r.text)\n",
    "\n",
    "    if len(toots) == 0:\n",
    "        break\n",
    "    \n",
    "    results.extend(toots)\n",
    "    \n",
    "    max_id = toots[-1]['id']\n",
    "    params['max_id'] = max_id\n",
    "    if(LATEST == 0):\n",
    "        # remember the highest toot id processed\n",
    "        LATEST = toots[0][\"id\"]\n",
    "    print(f'first:{toots[0][\"id\"]} last:{max_id}')\n",
    "    \n",
    "df = pd.DataFrame(results)\n",
    "print(f'Latest: {LATEST}; Total new toots: {df.shape[0]}')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(df.shape[0] == 0):\n",
    "    raise ValueError(\"No new toots to process. Stopping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some toots are reblogs; we would need to bring their content to the content column for normalization.\n",
    "'''\n",
    "def update_content(row):\n",
    "    #print(row)\n",
    "    reblog_dict = row['reblog']\n",
    "    if reblog_dict and ('content' in reblog_dict):\n",
    "        row['content'] += reblog_dict['content']\n",
    "    return row\n",
    "\n",
    "df = df.apply(lambda row: update_content(row), axis = 1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = r'(https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]*)' \n",
    "# some toots do not have the protocol or www; for these there must be at least 2 '/' to match the arxiv pattern\n",
    "# this picks up the first 2 links although it matches all links\n",
    "# pattern = r'((https?:\\/\\/(?:www\\.)?)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]{2,256})'\n",
    "# restrict to arxiv articles with creative versioning\n",
    "pattern = r'((https?:\\/\\/(?:www\\.)?)?(arxiv)\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]{2,256})|[0-9]{4,4}\\.[0-9]+[a-zA-Z]*[0-9]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['links'] = df[\"content\"].str.extract(pattern, expand=True)[0]\n",
    "# TODO - are there several different arxiv articles in the same toot?\n",
    "#df = df.join(df[\"content\"].str.extract(pattern, expand=True))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract article id to use for download\n",
    "ARXIV_PREFIX_1 = \"https://arxiv.org/\"\n",
    "ARXIV_PREFIX_2 = \"arxiv.org/\"\n",
    "ARXIV_PREFIX_3 = \"arXiv\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_article_id(col_str):\n",
    "    #print(col_str)\n",
    "    if col_str is np.nan:\n",
    "         print(f'Not an arxiv article: {col_str}')\n",
    "         return\n",
    "    if col_str.startswith(ARXIV_PREFIX_1) | col_str.startswith(ARXIV_PREFIX_2):\n",
    "        #an arxiv article\n",
    "        article_id = col_str.split('/')[-1]\n",
    "        #some articles have an extension\n",
    "        article_id = '.'.join(article_id.split('.')[:2])\n",
    "        articles = re.findall(r'[0-9]{4,4}\\.[0-9]+[a-zA-Z]*[0-9]*', article_id)\n",
    "        #print(articles)\n",
    "        article_id = None\n",
    "        if(len(articles) > 0):\n",
    "            article_id = articles[0]\n",
    "        else:\n",
    "            print(f'{col_str} not an arxiv article.')\n",
    "        return article_id\n",
    "    elif col_str.startswith(ARXIV_PREFIX_3):\n",
    "        #an arxiv article\n",
    "        article_id = col_str.split(':')[-1]\n",
    "        #some article have an extension\n",
    "        article_id = '.'.join(article_id.split('.')[:2])\n",
    "        return article_id\n",
    "    elif re.search(r'[0-9]{4,4}\\.[0-9]+[a-zA-Z]*[0-9]*', col_str):\n",
    "        articles = re.findall(r'[0-9]{4,4}\\.[0-9]+[a-zA-Z]*[0-9]*', col_str)\n",
    "        #print(articles)\n",
    "        article_id = None\n",
    "        if(len(articles) > 0):\n",
    "            article_id = articles[0]\n",
    "        else:\n",
    "            print(f'{col_str} not an arxiv article.')\n",
    "        return article_id\n",
    "    else:\n",
    "        print(f'Not an arxiv article: {col_str}')\n",
    "\n",
    "df['article_id'] = df['links'].apply(get_article_id)\n",
    "\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.article_id.duplicated()]['article_id']\n",
    "duplicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_files = [f for f in listdir(PDF_BASE) if isfile(join(PDF_BASE, f))]\n",
    "len(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_article_ids = ['.'.join(f.split('.')[:2])[:10] for f in pdf_files]\n",
    "#downloaded_article_ids\n",
    "len(downloaded_article_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client.create_anonymous_client()\n",
    "bucket = client.bucket('arxiv-dataset')\n",
    "\n",
    "'''\n",
    "paper_id must contain the required version e.g. 2211.00350v3\n",
    "'''\n",
    "def google_cloud_download(paper_id, file_name):\n",
    "    # blob = bucket.blob(\"arxiv/arxiv/pdf/2211/2211.00350v3.pdf\")\n",
    "    year = paper_id.split('.')[0]\n",
    "    try:\n",
    "        #blob = bucket.blob(f\"arxiv/arxiv/pdf/{year}/{paper_id}v{vn}.pdf\")\n",
    "        blob = bucket.blob(f\"arxiv/arxiv/pdf/{year}/{paper_id}.pdf\")\n",
    "        blob.download_to_filename(file_name)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'Trying: {paper_id[:10]}v1')\n",
    "        # version declared but not available for download, try v1; \n",
    "        # TODO perhaps find the most recent available version\n",
    "        try:\n",
    "            blob = bucket.blob(f\"arxiv/arxiv/pdf/{year}/{paper_id[:10]}v1.pdf\")\n",
    "            blob.download_to_filename(file_name)\n",
    "        except Exception as e1:\n",
    "            print(e1)\n",
    "            #try without the version\n",
    "            try:\n",
    "                blob = bucket.blob(f\"arxiv/arxiv/pdf/{year}/{paper_id[:10]}.pdf\")\n",
    "                blob.download_to_filename(file_name)\n",
    "            except Exception as e2:\n",
    "                print(e2)\n",
    "\n",
    "#google_cloud_download('2211.00350v3', 'test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 0\n",
    "metadata_records = []\n",
    "with open(\"arxiv-metadata-oai-snapshot.json\") as f1:\n",
    "    for line in f1:\n",
    "        #print(line)   \n",
    "        metadata_record = json.loads(line)\n",
    "        #print(metadata_record)\n",
    "        metadata_records.extend([metadata_record])\n",
    "        #nl+=1\n",
    "        #if (nl == 5): break\n",
    "\n",
    "#print(metadata_records)\n",
    "metadata_df = pd.DataFrame(metadata_records)\n",
    "metadata_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for paper in the metadata_df\n",
    "def kaggle_search(paper_id):\n",
    "    row = metadata_df.loc[metadata_df['id'] == paper_id]\n",
    "    #print(row)\n",
    "    paper = None\n",
    "    try:\n",
    "        paper = {}\n",
    "        paper['id'] = row['id'].values[0]\n",
    "        paper['title'] = row['title'].values[0]\n",
    "        paper['versions'] = row['versions'].values[0]\n",
    "        paper['abstract'] = row['abstract'].values[0]\n",
    "\n",
    "        latest_version = 'v1'\n",
    "        for version in paper['versions'] :\n",
    "            #v = json.loads(version)\n",
    "            if version['version'] > latest_version:\n",
    "                latest_version = version['version']\n",
    "        paper['latest_version'] = latest_version\n",
    "    except IndexError as ie:\n",
    "        print(ie)\n",
    "        print(f'Paper {paper_id} not found. Perhas should download a new metadata db version?')\n",
    "    \n",
    "    return paper\n",
    "\n",
    "#paper = kaggle_search('2212.09410')\n",
    "paper = kaggle_search('0704.0001')\n",
    "paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download papers based on the id; handles arxiv rates limits\n",
    "i = 0\n",
    "\n",
    "def download_paper_arxiv_api(paper_id):\n",
    "    global i\n",
    "    global downloaded_article_ids\n",
    "    i = i+1\n",
    "    if(paper_id):\n",
    "        if paper_id[:10] in downloaded_article_ids:\n",
    "            print(f'{i} A version of {paper_id} exists.')\n",
    "        else:\n",
    "            paper = next(arxiv.Search(id_list=[paper_id]).results())\n",
    "            paper_title = re.sub('[^a-zA-Z0-9]', '_', paper.title)\n",
    "            short_id = paper.get_short_id()\n",
    "            long_file_name = f\"{PDF_BASE}/{short_id}.{paper_title}.pdf\"\n",
    "            file_name = f\"{short_id}.{paper_title}.pdf\"\n",
    "            if(os.path.exists(long_file_name)):\n",
    "                print(f'{i} File exists. Skipping {file_name}')\n",
    "            else:\n",
    "                print(f'{i} Downloading {file_name}')\n",
    "                # this might hit arxiv's rate limits\n",
    "                paper.download_pdf(dirpath = PDF_BASE, filename=file_name)\n",
    "            time.sleep(5)\n",
    "\n",
    "def download_paper_kaggle(paper_id):\n",
    "    global i\n",
    "    global downloaded_article_ids\n",
    "    i = i+1\n",
    "    if(paper_id):\n",
    "        if paper_id[:10] in downloaded_article_ids:\n",
    "            print(f'{i} A version of {paper_id} exists.')\n",
    "        else:\n",
    "            paper = kaggle_search(paper_id[:10])\n",
    "            if paper:\n",
    "                paper_title = re.sub('[^a-zA-Z0-9]', '_', paper['title'])\n",
    "                short_id = f'{paper[\"id\"]}{paper[\"latest_version\"]}'\n",
    "                long_file_name = f\"{PDF_BASE}/{short_id}.{paper_title}.pdf\"\n",
    "                file_name = f\"{short_id}.{paper_title}.pdf\"\n",
    "                if(os.path.exists(long_file_name)):\n",
    "                    print(f'{i} File exists. Skipping {file_name}')\n",
    "                else:\n",
    "                    print(f'{i} Downloading {file_name}')\n",
    "                    # this might hit arxiv's rate limits\n",
    "                    google_cloud_download(short_id, long_file_name)\n",
    "                    time.sleep(5)\n",
    "            else:\n",
    "                # TODO missed papers - write them down in a file for later download\n",
    "                print(f'Paper {paper_id} not in metadata, probably not on gcloud yet.')\n",
    "                google_cloud_download(paper_id, paper_id)\n",
    "                time.sleep(5)\n",
    "\n",
    "\n",
    "df['article_id'].apply(download_paper_kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{CORPUS}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_properties['latest'] = LATEST\n",
    "save_properties(corpus_properties, f'corpora/{CORPUS}.properties')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
