{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aece9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import boto3\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255d8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "AWS_ACCESS_KEY_ID = os.getenv('aws_access_key_id')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('aws_secret_access_key')\n",
    "ENDPOINT_URL = os.getenv('endpoint_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebfc05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    #model_name: str = \"facebook/opt-30b\"\n",
    "    model_name: str = \"gpt2\"\n",
    "    # Path to HuggingFace cache directory. Default is ~/.cache/huggingface/hub.\n",
    "    cache_dir: Optional[str] = '/data/modelcache/huggingface/hub' #None\n",
    "    # Path to the directory that actually holds model files.\n",
    "    # e.g., ~/.cache/huggingface/models--facebook--opt-30b/snapshots/xxx/\n",
    "    # If this path is not None, we skip download models from HuggingFace.\n",
    "    repo_root: Optional[str] = None\n",
    "    # This is how many DeepSpeed-inference replicas to run for\n",
    "    # this batch inference job.\n",
    "    num_worker_groups: int = 1\n",
    "    # Number of DeepSpeed workers per group.\n",
    "    num_workers_per_group: int = 8\n",
    "\n",
    "    batch_size: int = 1\n",
    "    dtype: str = \"float16\"\n",
    "    # Maximum number of tokens DeepSpeed inference-engine can work with,\n",
    "    # including the input and output tokens.\n",
    "    max_tokens: int = 1024\n",
    "    # Use meta tensors to initialize model.\n",
    "    use_meta_tensor: bool = True\n",
    "    # Use cache for generation.\n",
    "    use_cache: bool = True\n",
    "    # The path for which we want to save the loaded model with a checkpoint.\n",
    "    save_mp_checkpoint_path: Optional[str] = None\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "978d3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(config: Config):\n",
    "    # This function downloads the specified HF model into a local directory.\n",
    "    # This can also download models from cloud storages like S3.\n",
    "    return snapshot_download(\n",
    "        repo_id=config.model_name,\n",
    "        cache_dir=config.cache_dir,\n",
    "        allow_patterns=[\"*\"],\n",
    "        # Skip downloading TF and FLAX weight files.\n",
    "        ignore_patterns=[\"*.safetensors\", \"*.msgpack\", \"*.h5\"],\n",
    "        revision=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7afba225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df596835b9ea4678937db6a0da483890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/data/modelcache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = download_model(config)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4138256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(    \n",
    "    aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key = AWS_SECRET_ACCESS_KEY,\n",
    ")\n",
    "\n",
    "s3_client = session.client(\n",
    "    service_name='s3',\n",
    "    endpoint_url= ENDPOINT_URL,\n",
    ")\n",
    "\n",
    "s3_client.upload_file(Filename='test.txt', Bucket='models', Key='test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e52fac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, botocore\n",
    "def upload_files(path):\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            try:\n",
    "                s3_client.head_object(Bucket='models', Key=full_path[len(path)+1:])\n",
    "                #print(f'Existing: {full_path[len(path)+1:]}')\n",
    "            except botocore.exceptions.ClientError as e:\n",
    "                if e.response['Error']['Code'] == \"404\":\n",
    "                    # The key does not exist.\n",
    "                    print(f'Uploading: {full_path[len(path)+1:]}')\n",
    "                    s3_client.upload_file(Filename=full_path, Bucket='models', Key=full_path[len(path)+1:])\n",
    "                elif e.response['Error']['Code'] == \"403\":\n",
    "                    # Unauthorized, including invalid bucket\n",
    "                    print(e)\n",
    "                else:\n",
    "                  # Something else has gone wrong.\n",
    "                    raise e    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc132cd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading: hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/64-fp16.tflite\n",
      "Uploading: hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/64.tflite\n",
      "Uploading: hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
      "Uploading: hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/generation_config.json\n",
      "Uploading: hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/merges.txt\n",
      "Uploading: hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
      "Uploading: hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/README.md\n",
      "Uploading: hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/rust_model.ot\n",
      "Uploading: hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/tokenizer.json\n",
      "Uploading: hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/vocab.json\n",
      "elapsed: 429.85366702079773\n"
     ]
    }
   ],
   "source": [
    "#upload_files('/home/arylwen/.cache/huggingface')\n",
    "start = time.time()\n",
    "upload_files('/data/modelcache/huggingface')\n",
    "end = time.time()\n",
    "print(f'elapsed: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45fd44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
